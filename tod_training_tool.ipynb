{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dependencies\n",
    "Install **anaconda** is recommended\n",
    "\n",
    "| Name             | Version | Numpy & Python Version   |             |\n",
    "| ---------------- |---------|--------------------------|-------------|\n",
    "| cassandra-driver | 3.11.0  |      py35_1              | conda-forge |\n",
    "| pandas           | 0.19.1  | np111py35_0              |             | \n",
    "| scikit-learn     | 0.18.1  | np111py35_0              |             |\n",
    "| scipy            | 0.18.1  | np111py35_0              |             |\n",
    "| matplotlib       | 2.0.0   | np111py35_0              |             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "commands.\n",
    "'''\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters. \n",
    "Parameters that are defined in this cell can be injected and overwritten by the machine learning platform.\n",
    "'''\n",
    "\n",
    "# MLP defined parameters \n",
    "training_runner = None\n",
    "project_id = None\n",
    "training_id = None\n",
    "metrics_feedback_url = None\n",
    "model_file = None\n",
    "output_dir = 'out'\n",
    "training_metrics_file = 'training.metrics'\n",
    "cross_validation_metrics_file = 'cross_validation.metrics'\n",
    "testing_metrics_file = 'testing.metrics'\n",
    "feature_importance_file = \"feature.importance\"\n",
    "\n",
    "# user defined parameters\n",
    "\n",
    "# label keys\n",
    "label = 'success'\n",
    "# model file directory\n",
    "model_type = 'ML-TOD'\n",
    "model_id = 'ML-TOD-2'\n",
    "start_date = '2019-09-01'\n",
    "\n",
    "end_date = '2019-12-31'\n",
    "\n",
    "# features\n",
    "input_features = {\n",
    "            \"billing_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"bin\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"bank_name\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_brand\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_category\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_class\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_usage\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"day_of_month\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"funding_source\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"issuer_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"merchant_number\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_amount_usd\": {\n",
    "                \"type\": \"number\"\n",
    "            },\n",
    "            \"payment_currency\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_method_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_service_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"site_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"transaction_date_in_string\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"duration\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"segment_num\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"sub_age\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "}\n",
    "\n",
    "features_cat = ['bin',  \n",
    "                'segment_num_group', \n",
    "                'sub_duration_group',  \n",
    "                'sub_age_group',\n",
    "                'renew_att_num',\n",
    "                'is_first_renewal',\n",
    "                'payment_currency']  #, 'txn_hour_group', 'funding_source', 'card_brand', 'duration', 'segment_num', 'merchant_number' , 'bank_name', 'card_category',\n",
    "features_float = ['bin', 'renew_att_num']\n",
    "features_num = [ \n",
    "    \n",
    "]\n",
    "# 'duration', \n",
    "#     'sub_age',\n",
    "#     'segment_num'\n",
    "\n",
    "features_num_encoded = []\n",
    "features_num_calculated = []\n",
    "\n",
    "features_cat_encoded = ['txn_hour_min_segment', 'week_of_month', 'day_of_week', 'txn_hour_group',\n",
    "                        'segment_num_group', \n",
    "                        'sub_duration_group',  \n",
    "                        'sub_age_group',\n",
    "                        'is_first_renewal',\n",
    "                        'payment_service_id', 'merchant_number'] \n",
    "features_encoded = features_cat_encoded + features_num_encoded\n",
    "\n",
    "# features_grouped = [['txn_hour_min_segment', 'bin'], \n",
    "#                     ['txn_hour_min_segment', 'payment_service_id'],\n",
    "#                     ['txn_hour_min_segment', 'bank_name'], \n",
    "#                     ['txn_hour_min_segment', 'day_of_week'],\n",
    "#                     ['txn_hour_min_segment', 'day_of_month'],\n",
    "#                     ['txn_hour_min_segment', 'week_of_month'],\n",
    "#                     ['txn_hour_min_segment'],\n",
    "#                     ['txn_hour_group', 'bin'],\n",
    "#                     ['transaction_hour', 'bin'], \n",
    "#                     ['transaction_hour', 'payment_service_id'],\n",
    "#                     ['transaction_hour', 'bank_name'], \n",
    "#                     ['transaction_hour', 'day_of_week'],\n",
    "#                     ['transaction_hour', 'day_of_month'],\n",
    "#                     ['transaction_hour', 'week_of_month']\n",
    "#                    ]\n",
    "\n",
    "# features_grouped = [\n",
    "#                     ['bank_name', 'card_category'],\n",
    "#                     ['txn_hour_group', 'payment_service_id'],\n",
    "#                     ['payment_service_id', 'merchant_number'],\n",
    "#                     ['txn_hour_min_segment', 'payment_service_id'],\n",
    "#                     ['txn_hour_group', 'day_of_week'],\n",
    "#                     ['txn_hour_group', 'week_of_month'],\n",
    "#                     ['txn_hour_group', 'day_of_month'],\n",
    "#                     ['card_brand', 'funding_source'],\n",
    "#                     ['txn_hour_min_segment'],\n",
    "#                     ['txn_hour_min_segment', 'bin']\n",
    "#                    ]\n",
    "\n",
    "features_grouped = [\n",
    "                    ['txn_hour_min_segment', 'bank_name', 'card_category'],\n",
    "                    ['txn_hour_min_segment', 'payment_service_id'],\n",
    "                    ['payment_service_id', 'merchant_number'],\n",
    "                    ['txn_hour_min_segment', 'day_of_week'],\n",
    "                    ['txn_hour_min_segment', 'week_of_month'],\n",
    "                    ['txn_hour_min_segment', 'day_of_month'],\n",
    "                    ['txn_hour_group', 'bank_name', 'card_category'],\n",
    "                    ['txn_hour_group', 'payment_service_id'],\n",
    "                    ['txn_hour_group', 'bin'],\n",
    "                    ['txn_hour_group', 'day_of_month'],\n",
    "                    ['card_brand', 'funding_source'],\n",
    "                    ['txn_hour_group'],\n",
    "                    ['transaction_hour'],\n",
    "                    ['txn_hour_min_segment', 'bin']\n",
    "                   ]\n",
    "\n",
    "#                      ['txn_hour_min_segment', 'bank_name'],\n",
    "#                     ['txn_hour_group', 'bank_name'],\n",
    "\n",
    "\n",
    "additional_fields =  [ 'payment_amount_usd' ,'issuer_country', 'billing_country', 'day_of_month', 'site_id', 'merchant_number', 'transaction_hour',\n",
    "                'payment_service_id', 'bin', 'payment_currency', 'bank_name', 'transaction_date_in_string',  'card_category', 'card_brand', 'funding_source']\n",
    "\n",
    "feature_candidates = [ 'card_brand', 'issuer_country', 'day_of_month', 'site_id', 'merchant_number', 'transaction_hour', 'funding_source',\n",
    "                'payment_service_id', 'bin', 'payment_currency', 'bank_name', 'card_category', 'date_increment', 'decline_type']\n",
    "\n",
    "\n",
    "usecols = feature_candidates +  ['renew_att_num', 'cid' ,'payment_amount_usd', 'new_status', 'response_message', 'subscription_id', 'subsegment_id','success', 'cid' , 'received_date', 'billing_country', 'transaction_date_in_string', 'cc_expiration_date']\n",
    "\n",
    "scale_pos_weight = 1\n",
    "tuned_parameters = {}\n",
    "\n",
    "best_parameters = {\n",
    "              'depth': 5,\n",
    "              'iterations': 1201,\n",
    "              'random_seed': 7,\n",
    "              'scale_pos_weight': scale_pos_weight,\n",
    "              'subsample': 0.4,\n",
    "              'bagging_temperature': 3.5,\n",
    "              'rsm': 0.35,\n",
    "              'eval_metric': 'AUC:hints=skip_train~false',\n",
    "              'early_stopping_rounds': 500,\n",
    "              'model_size_reg': 1.0,\n",
    "              'random_strength': 5.0\n",
    "              }\n",
    "\n",
    "\n",
    "#  'l2_leaf_reg': 20.9,\n",
    "\n",
    "training_data_paths =  ['tod_all_fields_2019-08.csv', 'tod_all_fields_2019-09.csv', 'tod_all_fields_2019-10.csv', \n",
    "                        'tod_all_fields_2019-11.csv', 'tod_all_fields_2019-12.csv', 'tod_all_fields_2020-01.csv', \n",
    "                        'tod_all_fields_2020-02.csv', 'tod_all_fields_2020-03.csv', 'tod_all_fields_2020-05.csv']\n",
    "eval_data_paths = ['tod_all_fields_2020-04.csv']\n",
    "test_data_paths = ['tod_all_fields_2020-06.csv']\n",
    "\n",
    "subs_creation_date_files = [\n",
    "     'subs_subscription_creation_date_2017_01_2017_12.csv', \n",
    "     'subs_subscription_creation_date_2018_01_2018_05.csv',\n",
    "     'subs_subscription_creation_date_2018_06_2018_12.csv',\n",
    "     'subs_subscription_creation_date_2019_01_2019_12.csv', \n",
    "     'subs_subscription_creation_date_2020_01_2020_03.csv',\n",
    "     'subs_subscription_creation_date_2020_04.csv'\n",
    "]\n",
    "\n",
    "subs_files = [\n",
    "    'subs_subscription_2018_12_to_2020_01.csv', \n",
    "    'subs_li_item_2020_02_to_2020_02_20.csv',  \n",
    "    'subs_li_item_2020_03_2020_05.csv'\n",
    "]\n",
    "\n",
    "sub_seg_expire_files = [\n",
    "    'sub_seg_expire_2019_all.csv', \n",
    "    'sub_seg_expire_2020_01_2020_02.csv', \n",
    "    'sub_seg_expire_2020_03_to_2020_04.csv', \n",
    "    'sub_seg_expire_2020_04_2020_07.csv'\n",
    "]\n",
    "\n",
    "'''data  conditions'''\n",
    "\n",
    "exclude_decline_types = ['invalid_account', 'invalid_cc', 'invalid_txn','correct_cc_retry', 'expired_card']\n",
    "excluded_processors = ['paypalExpress'] #['mes', \n",
    "included_site_ids = [] #['kasperbr']\n",
    "included_billing_countries = ['BR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "print out all parameters.\n",
    "'''\n",
    "\n",
    "print('training_runner = ', training_runner)\n",
    "print('project_id =', project_id)\n",
    "print('training_id =', training_id)\n",
    "print('metrics_feedback_url =', metrics_feedback_url)\n",
    "print('model_file =', model_file)\n",
    "print('label =', label)\n",
    "\n",
    "print('training_data_paths =', training_data_paths)\n",
    "print('eval_data_paths =', eval_data_paths)\n",
    "print('test_data_paths =', test_data_paths)\n",
    "print('sub_seg_expire_files =', sub_seg_expire_files)\n",
    "print('exclude_decline_types =', exclude_decline_types)\n",
    "\n",
    "\n",
    "'''\n",
    "print out manipulated and aggregated features.\n",
    "'''\n",
    "print('\\n============== training parameters & features ================ ')\n",
    "print('input_features =', input_features)\n",
    "print('additional_fields =', additional_fields)\n",
    "print('tuned_parameters =', tuned_parameters)\n",
    "print('best_parameters =', best_parameters)\n",
    "print('features_cat =', features_cat)\n",
    "print('features_float =', features_float)\n",
    "print('features_num =', features_num)\n",
    "print('features_grouped =', features_grouped)\n",
    "\n",
    "print('feature_num_encoded', features_encoded)\n",
    "print('features_encoded', features_encoded)\n",
    "print('features_num_calculated', features_num_calculated)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "imports.\n",
    "'''\n",
    "\n",
    "#import for training\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# from src.web.utils import PreProcessing\n",
    "from src.web.preprocessing import PreProcessing\n",
    "# from src.web.encoder import EnhancedLeaveOneOutEncoder\n",
    "from src.web.train_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "configurations.\n",
    "'''\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.web.utils import to_date\n",
    "from src.web.utils import days_between\n",
    "from src.web.utils import is_expired\n",
    "\n",
    "def days_between_period(df):\n",
    "    d1 = to_date(df['next_renewal_date'])\n",
    "    d2 = to_date(df['grace_period_date'])\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "def process_data(df):\n",
    "    df = df[~(df['payment_service_id'].isin(excluded_processors))]\n",
    "#     df = df[~df['payment_amount_usd'].isna()]\n",
    "#     df = df[~(df['new_status'] == 'Reversed')]\n",
    "#     df = df[df.site_id.isin(included_site_ids)]\n",
    "    df = df[df.billing_country.isin(included_billing_countries)]\n",
    "\n",
    "    df['bin'] = df['bin'].fillna('').astype(str).str.replace('.0', '', regex=False)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def process_data(dca_df, df_sub, df_sub_seg, df_creation_date):\n",
    "    df = dca_df.copy()\n",
    "    df = df[df.billing_country.isin(included_billing_countries)]\n",
    "    df_amex = df[(df.payment_service_id == 'mes') & (df.card_brand.isin(['American Express', 'Discover']))] \n",
    "\n",
    "    df = df\n",
    "    df = df[~(df.payment_service_id.isin(excluded_processors))]\n",
    "    df = df[df['date_increment'].isna()]\n",
    "    \n",
    "#     df = df[~(df['new_status'] == 'Reversed')]    \n",
    "#     df = df[~df['payment_amount_usd'].isna()]\n",
    "    df = pd.merge(df, df_sub[['subsegment_id', 'renewal_window', 'grace_period_date', 'next_renewal_date']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "    df = pd.merge(df, df_sub_seg[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "\n",
    "    \n",
    "#     df['is_expired'] = df.apply(is_expired, axis=1)\n",
    "#     df.loc[~df['date_increment'].isna(), 'is_expired'] = True\n",
    "\n",
    "#     df = df[~(df.duration.isna())]\n",
    "#     df = df[~(df['bin'] == 'nan')]\n",
    "#     df = df[~(df['cc_expiration_date'] == 'nan')]\n",
    "    \n",
    "    df = pd.concat([df, df_amex])\n",
    "    df = pd.merge(df, df_creation_date, left_on='subscription_id', right_on='subscription_id', how='left')\n",
    "    df.subs_activation_date.fillna('2017-01-01 00:00:00', inplace=True)\n",
    "    df['sub_age'] = df.apply(lambda x: days_between(x.transaction_date_in_string, x.subs_activation_date), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subs_creation_date =  pd.concat((read_from(file, s3_dir='ml_files') for file in subs_creation_date_files) , ignore_index=True)\n",
    "subs_creation_date = subs_creation_date.rename(columns={\"SUBSCRIPTION_ID\": \"subscription_id\", \"CREATION_DATE\": \"subs_activation_date\"})\n",
    "subs_creation_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_subs =  pd.concat((read_from(file, s3_dir='training_files') for file in subs_files) , ignore_index=True)\n",
    "\n",
    "df_subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_seg_expire =  pd.concat((read_from(file) for file in sub_seg_expire_files) , ignore_index=True).drop_duplicates(subset=['subsegment_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_paths = ['tod_all_fields_2019_09.csv', 'tod_all_fields_2019_10.csv', 'tod_all_fields_2019_11.csv']\n",
    "# training_data_paths =  ['tod_all_fields_2019_11.csv']\n",
    "\n",
    "df_train =  pd.concat((read_from( file, usecols=usecols, s3_dir='ml_files') for file in training_data_paths) , ignore_index=True)\n",
    "# df_train = process_data(df_train)\n",
    "df_train = process_data(df_train, df_subs, df_sub_seg_expire, subs_creation_date)\n",
    "print(training_data_paths)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_data_paths = ['tod_all_fields_2019_12.csv']\n",
    "# df_eval =  read_from(eval_data_path, usecols=usecols)\n",
    "df_eval =  pd.concat((read_from( file, usecols=usecols, s3_dir='ml_files') for file in eval_data_paths) , ignore_index=True)\n",
    "# df_eval = process_data(df_eval)\n",
    "df_eval = process_data(df_eval, df_subs, df_sub_seg_expire, subs_creation_date)\n",
    "\n",
    "print(df_eval.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test =  read_from(test_data_path, usecols=usecols)\n",
    "df_test =  pd.concat((read_from( file, usecols=usecols, s3_dir='ml_files') for file in test_data_paths) , ignore_index=True)\n",
    "# df_test = process_data(df_test)\n",
    "df_test = process_data(df_test, df_subs, df_sub_seg_expire, subs_creation_date)\n",
    "\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub_seg_expire =  pd.concat((read_from(file) for file in sub_seg_expire_files) , ignore_index=True).drop_duplicates(subset=['subsegment_id'], keep='first')\n",
    "# # df_sub_seg_expire_2020 = pd.read_csv(WORK_DIR + 'sub_seg_expire_2020_01_2020_02.csv')\n",
    "\n",
    "# df_train = pd.merge(df_train, df_sub_seg_expire[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "# df_eval = pd.merge(df_eval, df_sub_seg_expire[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "# df_test = pd.merge(df_test, df_sub_seg_expire[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data manipulation.\n",
    "'''\n",
    "\n",
    "df_train = df_train[df_train['date_increment'].isna()]\n",
    "df_train = df_train[~(df_train['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "df_eval = df_eval[df_eval['date_increment'].isna()]\n",
    "df_eval = df_eval[~(df_eval['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "df_test = df_test[df_test['date_increment'].isna()]\n",
    "df_test = df_test[~(df_test['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "#Exclude some data\n",
    "# df_train = df_train[~(df_train['bin'] == 'nan')]\n",
    "# df_eval = df_eval[~(df_eval['bin'] == 'nan')]\n",
    "# df_test = df_test[~(df_test['bin'] == 'nan')]\n",
    "\n",
    "# df_train = df_train[~(df_train['cc_expiration_date'] == 'nan')]\n",
    "# df_eval = df_eval[~(df_eval['cc_expiration_date'] == 'nan')]\n",
    "# df_test = df_test[~(df_test['cc_expiration_date'] == 'nan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_hour_group = [0,2, 6, 10, 14, 18, 22, 25]\n",
    "\n",
    "df_train['txn_hour_group'] = pd.cut(df_train['transaction_hour'], txn_hour_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_eval['txn_hour_group'] = pd.cut(df_eval['transaction_hour'], txn_hour_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_test['txn_hour_group'] = pd.cut(df_test['transaction_hour'], txn_hour_group).astype(str).str.replace('.0', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_num_group = [-1, 1, 2, 3, 4, 5, 6, 7, 8, 15, 20, 25, 30, 40, 50, 70, 100, 150]\n",
    "\n",
    "df_train['segment_num_group'] = pd.cut(df_train['segment_num'], segment_num_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_eval['segment_num_group'] = pd.cut(df_eval['segment_num'], segment_num_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_test['segment_num_group'] = pd.cut(df_test['segment_num'], segment_num_group).astype(str).str.replace('.0', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_first_renewal'] = (df_train['segment_num'] < 2) & (df_train['segment_num'] >= 0)\n",
    "df_eval['is_first_renewal'] = (df_eval['segment_num'] < 2) & (df_eval['segment_num'] >= 0)\n",
    "df_test['is_first_renewal'] = (df_test['segment_num'] < 2) & (df_test['segment_num'] >= 0)\n",
    "\n",
    "df_train.loc[(df_train.card_brand.str.lower().str.startswith('american', na=False)), 'card_category'] = 'american_express'\n",
    "df_eval.loc[(df_eval.card_brand.str.lower().str.startswith('american', na=False)), 'card_category'] = 'american_express'\n",
    "df_test.loc[(df_test.card_brand.str.lower().str.startswith('american', na=False)), 'card_category'] = 'american_express'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[(df_train['duration'] == 28) | (df_train['duration'] == 29) | (df_train['duration'] == 31) , 'duration'] = 30\n",
    "df_train.loc[(df_train['duration'] == 366) , 'duration'] = 365\n",
    "df_train.loc[(df_train['duration'] == 731) , 'duration'] = 730\n",
    "\n",
    "df_eval.loc[(df_eval['duration'] == 28) | (df_eval['duration'] == 29) | (df_eval['duration'] == 31) , 'duration'] = 30\n",
    "df_eval.loc[(df_eval['duration'] == 366) , 'duration'] = 365\n",
    "df_eval.loc[(df_eval['duration'] == 731) , 'duration'] = 730\n",
    "\n",
    "df_test.loc[(df_test['duration'] == 28) | (df_test['duration'] == 29) | (df_test['duration'] == 31) , 'duration'] = 30\n",
    "df_test.loc[df_test['duration'] == 366 , 'duration'] = 365\n",
    "df_test.loc[(df_test['duration'] == 731) , 'duration'] = 730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_group = [0, 3, 6, 9, 13, 17, 20, 25, 27, 33, 39, 43, 62, 70, 80, 88, 94, 100, 118, 125, 130, 146, 155, 176, 184, 200, 213, 230, 263, 300, 363, 368, 373,729, 733, 1000, 2000]\n",
    "\n",
    "df_train['sub_duration_group'] = pd.cut(df_train['duration'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_eval['sub_duration_group'] = pd.cut(df_eval['duration'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_test['sub_duration_group'] = pd.cut(df_test['duration'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "\n",
    "df_train['sub_age_group'] = pd.cut(df_train['sub_age'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_eval['sub_age_group'] = pd.cut(df_eval['sub_age'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_test['sub_age_group'] = pd.cut(df_test['sub_age'], duration_group).astype(str).str.replace('.0', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_size = len(df_train)\n",
    "balanced_size = len(df_train)\n",
    "fail_size = df_train[label].value_counts(normalize=True)[0.0]\n",
    "success_size =  df_train[label].value_counts(normalize=True)[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "feature manipulation and aggregation.\n",
    "'''\n",
    "features = features_cat + features_encoded\n",
    "\n",
    "additional_fields = [x for x in additional_fields if x not in features_cat]\n",
    "fields = features_cat + features_num +  additional_fields\n",
    "\n",
    "\n",
    "features_dict = {'LABEL': label, 'FIELDS': fields ,'FEATURES_CAT': features_cat, 'FEATURES_NUM':features_num, 'FEATURES_ENCODED':features_encoded, 'FEATURES_NUM_ENCODED':features_num_encoded, 'FEATURES_NUM_CALCULATED':features_num_calculated, 'FEATURES_FLOAT': features_float}\n",
    "features_dict_key = 'preprocessing__features_dict'\n",
    "features_dict['FEATURES_GROUPED'] = features_grouped\n",
    "features_dict['ADDITIONAL_FIELDS'] = additional_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('fields', fields)\n",
    "print('features_dict', features_dict)\n",
    "print('features_dict_key', features_dict_key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(df):\n",
    "    print(df[label].value_counts(normalize=True))\n",
    "    df_0 = df[df[label] == 0]\n",
    "    df_1 = df[df[label] == 1]\n",
    "    \n",
    "    df_0_over = df_0.sample(int(len(df_1)/2), replace=True)\n",
    "    df = pd.concat([df_0_over, df_1], axis=0, ignore_index=False)\n",
    "    \n",
    "    print(df[label].value_counts(normalize=True))\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filters data'''\n",
    "\n",
    "# input_data = df_train[df_train.renew_att_num==1]\n",
    "# _df_eval = df_eval[df_eval.renew_att_num==1]\n",
    "# _df_test = df_test[df_test.renew_att_num==1]\n",
    "\n",
    "input_data = df_train\n",
    "_df_eval = df_eval\n",
    "_df_test = df_test\n",
    "\n",
    "# input_data = over_sampling(input_data)\n",
    "\n",
    "# _df_eval = over_sampling(_df_eval)\n",
    "# _df_test = over_sampling(_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prepares training parameters'''\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import src.web.preprocessing\n",
    "from src.web.preprocessing import PreProcessing\n",
    "from src.web.train_util import *\n",
    "from importlib import import_module\n",
    "import sys\n",
    "\n",
    "classifier = CatBoostClassifier\n",
    "\n",
    "cat_features_len = len(features_cat) +  len (features_grouped)\n",
    "\n",
    "# scale_pos_weight = (input_data[label].value_counts(normalize=True)[0.0] / input_data[label].value_counts(normalize=True)[1.0] ) + 0.5\n",
    "if not scale_pos_weight:\n",
    "    scale_pos_weight = (input_data[label].value_counts(normalize=True)[0.0] / input_data[label].value_counts(normalize=True)[1.0] ) + 0.5\n",
    "\n",
    "# scale_pos_weight = 1\n",
    "best_parameters['scale_pos_weight'] = scale_pos_weight\n",
    "    \n",
    "features_dict['use_cat_encoder'] = False\n",
    "_preProcessor = PreProcessing().fit(input_data, input_data['success'], features_dict=features_dict)            \n",
    "_x_eval = _preProcessor.transform(_df_eval)\n",
    "_y_eval = _df_eval[\"success\"]\n",
    "\n",
    "alg_name = 'catboostclassifier'\n",
    "\n",
    "\n",
    "cat_features = list(range(0,cat_features_len))\n",
    "\n",
    "fit_params = {\n",
    "    f\"{alg_name}__verbose\": True,\n",
    "    f\"{alg_name}__cat_features\": cat_features,\n",
    "    f\"{alg_name}__plot\": True,\n",
    "    f\"{alg_name}__eval_set\": Pool(_x_eval, _y_eval, cat_features)\n",
    "}\n",
    "\n",
    "\n",
    "features_dict['fit_params'] = fit_params\n",
    "\n",
    "model_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "if training_runner is None:\n",
    "    version = get_latest_version(model_id, model_type) + 1\n",
    "    model_name = model_id + '.' + str(version)\n",
    "    features_dict['model_name'] = model_name\n",
    "\n",
    "clf, result_d = build_and_train(\n",
    "    input_data, \n",
    "    classifier, \n",
    "    tuned_parameters, \n",
    "    alg_name, \n",
    "    model_file, \n",
    "    best_param=best_parameters, \n",
    "    features_dict=features_dict, \n",
    "    test_data=_df_test,\n",
    "    metrics_feedback_url=metrics_feedback_url)\n",
    "                                   \n",
    "print(\"result_dict: \", result_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "save feature importance scores.\n",
    "'''\n",
    "\n",
    "import random\n",
    "\n",
    "# saving feature importance\n",
    "feature_importance = {}\n",
    "for feature_name in input_features.keys():\n",
    "    feature_importance[feature_name] = random.uniform(0.0, 1.0)\n",
    "\n",
    "with open(feature_importance_file, 'w') as feature_importance_out:\n",
    "    json.dump(feature_importance, feature_importance_out, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "output the model\n",
    "'''\n",
    "\n",
    "start_date = '2019-08-01'\n",
    "end_date = '2020-05-30'\n",
    "if training_runner is None:\n",
    "    version = get_latest_version(model_id, model_type) + 1\n",
    "    model_name = model_id + '.' + str(version)\n",
    "    model_file, model_file_name = write_model(clf, model_name)\n",
    "    \n",
    "    preprocess_repo_path = handle_preprocessing_file(model_id, version)\n",
    "    size_desc = str(\", original size: %s (fail: %s, success: %s), balanced_size: %s\" % (original_size, fail_size, success_size, original_size))\n",
    "    desc = f'TOD for kasperbr. Include all renew_att. Use txn_hour_min_segment to aggregate instead of thx_hour_group. Add card_brand-funding_source, txn_hour_min_segment. Only include date_increment is na. Include duration_group, sub_age_group and segment_num_group. With new ROC AUC using predict_proba. No oversampling failed samples. Cal attempt 1 only. Add txn_hour_group. {start_date}_{end_date}_for training data, 2020-04 for eval data, 2020-06 for test data, eval_metric=AUC. Size: {size_desc}'\n",
    "\n",
    "    hyper_params = result_d.pop('hyper_params', None)\n",
    "    extended_att = {\"preprocess_repo_path\": preprocess_repo_path, \"input_features\": input_features}\n",
    "    repo_path = upload_artifact(model_file_name)\n",
    "    insert_model_info(model_id, version, repo_path, desc=desc, model_type=model_type,eval_metrics=json.dumps(result_d), \n",
    "                      hyper_parameter=json.dumps(hyper_params), extended_att=json.dumps(extended_att), features_dict=features_dict, algorithm='CatBoostClassifier')\n",
    "    \n",
    "else:\n",
    "    model_file = joblib.dump(clf, model_file)\n",
    "\n",
    "print('model_file generated: ', model_file)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}