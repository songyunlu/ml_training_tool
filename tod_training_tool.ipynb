{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dependencies\n",
    "Install **anaconda** is recommended\n",
    "\n",
    "| Name             | Version | Numpy & Python Version   |             |\n",
    "| ---------------- |---------|--------------------------|-------------|\n",
    "| cassandra-driver | 3.11.0  |      py35_1              | conda-forge |\n",
    "| pandas           | 0.19.1  | np111py35_0              |             | \n",
    "| scikit-learn     | 0.18.1  | np111py35_0              |             |\n",
    "| scipy            | 0.18.1  | np111py35_0              |             |\n",
    "| matplotlib       | 2.0.0   | np111py35_0              |             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "commands.\n",
    "'''\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters. \n",
    "Parameters that are defined in this cell can be injected and overwritten by the machine learning platform.\n",
    "'''\n",
    "\n",
    "# MLP defined parameters \n",
    "training_runner = None\n",
    "project_id = None\n",
    "training_id = None\n",
    "metrics_feedback_url = None\n",
    "model_file = None\n",
    "output_dir = 'out'\n",
    "training_metrics_file = 'training.metrics'\n",
    "cross_validation_metrics_file = 'cross_validation.metrics'\n",
    "testing_metrics_file = 'testing.metrics'\n",
    "feature_importance_file = \"feature.importance\"\n",
    "\n",
    "# user defined parameters\n",
    "\n",
    "# label keys\n",
    "label = 'success'\n",
    "# model file directory\n",
    "work_dir = '/var/spark/ml_files/'\n",
    "model_type = 'ML-TOD'\n",
    "start_date = '2019-09-01'\n",
    "\n",
    "end_date = '2019-12-31'\n",
    "# desc = '%s_%s_for_calendar_retry_attempt'.format(start_date, end_date)\n",
    "\n",
    "# data\n",
    "training_data = work_dir + 'tod_all_fields_2019_06.csv'\n",
    "# bin_profile_data =  work_dir + 'bin_profile_2019_01_to_2019_05.csv'\n",
    "# payment_mid_bin_data = work_dir + 'payment_mid_bin_2019_01_to_05.csv'\n",
    "# decline_type_data = work_dir + 'Decline_Type.csv'\n",
    "\n",
    "# features\n",
    "input_features = {\n",
    "            \"billing_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"bin\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"bank_name\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_brand\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_category\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_class\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_usage\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"day_of_month\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"funding_source\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"issuer_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"merchant_number\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_amount_usd\": {\n",
    "                \"type\": \"number\"\n",
    "            },\n",
    "            \"payment_currency\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_method_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_service_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"site_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"transaction_date_in_string\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "}\n",
    "\n",
    "features_cat = [ 'duration' ]\n",
    "features_float = ['bin', 'renew_att_num']\n",
    "features_num = ['segment_num']\n",
    "features_num_encoded = []\n",
    "features_num_calculated = []\n",
    "\n",
    "features_cat_encoded = ['txn_hour_min_segment', 'week_of_month', 'day_of_week', 'payment_service_id', 'merchant_number'] \n",
    "features_encoded = features_cat_encoded + features_num_encoded\n",
    "\n",
    "features_grouped = [['txn_hour_min_segment', 'bin'], \n",
    "                    ['txn_hour_min_segment', 'billing_country'],\n",
    "                    ['txn_hour_min_segment', 'site_id'], \n",
    "                    ['txn_hour_min_segment', 'week_of_month'],\n",
    "                    ['txn_hour_min_segment', 'payment_currency']]\n",
    "\n",
    "additional_fields =  [ 'payment_amount_usd' ,'issuer_country', 'billing_country', 'day_of_month', 'site_id', 'merchant_number', 'transaction_hour',\n",
    "                'payment_service_id', 'bin', 'payment_currency', 'bank_name', 'transaction_date_in_string']\n",
    "\n",
    "scale_pos_weight = None\n",
    "tuned_parameters = {}\n",
    "\n",
    "best_parameters = {\n",
    "              'depth': 5,\n",
    "              'iterations': 1201,\n",
    "              'random_seed': 7,\n",
    "              'scale_pos_weight': scale_pos_weight,\n",
    "              'subsample': 0.5,\n",
    "              'bagging_temperature': 3.5,\n",
    "              'rsm': 0.35,\n",
    "              'eval_metric': 'BrierScore',\n",
    "              'early_stopping_rounds': 500,\n",
    "              'model_size_reg': 2.5,\n",
    "              'l2_leaf_reg': 20.9,\n",
    "              'random_strength': 5.0\n",
    "              }\n",
    "\n",
    "training_data_paths =  ['tod_all_fields_2019_11.csv']\n",
    "eval_data_paths = ['tod_all_fields_2019_12.csv']\n",
    "test_data_paths = ['tod_all_fields_2020_01.csv']\n",
    "sub_seg_expire_files = [ 'sub_seg_expire_2019_all.csv', 'sub_seg_expire_2020_01_2020_02.csv']\n",
    "\n",
    "exclude_decline_types = ['invalid_account', 'invalid_cc', 'invalid_txn','correct_cc_retry', 'expired_card', 'format error', 'no savings account', 'revoc', 'declined non generic', 'do not try again/use alternate payment card']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.web.train_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "configurations.\n",
    "'''\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_candidates = [ 'issuer_country', 'day_of_month', 'site_id', 'merchant_number', 'transaction_hour',\n",
    "                'payment_service_id', 'bin', 'payment_currency', 'bank_name', \"card_category\", \"date_increment\", 'decline_type']\n",
    "\n",
    "\n",
    "usecols = feature_candidates +  ['renew_att_num', 'cid' ,'payment_amount_usd', 'new_status', 'response_message', 'subscription_id', 'subsegment_id','success', 'cid' ,'added_expiry_years', 'received_date', 'billing_country', 'transaction_date_in_string', 'cc_expiration_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_paths = ['tod_all_fields_2019_09.csv', 'tod_all_fields_2019_10.csv', 'tod_all_fields_2019_11.csv']\n",
    "# training_data_paths =  ['tod_all_fields_2019_11.csv']\n",
    "\n",
    "df_train =  pd.concat((read_from( file, usecols=usecols) for file in training_data_paths) , ignore_index=True)\n",
    "df_train = df_train[~(df_train['payment_service_id'] == 'mes')]\n",
    "df_train = df_train[~(df_train['payment_service_id'] == 'paypalExpress')]\n",
    "df_train = df_train[~df_train['payment_amount_usd'].isna()]\n",
    "df_train = df_train[~(df_train['new_status'] == 'Reversed')]\n",
    "df_train['bin'] = df_train['bin'].fillna('').astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "print(training_data_paths)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_data_paths = ['tod_all_fields_2019_12.csv']\n",
    "# df_eval =  read_from(eval_data_path, usecols=usecols)\n",
    "df_eval =  pd.concat((read_from( file, usecols=usecols) for file in eval_data_paths) , ignore_index=True)\n",
    "df_eval['bin'] = df_eval['bin'].fillna('').astype(str).str.replace('.0', '', regex=False)\n",
    "df_eval = df_eval[df_eval['renew_att_num'] == 1]\n",
    "df_eval = df_eval[~(df_eval['payment_service_id'] == 'mes')]\n",
    "df_eval = df_eval[~(df_eval['payment_service_id'] == 'paypalExpress')]\n",
    "df_eval = df_eval[~df_eval['payment_amount_usd'].isna()]\n",
    "df_eval = df_eval[~(df_eval['new_status'] == 'Reversed')]\n",
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test =  read_from(test_data_path, usecols=usecols)\n",
    "df_test =  pd.concat((read_from( file, usecols=usecols) for file in test_data_paths) , ignore_index=True)\n",
    "\n",
    "df_test['bin'] = df_test['bin'].fillna('').astype(str).str.replace('.0', '', regex=False)\n",
    "df_test = df_test[df_test['renew_att_num'] == 1]\n",
    "df_test = df_test[~(df_test['payment_service_id'] == 'mes')]\n",
    "df_test = df_test[~(df_test['payment_service_id'] == 'paypalExpress')]\n",
    "df_test = df_test[~df_test['payment_amount_usd'].isna()]\n",
    "df_test = df_test[~(df_test['new_status'] == 'Reversed')]\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_seg_expire =  pd.concat((read_from(file) for file in sub_seg_expire_files) , ignore_index=True)\n",
    "# df_sub_seg_expire_2020 = pd.read_csv(WORK_DIR + 'sub_seg_expire_2020_01_2020_02.csv')\n",
    "\n",
    "df_train = pd.merge(df_train, df_sub_seg_expire[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "df_eval = pd.merge(df_eval, df_sub_seg_expire[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "df_test = pd.merge(df_test, df_sub_seg_expire[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data manipulation.\n",
    "'''\n",
    "\n",
    "df_train = df_train[df_train['date_increment'].isna()]\n",
    "df_train = df_train[~(df_train['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "df_eval = df_eval[df_eval['date_increment'].isna()]\n",
    "df_eval = df_eval[~(df_eval['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "df_test = df_test[df_test['date_increment'].isna()]\n",
    "df_test = df_test[~(df_test['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "#Exclude some data\n",
    "df_train = df_train[~(df_train['bin'] == 'nan')]\n",
    "df_eval = df_eval[~(df_eval['bin'] == 'nan')]\n",
    "df_test = df_test[~(df_test['bin'] == 'nan')]\n",
    "\n",
    "df_train = df_train[~(df_train['cc_expiration_date'] == 'nan')]\n",
    "df_eval = df_eval[~(df_eval['cc_expiration_date'] == 'nan')]\n",
    "df_test = df_test[~(df_test['cc_expiration_date'] == 'nan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_size = len(df_train)\n",
    "balanced_size = len(df_train)\n",
    "fail_size = df_train[label].value_counts(normalize=True)[0.0]\n",
    "success_size =  df_train[label].value_counts(normalize=True)[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "feature manipulation and aggregation.\n",
    "'''\n",
    "# features_cat = [ 'duration' ]\n",
    "# features_float = ['bin', 'renew_att_num']\n",
    "# features_num = ['segment_num']\n",
    "# features_num_encoded = []\n",
    "# features_num_calculated = []\n",
    "features = features_cat + features_encoded\n",
    "\n",
    "# features_cat_encoded = ['txn_hour_min_segment', 'week_of_month', 'day_of_week', 'payment_service_id', 'merchant_number'] \n",
    "\n",
    "# features_grouped = [['txn_hour_min_segment', 'bin'], \n",
    "#                     ['txn_hour_min_segment', 'billing_country'],\n",
    "#                     ['txn_hour_min_segment', 'site_id'], \n",
    "#                     ['txn_hour_min_segment', 'week_of_month'],\n",
    "#                     ['txn_hour_min_segment', 'payment_currency']]\n",
    "\n",
    "# features_encoded = features_cat_encoded + features_num_encoded\n",
    "\n",
    "\n",
    "# additional_fields =  [ 'payment_amount_usd' ,'issuer_country', 'billing_country', 'day_of_month', 'site_id', 'merchant_number', 'transaction_hour',\n",
    "#                 'payment_service_id', 'bin', 'payment_currency', 'bank_name', 'transaction_date_in_string']\n",
    "additional_fields = [x for x in additional_fields if x not in features_cat]\n",
    "fields = features_cat + features_num +  additional_fields\n",
    "\n",
    "\n",
    "# df_decline_type = pd.read_csv(WORK_DIR + 'Decline_Type.csv')\n",
    "\n",
    "features_dict = {'LABEL': label, 'FIELDS': fields ,'FEATURES_CAT': features_cat, 'FEATURES_NUM':features_num, 'FEATURES_ENCODED':features_encoded, 'FEATURES_NUM_ENCODED':features_num_encoded, 'FEATURES_NUM_CALCULATED':features_num_calculated, 'FEATURES_FLOAT': features_float}\n",
    "features_dict_key = 'preprocessing__features_dict'\n",
    "# features_dict['df_bin_profile'] = None #bin_profile\n",
    "features_dict['FEATURES_GROUPED'] = features_grouped\n",
    "features_dict['ADDITIONAL_FIELDS'] = additional_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prepares training parameters'''\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from src.web.train_util import *\n",
    "\n",
    "classifier = CatBoostClassifier\n",
    "\n",
    "cat_features_len = len(features_cat) +  len (features_grouped)\n",
    "input_data = df_train\n",
    "# scale_pos_weight = (input_data[LABEL].value_counts(normalize=True)[0.0] / input_data[LABEL].value_counts(normalize=True)[1.0] ) + 0.5\n",
    "if not scale_pos_weight:\n",
    "    scale_pos_weight = 1 #(input_data[LABEL].value_counts(normalize=True)[1.0] / input_data[LABEL].value_counts(normalize=True)[0.0] )\n",
    "\n",
    "features_dict['use_cat_encoder'] = False\n",
    "_preProcessor = PreProcessing().fit(input_data, input_data['success'], features_dict=features_dict)            \n",
    "_x_eval = _preProcessor.transform(df_eval)\n",
    "_y_eval = df_eval[\"success\"]\n",
    "\n",
    "alg_name = 'catboostclassifier'\n",
    "\n",
    "\n",
    "cat_features = list(range(0,cat_features_len))\n",
    "\n",
    "fit_params = {\n",
    "    f\"{alg_name}__verbose\": True,\n",
    "    f\"{alg_name}__cat_features\": cat_features,\n",
    "    f\"{alg_name}__plot\": True,\n",
    "    f\"{alg_name}__eval_set\": Pool(_x_eval, _y_eval, cat_features)\n",
    "}\n",
    "\n",
    "\n",
    "features_dict['fit_params'] = fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "\n",
    "clf, result_d = build_and_train(\n",
    "    input_data, \n",
    "    classifier, \n",
    "    tuned_parameters, \n",
    "    alg_name, \n",
    "    model_file, \n",
    "    best_param=best_parameters, \n",
    "    features_dict=features_dict, \n",
    "    test_data=df_test,\n",
    "    output_dir=output_dir)\n",
    "# model_file, model_file_name = write_model(clf, model_name)\n",
    "# print(\"model_file is generated: \", model_file)\n",
    "print(\"result_dict: \", result_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "save feature importance scores.\n",
    "'''\n",
    "\n",
    "if training_runner:\n",
    "    import random\n",
    "    \n",
    "    # saving feature importance\n",
    "    feature_importance = {}\n",
    "    for feature_name in input_features.keys():\n",
    "        feature_importance[feature_name] = random.uniform(0.0, 1.0)\n",
    "    \n",
    "    with open(feature_importance_file, 'w') as feature_importance_out:\n",
    "        json.dump(feature_importance, feature_importance_out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "output the model\n",
    "'''\n",
    "\n",
    "if training_runner is None:\n",
    "#     from src.web.model_info_repository import get_latest_version\n",
    "    model_id = 'ML-TOD-TEST-1'\n",
    "    version = get_latest_version(model_id, model_type) + 1\n",
    "    model_name = model_id + '.' + str(version)\n",
    "    model_file, model_file_name = write_model(clf, model_name)\n",
    "else:\n",
    "    model_file = joblib.dump(clf, model_file)\n",
    "\n",
    "print('model_file generated: ', model_file)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}