{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dependencies\n",
    "* install anaconda is recommended\n",
    "\n",
    "```\n",
    "cassandra-driver          3.11.0                   py35_1    conda-forge\n",
    "pandas                    0.19.1              np111py35_0\n",
    "scikit-learn              0.18.1              np111py35_0\n",
    "scipy                     0.18.1              np111py35_0\n",
    "matplotlib                2.0.0               np111py35_0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_columns = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model File Directory\n",
    "WORK_DIR = '/var/spark/ml_files/'\n",
    "\n",
    "#Label Keys\n",
    "LABEL = \"success\"\n",
    "\n",
    "MODEL_TYPE = \"ML-TOD\"\n",
    "\n",
    "start_date = '2019-06-01'\n",
    "\n",
    "end_date = '2019-06-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cassandra_endpoint = '10.81.12.121' #'10.62.1.118'\n",
    "# cassandra_endpoint = '10.224.12.32'\n",
    "cluster = Cluster([cassandra_endpoint])\n",
    "# session = cluster.connect('subs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_CANDIDATES = [ 'issuer_country', 'day_of_month', 'site_id', 'merchant_number', 'transaction_hour',\n",
    "                'payment_service_id', 'bin', 'payment_currency', 'bank_name', \"card_category\", \"date_increment\", 'decline_type']\n",
    "\n",
    "\n",
    "USECOLS = FEATURES_CANDIDATES +  ['renew_att_num', 'cid' ,'payment_amount_usd', 'new_status', 'response_message', 'subscription_id', 'success', 'cid' ,'added_expiry_years', 'received_date', 'billing_country', 'transaction_date_in_string', 'cc_expiration_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1701778, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA_PATH = WORK_DIR + 'tod_all_fields_2019_06.csv'\n",
    "df =  pd.read_csv(TRAINING_DATA_PATH, usecols=USECOLS)\n",
    "df['bin'] = df['bin'].fillna('').astype(str).str.replace('.0', '', regex=False)\n",
    "df = df[~(df['new_status'] == 'Reversed')]\n",
    "df.shape  #(1702331, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542736, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_DATA_PATH = WORK_DIR + 'tod_all_fields_2019_07.csv'\n",
    "df_eval =  pd.read_csv(EVAL_DATA_PATH, usecols=USECOLS)\n",
    "df_eval['bin'] = df_eval['bin'].fillna('').astype(str).str.replace('.0', '', regex=False)\n",
    "df_eval = df_eval[~(df_eval['new_status'] == 'Reversed')]\n",
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1709006, 24)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA_PATH = WORK_DIR + 'tod_all_fields_2019_08.csv'\n",
    "df_test =  pd.read_csv(TEST_DATA_PATH, usecols=USECOLS)\n",
    "df_test['bin'] = df_test['bin'].fillna('').astype(str).str.replace('.0', '', regex=False)\n",
    "df_test = df_test[~(df_test['new_status'] == 'Reversed')]\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_hour_bin_dict = None\n",
    "txn_hour_processor_dict = None\n",
    "txn_hour_country_dict = None\n",
    "\n",
    "TXN_HOUR_BIN_PATH = WORK_DIR + 'txn_hour_bin_2019_06_07.csv'\n",
    "TXN_HOUR_PROCESSOR_PATH = WORK_DIR + 'txn_hour_processor_2019_06_07.csv'\n",
    "TXN_HOUR_COUNTRY_PATH = WORK_DIR + 'txn_hour_country_2019_06_07.csv'\n",
    "TXN_HOUR_SITE_PATH = WORK_DIR + 'txn_hour_site_2019_06_07.csv'\n",
    "TXN_HOUR_BANK_NAME_PATH = WORK_DIR + 'txn_hour_bank_name_2019_06_07.csv'\n",
    "\n",
    "TXN_HOUR_CURRENCY_PATH = WORK_DIR + 'txn_hour_currency_2019_06_07.csv'\n",
    "TXN_HOUR_MID_PATH = WORK_DIR + 'txn_hour_mid_2019_06_07.csv'\n",
    "TXN_HOUR_DATE_PATH = WORK_DIR + 'txn_hour_date_2019_06_07.csv'\n",
    "\n",
    "TXN_HOUR_BIN_PROFILE_PATH = WORK_DIR + 'bin_profile_per_hour_2019_06_07.csv'\n",
    "TXN_HOUR_PROCESSOR_PROFILE_PATH = WORK_DIR + 'processor_profile_per_hour_2019_06_07.csv'\n",
    "\n",
    "if TXN_HOUR_BIN_PROFILE_PATH:\n",
    "    bin_profile_per_hour = pd.read_csv(TXN_HOUR_BIN_PROFILE_PATH)\n",
    "    bin_profile_per_hour.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    bin_profile_per_hour['bin'] = bin_profile_per_hour['bin'].apply(str).str.replace('.0', '', regex=False)\n",
    "    bin_profile_per_hour['transaction_hour'] = bin_profile_per_hour['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "        \n",
    "    bin_max_amt_per_hour_dict = bin_profile_per_hour.set_index(['bin', 'transaction_hour'])['Max_99'].T.to_dict()\n",
    "\n",
    "\n",
    "if TXN_HOUR_PROCESSOR_PROFILE_PATH:\n",
    "    processor_profile_per_hour = pd.read_csv(TXN_HOUR_PROCESSOR_PROFILE_PATH)\n",
    "    processor_profile_per_hour.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    processor_profile_per_hour['transaction_hour'] = processor_profile_per_hour['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "        \n",
    "    processor_max_amt_per_hour_dict = processor_profile_per_hour.set_index(['payment_service_id', 'transaction_hour'])['Max_99'].T.to_dict()    \n",
    "\n",
    "if TXN_HOUR_BIN_PATH:\n",
    "    txn_hour_bin = pd.read_csv(TXN_HOUR_BIN_PATH)\n",
    "    txn_hour_bin.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_bin['bin'] = txn_hour_bin['bin'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_bin['transaction_hour'] = txn_hour_bin['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_bin_dict = txn_hour_bin[txn_hour_bin['num_of_subs'] >= 10].set_index(['bin', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "\n",
    "    \n",
    "# if  TXN_HOUR_PROCESSOR_PATH:\n",
    "#     txn_hour_processor = pd.read_csv(TXN_HOUR_PROCESSOR_PATH)\n",
    "#     txn_hour_processor.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "#     txn_hour_processor['processor_att_num'] = txn_hour_processor['processor_att_num'].apply(str).str.replace('.0', '', regex=False)\n",
    "#     txn_hour_processor['transaction_hour'] = txn_hour_processor['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "#     txn_hour_processor_dict = txn_hour_processor[txn_hour_processor['count'] >=10].set_index(['payment_service_id', 'processor_att_num', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "\n",
    "if  TXN_HOUR_PROCESSOR_PATH:\n",
    "    txn_hour_processor = pd.read_csv(TXN_HOUR_PROCESSOR_PATH)\n",
    "    txn_hour_processor.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_processor['transaction_hour'] = txn_hour_processor['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_processor_dict = txn_hour_processor[txn_hour_processor['num_of_subs'] >=10].set_index(['payment_service_id', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "\n",
    "        \n",
    "if  TXN_HOUR_COUNTRY_PATH:\n",
    "    txn_hour_country = pd.read_csv(TXN_HOUR_COUNTRY_PATH)\n",
    "    txn_hour_country.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_country['transaction_hour'] = txn_hour_country['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_country_dict = txn_hour_country[txn_hour_country['num_of_subs'] >=10].set_index(['issuer_country', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "\n",
    "    \n",
    "if  TXN_HOUR_SITE_PATH:\n",
    "    txn_hour_site = pd.read_csv(TXN_HOUR_SITE_PATH)\n",
    "    txn_hour_site.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_site['transaction_hour'] = txn_hour_site['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_site_dict = txn_hour_site[txn_hour_site['num_of_subs'] >=10].set_index(['site_id', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "    \n",
    "if  TXN_HOUR_BANK_NAME_PATH:\n",
    "    txn_hour_bank_name = pd.read_csv(TXN_HOUR_BANK_NAME_PATH)\n",
    "    txn_hour_bank_name.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_bank_name['transaction_hour'] = txn_hour_bank_name['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_bank_name_dict = txn_hour_bank_name[txn_hour_bank_name['num_of_subs'] >=10].set_index(['bank_name', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "\n",
    "if  TXN_HOUR_CURRENCY_PATH:\n",
    "    txn_hour_currency = pd.read_csv(TXN_HOUR_CURRENCY_PATH)\n",
    "    txn_hour_currency.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_currency['transaction_hour'] = txn_hour_currency['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_currency_dict = txn_hour_currency[txn_hour_currency['num_of_subs'] >=10].set_index(['payment_currency', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "    \n",
    "if  TXN_HOUR_MID_PATH:\n",
    "    txn_hour_mid = pd.read_csv(TXN_HOUR_MID_PATH)\n",
    "    txn_hour_mid.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_mid['transaction_hour'] = txn_hour_mid['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_mid_dict = txn_hour_mid[txn_hour_mid['num_of_subs'] >=10].set_index(['merchant_number', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "\n",
    "if  TXN_HOUR_DATE_PATH:\n",
    "    txn_hour_date = pd.read_csv(TXN_HOUR_DATE_PATH)\n",
    "    txn_hour_date.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    txn_hour_date['day_of_month'] = txn_hour_date['day_of_month'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_date['transaction_hour'] = txn_hour_date['transaction_hour'].apply(str).str.replace('.0', '', regex=False)\n",
    "    txn_hour_date_dict = txn_hour_date[txn_hour_date['num_of_subs'] >=10].set_index(['day_of_month', 'transaction_hour'])['success_rate'].T.to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = len(df)\n",
    "fail_size = df[LABEL].value_counts(normalize=True)[0.0]\n",
    "success_size =  df[LABEL].value_counts(normalize=True)[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#import for training\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# from src.web.utils import PreProcessing\n",
    "from src.web.preprocessing import PreProcessing\n",
    "from src.web.encoder import EnhancedLeaveOneOutEncoder\n",
    "from src.web.train_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FEATURES = {\n",
    "            \"billing_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"bin\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"bank_name\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_brand\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_category\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_class\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_usage\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"day_of_month\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"funding_source\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"issuer_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"billing_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"merchant_number\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_amount_usd\": {\n",
    "                \"type\": \"number\"\n",
    "            },\n",
    "            \"payment_currency\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_method_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_service_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"site_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"transaction_date_in_string\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES_CAT = [ 'issuer_country',  'day_of_month', 'site_id', 'merchant_number', \n",
    "#                 'payment_service_id', 'bin', 'payment_currency', 'bank_name', 'transaction_hour']\n",
    "\n",
    "FEATURES_CAT = [  'transaction_hour']\n",
    "\n",
    "FEATURES_FLOAT = ['bin', 'renew_att_num']\n",
    "FEATURES_NUM = []\n",
    "FEATURES_NUM_CALCULATED = []\n",
    "\n",
    "FEATURES_NUM_ENCODED = ['txn_hour_bin', 'txn_hour_processor', 'txn_hour_country', 'txn_hour_site', 'txn_hour_bank_name',\n",
    "                        'txn_hour_currency', 'txn_hour_mid', 'txn_hour_date'] + FEATURES_NUM_BIN_PROFILE  #, 'payment_mid_bin'\n",
    "\n",
    "FEATURES_GROUPED = []\n",
    "FEATURES_CAT_ENCODED = [ 'week_of_month', 'day_of_week'] \n",
    "FEATURES_ENCODED = FEATURES_CAT_ENCODED + FEATURES_NUM_ENCODED\n",
    "\n",
    "FEATURES = FEATURES_CAT + FEATURES_ENCODED\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from src.web.preprocessing import PreProcessing\n",
    "from src.web.preprocessing import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "ADDITIONAL_FIELDS =  ['payment_amount_usd' ,'issuer_country',  'day_of_month', 'site_id', 'merchant_number', \n",
    "                'payment_service_id', 'bin', 'payment_currency', 'bank_name', 'transaction_date_in_string']\n",
    "ADDITIONAL_FIELDS = [x for x in ADDITIONAL_FIELDS if x not in FEATURES_CAT]\n",
    "FIELDS = FEATURES_CAT + FEATURES_NUM +  ADDITIONAL_FIELDS\n",
    "\n",
    "\n",
    "# df_decline_type = pd.read_csv(WORK_DIR + 'Decline_Type.csv')\n",
    "\n",
    "features_dict = {'LABEL': LABEL, 'FIELDS': FIELDS ,'FEATURES_CAT': FEATURES_CAT, 'FEATURES_NUM':FEATURES_NUM, 'FEATURES_ENCODED':FEATURES_ENCODED, 'FEATURES_NUM_ENCODED':FEATURES_NUM_ENCODED, 'FEATURES_NUM_CALCULATED':FEATURES_NUM_CALCULATED, 'FEATURES_FLOAT': FEATURES_FLOAT}\n",
    "features_dict_key = 'preprocessing__features_dict'\n",
    "features_dict['df_bin_profile'] = None #bin_profile\n",
    "features_dict['FEATURES_GROUPED'] = FEATURES_GROUPED\n",
    "\n",
    "\n",
    "features_dict['txn_hour_group_dict'] = {\"txn_hour_bin_dict\": txn_hour_bin_dict, \"txn_hour_processor_dict\": txn_hour_processor_dict,\\\n",
    "                               \"txn_hour_country_dict\": txn_hour_country_dict, 'txn_hour_site_dict': txn_hour_site_dict, \\\n",
    "                               \"txn_hour_currency_dict\": txn_hour_currency_dict, 'txn_hour_mid_dict': txn_hour_mid_dict, \\\n",
    "                               \"txn_hour_bank_name_dict\": txn_hour_bank_name_dict, 'txn_hour_date_dict': txn_hour_date_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_decline_types = ['invalid_account', 'invalid_cc', 'correct_cc_retry', 'expired_card']\n",
    "df_train = df[df['date_increment'].isna()]\n",
    "df_train = df_train[~(df_train['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "df_eval = df_eval[df_eval['date_increment'].isna()]\n",
    "df_eval = df_eval[~(df_eval['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "df_test = df_test[df_test['date_increment'].isna()]\n",
    "df_test = df_test[~(df_test['decline_type'].isin(exclude_decline_types))]\n",
    "\n",
    "#Exclude some data\n",
    "df_train = df_train[~(df_train['payment_service_id'] == 'paypalExpress')]\n",
    "df_eval = df_eval[~(df_eval['payment_service_id'] == 'paypalExpress')]\n",
    "df_test = df_test[~(df_test['payment_service_id'] == 'paypalExpress')]\n",
    "\n",
    "df_train = df_train[~(df_train['bin'] == 'nan')]\n",
    "df_eval = df_eval[~(df_eval['bin'] == 'nan')]\n",
    "df_test = df_test[~(df_test['bin'] == 'nan')]\n",
    "\n",
    "df_train = df_train[~(df_train['cc_expiration_date'] == 'nan')]\n",
    "df_eval = df_eval[~(df_eval['cc_expiration_date'] == 'nan')]\n",
    "df_test = df_test[~(df_test['cc_expiration_date'] == 'nan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_bin_profile is NONE\n",
      "self.features_encoded: ['week_of_month', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import src.web.preprocessing\n",
    "from src.web.preprocessing import PreProcessing\n",
    "from src.web.train_util import *\n",
    "from importlib import import_module\n",
    "import sys\n",
    "\n",
    "classifier = CatBoostClassifier\n",
    "\n",
    "cat_features_len = len(FEATURES_CAT) + len(FEATURES_CAT_ENCODED) + len (FEATURES_GROUPED)\n",
    "input_data = df_train #retry_success[~retry_success[\"subscription_id\"].isin(duplicate_subs)]\n",
    "scale_pos_weight = (input_data[LABEL].value_counts(normalize=True)[0.0] / input_data[LABEL].value_counts(normalize=True)[1.0] ) + 0.5\n",
    "\n",
    "features_dict['use_cat_encoder'] = False\n",
    "_preProcessor = PreProcessing(None).fit(input_data, input_data['success'], features_dict=features_dict)            \n",
    "_x_eval = _preProcessor.transform(df_eval)\n",
    "_y_eval = df_eval[\"success\"]\n",
    "\n",
    "alg_name = 'catboostclassifier'\n",
    "\n",
    "tuned_parameters = {}\n",
    "\n",
    "best_parameters = {\n",
    "              'depth': 7,\n",
    "              'iterations': 1000,\n",
    "              'random_seed': 8,\n",
    "              'scale_pos_weight': scale_pos_weight,\n",
    "              'subsample': 0.3,\n",
    "              'bagging_temperature': 3.0,\n",
    "              'rsm': 0.35,\n",
    "              'eval_metric': 'AUC',\n",
    "              'early_stopping_rounds': 200,\n",
    "              'l2_leaf_reg': 10.9,\n",
    "              'random_strength': 3.0\n",
    "              }\n",
    "\n",
    "#               'border_count': 260,\n",
    "    \n",
    "model_file = ''\n",
    "model_id = 'ML-TOD-1'\n",
    "version = get_latest_version(model_id, MODEL_TYPE) + 1\n",
    "model_name = model_id + '.' + str(version)\n",
    "\n",
    "# features_dict['eval_metric'] = 'auc'\n",
    "# features_dict['early_stopping_rounds'] = None #1600\n",
    "# features_dict['eval_set'] = None #Pool(_x_eval, _y_eval) \n",
    "\n",
    "cat_features = list(range(0,cat_features_len))\n",
    "\n",
    "fit_params = {\n",
    "    f\"{alg_name}__verbose\": True,\n",
    "    f\"{alg_name}__cat_features\": cat_features,\n",
    "    f\"{alg_name}__plot\": True,\n",
    "    f\"{alg_name}__eval_set\": Pool(_x_eval, _y_eval, cat_features)\n",
    "}\n",
    "\n",
    "\n",
    "features_dict['fit_params'] = fit_params\n",
    "\n",
    "\n",
    "clf, result_d = build_and_train(input_data, classifier, tuned_parameters, alg_name, model_file, best_param=best_parameters, features_dict=features_dict, test_data=df_test)\n",
    "model_file, model_file_name = write_model(clf, model_name)\n",
    "print(\"model_file is generated: \", model_file)\n",
    "print(\"result_dict: \", result_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "http://nexus-master.digitalriverws.net/nexus/repository/foundationreleases/com/digitalriver/prediction-service/ML-BR/1.190/ML-BR-1.190.pkl\n",
      "[com.digitalriver.prediction-service:ML-BR:1.190::pkl]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following files were uploaded to repository foundationreleases\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "http://nexus-master.digitalriverws.net/nexus/repository/foundationreleases/com/digitalriver/prediction-service/ML-BR/1_190_preprocessing/ML-BR-1_190_preprocessing.py\n",
      "[com.digitalriver.prediction-service:ML-BR:1_190_preprocessing::py]\n",
      "Model ML-BR-1 version 190 is inserted into model repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following files were uploaded to repository foundationreleases\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Upload model to Nexus repo and insert the model info into Cassandra table\"\"\"\n",
    "import json\n",
    "\n",
    "# start_date = '2018-08-01'\n",
    "start_date = '2019-01-01'\n",
    "# start_date = '2019-03-01'\n",
    "\n",
    "# end_date = '2019-04-30'\n",
    "end_date = '2019-07-31'\n",
    "\n",
    "try:\n",
    "    repo_path = upload_artifact(model_file_name)\n",
    "    preprocess_repo_path = handle_preprocessing_file(model_id, version)\n",
    "    size_desc = str(\", original size: %s (fail: %s, success: %s), balanced_size: %s\" % (original_size, fail_size, success_size, original_size))\n",
    "    desc = '{}_{}_for_calendar retry model,  eval_metric= auc, with no date_increment, no payment amount and bin profile). {}'.format(start_date, end_date, size_desc)\n",
    "    hyper_params = result_d.pop('hyper_params', None)\n",
    "    extended_att = {\"preprocess_repo_path\": preprocess_repo_path, \"input_features\": INPUT_FEATURES}\n",
    "    insert_model_info(model_id, version, repo_path, desc=desc, model_type=MODEL_TYPE,eval_metrics=json.dumps(result_d), \n",
    "                      hyper_parameter=json.dumps(hyper_params), extended_att=json.dumps(extended_att), features_dict=features_dict, algorithm='CatBoostClassifier')\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(str(ex))\n",
    "    if not hyper_params:\n",
    "        result_d['hyper_params'] = hyper_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
