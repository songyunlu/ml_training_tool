{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dependencies\n",
    "* install anaconda is recommended\n",
    "\n",
    "```\n",
    "cassandra-driver          3.11.0                   py35_1    conda-forge\n",
    "pandas                    0.19.1              np111py35_0\n",
    "scikit-learn              0.18.1              np111py35_0\n",
    "scipy                     0.18.1              np111py35_0\n",
    "matplotlib                2.0.0               np111py35_0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# import training as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model File Directory\n",
    "WORK_DIR = '/opt/docker/workspace/ml/'\n",
    "\n",
    "#Label Keys\n",
    "LABEL = \"success\"\n",
    "\n",
    "MODEL_TYPE = \"ML-BR\"\n",
    "\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2018-03-31'\n",
    "site_ids = ['adbecnn', 'adbehap']  #['avast', 'kasperus', 'mcafeeus', 'mfeap', 'mfeeu']\n",
    "desc = str('%s_%s_%s' % (start_date, end_date, \"_\".join(site_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cassandra_endpoint = '10.224.12.32'\n",
    "cluster = Cluster([cassandra_endpoint])\n",
    "session = cluster.connect('subs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "import calendar, datetime\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Convert from str to datetime\n",
    "def to_date(datestr):\n",
    "    struct = time.strptime(datestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date = datetime.date(struct.tm_year,struct.tm_mon,struct.tm_mday)\n",
    "    return date\n",
    "\n",
    "#Get difference between d2 and d1 in days.\n",
    "def days_between(d1, d2):\n",
    "    d1 = to_date(d1)\n",
    "    d2 = to_date(d2)\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "#Determines the week (number) of the month\n",
    "def week_of_month(datestr):\n",
    "    date = to_date(datestr)\n",
    "    #Calendar object. 6 = Start on Sunday, 0 = Start on Monday\n",
    "    cal_object = calendar.Calendar(6)\n",
    "    month_calendar_dates = cal_object.itermonthdates(date.year,date.month)\n",
    "\n",
    "    day_of_week = 1\n",
    "    week_number = 1\n",
    "\n",
    "    for day in month_calendar_dates:\n",
    "        #add a week and reset day of week\n",
    "        if day_of_week > 7:\n",
    "            week_number += 1\n",
    "            day_of_week = 1\n",
    "\n",
    "        if date == day:\n",
    "            break\n",
    "        else:\n",
    "            day_of_week += 1\n",
    "\n",
    "    return str(week_number)\n",
    "\n",
    "def to_weekday(datestr):\n",
    "    struct = time.strptime(datestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date = datetime.date(struct.tm_year,struct.tm_mon,struct.tm_mday)\n",
    "    return date.isoweekday()\n",
    "\n",
    "def daterange(date1_str, date2_str):\n",
    "    start_date = to_date(date1_str + ' 00:00:00')\n",
    "    end_date = to_date(date2_str + ' 00:00:00')\n",
    "    if start_date > end_date:\n",
    "        raise ValueError('start date cannot be after the end date.')\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield str(start_date + timedelta(n))\n",
    "\n",
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom Pre-Processing estimator for SRS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Regular transform() that is a help for training, validation & testing datasets\n",
    "           (NOTE: The operations performed here are the ones that we did prior to this cell)\n",
    "        \"\"\"\n",
    "        # Consolidated feature processing\n",
    "        \n",
    "        df['week_of_month'] = df['transaction_date_in_string'].apply(week_of_month)\n",
    "        df['day_of_week'] =  df['transaction_date_in_string'].apply(to_weekday)\n",
    "        # one hot for categorical feature ###\n",
    "#         df_encoded = pd.get_dummies(df[FEATURES].astype(str), prefix=FEATURES)\n",
    "#         df_encoded = pd.DataFrame(df_encoded, columns=self.features_list).fillna(0)\n",
    "\n",
    "        df_encoded  = pd.DataFrame(columns=self.features_list)\n",
    "        for k, v in self.encoders.items():\n",
    "            if df[k].dtype == 'float64':\n",
    "                df[k] = df[k].fillna(-1).astype(int)\n",
    "            \n",
    "            df_encoded[k] = v.transform(df[k].astype(str).str.lower().str.replace(' ',''))\n",
    "        \n",
    "        #Num processing\n",
    "        df_num = df[self.FEATURES_NUM].astype(float)\n",
    "        df_num = self.scaler.transform(df_num)\n",
    "\n",
    "        df_encoded[self.FEATURES_NUM] = df_num\n",
    "\n",
    "        return df_encoded.as_matrix()\n",
    "\n",
    "    def fit(self, df, y=None, features_dict={}, **fit_params):\n",
    "        \n",
    "        print('features_dict: ', features_dict)\n",
    "        self.FEATURES_CAT = features_dict['FEATURES_CAT']\n",
    "        self.FEATURES_NUM = features_dict['FEATURES_NUM']\n",
    "        self.FEATURES_ENCODED = features_dict['FEATURES_ENCODED']\n",
    "        self.FEATURES = self.FEATURES_CAT + self.FEATURES_ENCODED\n",
    "        \n",
    "        df['week_of_month'] = df['transaction_date_in_string'].apply(week_of_month)\n",
    "        df['day_of_week'] =  df['transaction_date_in_string'].apply(to_weekday)\n",
    "\n",
    "        \n",
    "        # one hot for categorical feature ###\n",
    "#         self.features_list = list(pd.get_dummies(df[FEATURES].astype(str), prefix=FEATURES).columns.values) + FEATURES_NUM\n",
    "\n",
    "        self.features_list = self.FEATURES + self.FEATURES_NUM\n",
    "        print(\"self.features_list: \", self.features_list)\n",
    "        feature_encoders = {}\n",
    "        for f in self.FEATURES:\n",
    "            if df[f].dtype == 'float64':\n",
    "                df[f] = df[f].fillna(-1).astype(int)\n",
    "                \n",
    "            encoder = SafeLabelEncoder().fit(df[f].astype(str).str.lower().str.replace(' ',''))\n",
    "            feature_encoders[f] = encoder\n",
    "            \n",
    "        self.encoders = feature_encoders  \n",
    "\n",
    "        #Fit a scaler\n",
    "        df_num = df[self.FEATURES_NUM].astype(float)\n",
    "        self.scaler = preprocessing.StandardScaler().fit(df_num)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM cpg_transaction where received_date = '20180101' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180102' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180103' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180104' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180105' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180106' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180107' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180108' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180109' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180110' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180111' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180112' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180113' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180114' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180115' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180116' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180117' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180118' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180119' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180120' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180121' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180122' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180123' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180124' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180125' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180126' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180127' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180128' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180129' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180130' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180131' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180201' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180202' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180203' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180204' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180205' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180206' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180207' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180208' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180209' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180210' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180211' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180212' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180213' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180214' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180215' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180216' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180217' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180218' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180219' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180220' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180221' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180222' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180223' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180224' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180225' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180226' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180227' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180228' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180301' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180302' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180303' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180304' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180305' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180306' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180307' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180308' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180309' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180310' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180311' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180312' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180313' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180314' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180315' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180316' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180317' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180318' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180319' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180320' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180321' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180322' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180323' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180324' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180325' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180326' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180327' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180328' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180329' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180330' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180331' and site_id = 'adbecnn'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180101' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180102' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180103' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180104' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180105' and site_id = 'adbehap'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM cpg_transaction where received_date = '20180106' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180107' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180108' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180109' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180110' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180111' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180112' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180113' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180114' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180115' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180116' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180117' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180118' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180119' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180120' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180121' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180122' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180123' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180124' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180125' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180126' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180127' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180128' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180129' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180130' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180131' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180201' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180202' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180203' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180204' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180205' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180206' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180207' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180208' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180209' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180210' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180211' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180212' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180213' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180214' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180215' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180216' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180217' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180218' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180219' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180220' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180221' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180222' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180223' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180224' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180225' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180226' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180227' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180228' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180301' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180302' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180303' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180304' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180305' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180306' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180307' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180308' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180309' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180310' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180311' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180312' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180313' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180314' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180315' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180316' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180317' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180318' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180319' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180320' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180321' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180322' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180323' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180324' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180325' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180326' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180327' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180328' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180329' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180330' and site_id = 'adbehap'\n",
      "SELECT * FROM cpg_transaction where received_date = '20180331' and site_id = 'adbehap'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(778783, 47)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def get_site_date_cartesian_products(site_ids, dates):\n",
    "    return itertools.product(*[site_ids, dates])\n",
    "\n",
    "def query_raw_data(start, end, site_ids):\n",
    "    \"\"\" Queries cpg raw data based on the given start, end date and list of site_ids \"\"\"\n",
    "    \n",
    "    _result = pd.DataFrame()\n",
    "    query = \"SELECT * FROM cpg_transaction where received_date = '%s' and site_id = '%s'\"\n",
    "    dates = daterange(start, end)\n",
    "    for site_id, date in get_site_date_cartesian_products(site_ids, dates):\n",
    "        query_by_date = str(query % (date.replace('-',''), site_id))\n",
    "        print(query_by_date)\n",
    "        _result = pd.concat([_result, pd.DataFrame(list(session.execute(query_by_date)))] )\n",
    "    \n",
    "    return _result\n",
    "\n",
    "\"\"\" Load the raw data into df_cs_original \"\"\"\n",
    "df_cs_original = query_raw_data(start_date, end_date, site_ids)\n",
    "csv_name = str('df_cs_original_%s_%s_%s.csv' % (start_date, end_date, \"_\".join(site_ids)))\n",
    "df_cs_original.to_csv(WORK_DIR + csv_name)\n",
    "\n",
    "df_cs_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = str('df_cs_original_%s_%s_%s.csv' % (start_date, end_date, \"_\".join(site_ids)))\n",
    "df_cs_original.to_csv(WORK_DIR + csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adbehap    775326\n",
       "adbecnn      3457\n",
       "Name: site_id, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cs_original['site_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263814, 48)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get only data that failed at first attempt\n",
    "\n",
    "df_nl = df_cs_original\n",
    "# Add 'success' as column\n",
    "df_nl['success'] = df_nl['new_status'].map({'Completed':1,'Declined':0, 'Failed':0, 'Reversed':1})\n",
    "# Remove renewal data that success at first attempt\n",
    "df_nl = df_nl[~((df_nl['renew_att_num'] == '1') & (df_nl['success'] == 1))]\n",
    "df_nl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "msg_group = { 'declined' : 'decline', \n",
    "             'do_not_honor' : 'do not honor', \n",
    "             'txn_refused' : 'refuse', \n",
    "             'attempt_lower_amount' : 'lower amount',\n",
    "            'Insufficient Funds' : 'insufficient',\n",
    "            'not_allowed' : 'not allowed',\n",
    "            'correct_cc_retry' : 'correct card',\n",
    "            'invalid_cc' : 'invalid card',\n",
    "            'lost_stolen' : 'lost or stolen',\n",
    "            'invalid_account' : 'invalid account',\n",
    "            'do_not_try' : 'do not try',\n",
    "            'expired_card' : 'expired',\n",
    "            'pickup_card' : 'pick',\n",
    "            'blocked_first_used' : 'blocked',\n",
    "            'invalid_txn' : 'invalid trans',\n",
    "            'restricted_card' : 'restricted',\n",
    "            'not_permitted' : 'not permitted',\n",
    "            'expired card' : 'expired card',\n",
    "            'unable to determine format' : 'determine format',\n",
    "            'system error' : 'error'\n",
    "            }\n",
    "\n",
    "def group_response_msg(msg):\n",
    "    other = 'Base'\n",
    "    if isinstance(msg, str) == False:\n",
    "        return other\n",
    "    \n",
    "    msg_lower = msg.lower()\n",
    "    for key, val in msg_group.items():\n",
    "        if val in msg_lower:\n",
    "            return key\n",
    "        \n",
    "    return other   \n",
    "\n",
    "def merge_by_sub(group):\n",
    "\n",
    "    first = group[ group['renew_att_num'] == '1' ]\n",
    "    \n",
    "    if first.empty:\n",
    "        return pd.DataFrame({})\n",
    "    \n",
    "    last = group[ group['renew_att_num'] == group['renew_att_num'].max() ]\n",
    "        \n",
    "    first_attempt_date = first['transaction_date_in_string'].iloc[0]\n",
    "    last_attempt_date = last['transaction_date_in_string'].iloc[0]\n",
    "    \n",
    "    last['first_attempt_date'] = first_attempt_date\n",
    "    last['first_day_of_month'] = first['day_of_month'].iloc[0]\n",
    "    last['day_of_month'] = last['day_of_month'].astype(int)\n",
    "    last['first_transaction_hour'] = first['transaction_hour'].iloc[0]\n",
    "    last['first_response_code'] = first['response_code'].iloc[0]\n",
    "    \n",
    "    first_response_msg = first['response_message'].iloc[0]\n",
    "    last['first_response_message'] = first_response_msg  \n",
    "    last['first_response_group'] =   first['response_message'].apply(group_response_msg).iloc[0]\n",
    "    \n",
    "    last['first_decline_type'] = decline_type(first_response_msg)\n",
    "    last['days_between'] =  days_between(first_attempt_date, last_attempt_date)\n",
    "        \n",
    "    return last\n",
    "\n",
    "def decline_type(response_msg):\n",
    "    '''Converts to decline_type based on the given response_msg'''\n",
    "    dec_type = df_decline_type[df_decline_type['DECLINE_TEXT'] == response_msg]['DECLINE_TYPE']\n",
    "    if dec_type.empty or dec_type.iloc[0] == 'Base' :\n",
    "        return group_response_msg(response_msg)\n",
    "    else:\n",
    "        return dec_type.iloc[0]\n",
    "\n",
    "#Convert from str to datetime\n",
    "def to_date(datestr):\n",
    "    struct = time.strptime(datestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date = datetime.date(struct.tm_year,struct.tm_mon,struct.tm_mday)\n",
    "    return date\n",
    "\n",
    "#Get difference between d2 and d1 in days.\n",
    "def days_between(d1, d2):\n",
    "    d1 = to_date(d1)\n",
    "    d2 = to_date(d2)\n",
    "    return abs((d2 - d1).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decline_type = pd.read_csv(WORK_DIR + 'Decline_Type.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# merge time: 550.4875392913818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76643, 56)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group by subscription_id for failed data at first attempt\n",
    "time_start = time.time()\n",
    "df_nl = df_nl[~df_nl['renew_att_num'].isna()]\n",
    "df_subs_merged = pd.DataFrame()\n",
    "for site_id in site_ids:\n",
    "    df_subs = df_nl[df_nl['site_id'] == site_id].groupby(['subscription_id', 'subsegment_id'], sort=False)\n",
    "#     df_subs_merged = df_subs.apply(merge_by_sub)\n",
    "    df_subs_merged = pd.concat([df_subs_merged, df_subs.apply(merge_by_sub)], axis=0)\n",
    "    \n",
    "print(\"# merge time:\", time.time() - time_start)\n",
    "df_subs_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved into /opt/docker/workspace/ml/subs_retry_2018-01-01_2018-03-31_adbecnn_adbehap.csv\n"
     ]
    }
   ],
   "source": [
    "#Only take subscription that has at least two transactions that occured on different day\n",
    "retry_success = df_subs_merged[df_subs_merged['days_between'] > 0]\n",
    "\n",
    "# Drop duplicate columns\n",
    "retry_success = retry_success.drop(columns=['subscription_id', 'subsegment_id'])\n",
    "\n",
    "#Write merged subs to csv\n",
    "csv_name = str('subs_retry_%s_%s_%s.csv' % (start_date, end_date, \"_\".join(site_ids)))\n",
    "retry_success.to_csv(WORK_DIR + csv_name)\n",
    "print(\"Saved into \" + WORK_DIR + csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resampling, balancing classes ###\n",
    "def balancing_class(df_ml):\n",
    "    print(df_ml[LABEL].value_counts(normalize=True))\n",
    "    df_cls_0 = df_ml[df_ml[LABEL] == 0]\n",
    "    df_cls_1 = df_ml[df_ml[LABEL] == 1]\n",
    "\n",
    "    #over sampling\n",
    "#     df_cls_0_over = df_cls_0.sample(len(df_cls_1), replace=True)\n",
    "#     df_ml_bl = pd.concat([df_cls_0_over, df_cls_1], axis=0)\n",
    "    df_cls_1_over = df_cls_1.sample(len(df_cls_0), replace=True)\n",
    "    df_ml_bl = pd.concat([df_cls_1_over, df_cls_0], axis=0)\n",
    "\n",
    "#     #under sampling\n",
    "#     df_cls_1_under = df_cls_1.sample(len(df_cls_0), replace=True)\n",
    "#     df_ml_bl = pd.concat([df_cls_1_under, df_cls_0], axis=0)\n",
    "\n",
    "    print(df_ml_bl[LABEL].value_counts(normalize=True))\n",
    "    print(df_ml_bl.shape)\n",
    "    return df_ml_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    0.687066\n",
      "1.0    0.312934\n",
      "Name: success, dtype: float64\n",
      "0.0    0.5\n",
      "1.0    0.5\n",
      "Name: success, dtype: float64\n",
      "(92376, 54)\n",
      "Saved into /opt/docker/workspace/ml/subs_retry_balanced__2018-01-01_2018-03-31_adbecnn_adbehap.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92376, 54)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retry_success_balanced_all = balancing_class(retry_success)\n",
    "csv_name = str('subs_retry_balanced__%s_%s_%s.csv' % (start_date, end_date, \"_\".join(site_ids)))\n",
    "retry_success_balanced_all.to_csv(WORK_DIR + csv_name)\n",
    "print(\"Saved into \" + WORK_DIR + csv_name)\n",
    "retry_success_balanced_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92376"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_size = len(retry_success)\n",
    "balanced_size = len(retry_success_balanced_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import for training\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import column_or_1d\n",
    "from sklearn.preprocessing.label import _check_numpy_unicode_bug\n",
    "\n",
    "def _get_unseen():\n",
    "    \"\"\"Basically just a static method\n",
    "    instead of a class attribute to avoid\n",
    "    someone accidentally changing it.\"\"\"\n",
    "    return 99999\n",
    "\n",
    "class SafeLabelEncoder(LabelEncoder):\n",
    "    \"\"\"An extension of LabelEncoder that will\n",
    "    not throw an exception for unseen data, but will\n",
    "    instead return a default value of 99999\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : the classes that are encoded\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, y):\n",
    "        \"\"\"Perform encoding if already fit.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array_like, shape=(n_samples,)\n",
    "            The array to encode\n",
    "        Returns\n",
    "        -------\n",
    "        e : array_like, shape=(n_samples,)\n",
    "            The encoded array\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'classes_')\n",
    "        y = column_or_1d(y, warn=True)\n",
    "\n",
    "        classes = np.unique(y)\n",
    "        _check_numpy_unicode_bug(classes)\n",
    "\n",
    "        # Check not too many:\n",
    "        unseen = _get_unseen()\n",
    "        if len(classes) >= unseen:\n",
    "            raise ValueError('Too many factor levels in feature. Max is %i' % unseen)\n",
    "\n",
    "        e = np.array([\n",
    "                         np.searchsorted(self.classes_, x) if x in self.classes_ else unseen\n",
    "                         for x in y\n",
    "                         ])\n",
    "\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "def print_accuracy_report(classifier, X, y, num_validations=5):\n",
    "    accuracy = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='accuracy', cv=num_validations)\n",
    "    print(\"CV Accuracy: \" + str(round(100*accuracy.mean(), 2)) + \"%\")\n",
    "\n",
    "    f1 = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='f1_weighted', cv=num_validations)\n",
    "    print(\"CV F1: \" + str(round(100*f1.mean(), 2)) + \"%\")\n",
    "\n",
    "    precision = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='precision_weighted', cv=num_validations)\n",
    "    print(\"CV Precision: \" + str(round(100*precision.mean(), 2)) + \"%\")\n",
    "\n",
    "    recall = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='recall_weighted', cv=num_validations)\n",
    "    print(\"CV Recall: \" + str(round(100*recall.mean(), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_CAT = ['card_brand', 'funding_source', 'card_category', 'card_class', \n",
    "                 'issuer_country', 'first_response_code', 'day_of_month',\n",
    "               'first_decline_type',  'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month']\n",
    "\n",
    "FEATURES_NUM = ['payment_amount_usd']\n",
    "FEATURES_ENCODED = [ 'week_of_month', 'day_of_week']\n",
    "FEATURES = FEATURES_CAT + FEATURES_ENCODED\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from spark_sklearn import GridSearchCV\n",
    "\n",
    "# from training import PreProcessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "FIELDS = FEATURES_CAT + FEATURES_NUM + ['transaction_date_in_string']\n",
    "\n",
    "features_dict = {'FEATURES_CAT': FEATURES_CAT, 'FEATURES_NUM':FEATURES_NUM, 'FEATURES_ENCODED':FEATURES_ENCODED}\n",
    "features_dict_key = 'preprocessing__features_dict'\n",
    "def display_feature_importance(pipe, model_name, df_features):\n",
    "    classifier = pipe.named_steps[model_name]\n",
    "    feature_importance = classifier.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    print(\"feature_importance column \",df_features.columns[sorted_idx])\n",
    "    print(\"feature_importance val \",feature_importance[sorted_idx])\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    pvals = feature_importance[sorted_idx]\n",
    "    pcols = df_features.columns[sorted_idx]\n",
    "    plt.figure(figsize=(8,12))\n",
    "    plt.barh(pos, pvals, align='center')\n",
    "    plt.yticks(pos, pcols)\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "\n",
    "def build_and_train(df, clf, param_grid, model_name, model_file = ''):\n",
    "    model_prefix = model_name + '__'\n",
    "    time_start = time.time()\n",
    "    df_X = df[FIELDS]\n",
    " \n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(df_X, df[LABEL], \\\n",
    "                                                        test_size=0.25, random_state=42)\n",
    "\n",
    "    pipe = make_pipeline(PreProcessing(), clf())\n",
    "\n",
    "    score = 'accuracy' #  ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        \n",
    "    pipe_param_grid = {model_prefix + k: v for k, v in param_grid.items()}\n",
    "    print(\"pipe_param_grid \", pipe_param_grid)\n",
    "    clf_gs = GridSearchCV(sc, pipe, pipe_param_grid, cv=3, scoring=score, n_jobs=-1, fit_params={features_dict_key: features_dict})\n",
    "#     clf_gs = GridSearchCV(pipe, pipe_param_grid, cv=5, scoring=score, n_jobs=-1, fit_params={features_dict_key: features_dict})\n",
    "    clf_gs.fit(x_train, y_train)\n",
    "\n",
    "    print(\"# Best parameters set found on development set:\")\n",
    "    print(clf_gs.best_params_)\n",
    "    print(\"# Grid scores on development set:\")\n",
    "    means = clf_gs.cv_results_['mean_test_score']\n",
    "    stds = clf_gs.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf_gs.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "\n",
    "    print(\"x_train\", x_train.shape)\n",
    "    print(\"x_test\", x_test.shape)\n",
    "\n",
    "    best_parameters = clf_gs.best_params_\n",
    "    best_parameters = {k.replace(model_prefix,''): v for k, v in best_parameters.items()}\n",
    "    print(\"best_parameters \", best_parameters)\n",
    "    pipe = make_pipeline(PreProcessing(), clf(**best_parameters))\n",
    "    \n",
    "    if model_name == 'xgbclassifier':\n",
    "        print(\"training xgb ....... \")\n",
    "        if model_file == '':\n",
    "            pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict, xgbclassifier__eval_metric='error')\n",
    "        else:\n",
    "            print(\"Using model_file to train: \", model_file)\n",
    "            pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict, xgbclassifier__xgb_model=model_file, xgbclassifier__eval_metric='error')\n",
    "    else:        \n",
    "        pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict)\n",
    "        \n",
    "    preprocess = pipe.named_steps['preprocessing']\n",
    "\n",
    "        \n",
    "    y_pred_train = pipe.predict(x_train).round()\n",
    "    y_pred_test = pipe.predict(x_test).round()\n",
    "    \n",
    "    print(\"# training time:\", time.time() - time_start)\n",
    "\n",
    "    clf_d = DummyClassifier(strategy='most_frequent')\n",
    "    dummy_pipe = make_pipeline(PreProcessing(), clf_d)\n",
    "    dummy_pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict)\n",
    "    y_pred_test_dummy = dummy_pipe.predict(x_test).round()\n",
    "    print(\"accuracy_dummy:\", metrics.accuracy_score(y_test, y_pred_test_dummy))\n",
    "    print(\"training accuracy:\", metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"test accuracy:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "     \n",
    "#     print_accuracy_report(pipe, x_train, y_train, num_validations=3)\n",
    "    conf_mx = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"# confusion_matrix -  test:\\n\", conf_mx)\n",
    "    \n",
    "\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "MODEL_DIR = WORK_DIR + \"models\"\n",
    "\n",
    "\n",
    "def write_model(model, model_name, idx=None): \n",
    "    build_id = \"\" if idx is None else \"_\" + str(idx)\n",
    "    file_name = path.join(MODEL_DIR, '%s%s.pkl' % (model_name, build_id))\n",
    "    if not os.path.exists(os.path.dirname(file_name)):\n",
    "        os.makedirs(os.path.dirname(file_name))\n",
    "    file = joblib.dump(model, file_name)\n",
    "    return (file, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cassandra_endpoint = '10.224.12.32'\n",
    "cluster = Cluster([cassandra_endpoint])\n",
    "session = cluster.connect('subs')\n",
    "\n",
    "\n",
    "def insert_model_info(model_id, version, file_name, desc, model_type=MODEL_TYPE, algorithm='XGBClassifier', hyper_parameter=None, eval_metrics=None):\n",
    "#     file_name = model_id + '.' + str(version) + \".pkl\"\n",
    "    \n",
    "    session.execute(\n",
    "    \"\"\"\n",
    "    INSERT INTO ml_model_storage (model_type, model_id, version, features_cat, features_encoded, features_num, repo_path, description, creation_date, modification_date, algorithm, hyper_parameter, eval_metrics)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\",\n",
    "    (model_type, model_id, version, str(FEATURES_CAT), str(FEATURES_ENCODED), str(FEATURES_NUM), file_name, desc, datetime.datetime.utcnow(), datetime.datetime.utcnow(), algorithm, hyper_parameter, eval_metrics)\n",
    "        \n",
    "    )\n",
    "    print(\"Model %s version %d is inserted into model repo\" % (model_id, version))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_version(model_id, model_type=MODEL_TYPE):\n",
    "    \"\"\"Get latest version of the given model_id\"\"\"\n",
    "    latest_version_query = \"select version from subs.ml_model_storage  where model_type = '%s' and model_id = '%s' limit 1\" % (model_type, model_id)\n",
    "    \n",
    "    query_result = session.execute(latest_version_query).one()\n",
    "    if query_result is None:\n",
    "        latest_version = 1\n",
    "    else:\n",
    "        latest_version = query_result.version\n",
    "    \n",
    "    return latest_version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_URL = 'http://nexus.digitalriverws.net/nexus'\n",
    "REPO_USER = 'deployment'\n",
    "REPO_PWD = 'deployment123'\n",
    "REPO_ID = 'foundationreleases'\n",
    "REPO_GROUP = 'com.digitalriver.srs-ml'\n",
    "\n",
    "import repositorytools\n",
    "\n",
    "def upload_artifact(file_path):\n",
    "    \"\"\"Upload artifact to Nexus Repo\"\"\"\n",
    "    artifact = repositorytools.LocalArtifact(local_path=file_path, group=REPO_GROUP)\n",
    "\n",
    "    client = repositorytools.repository_client_factory(repository_url=REPOSITORY_URL, user=REPO_USER, password=REPO_PWD)\n",
    "    remote_artifacts = client.upload_artifacts(local_artifacts=[artifact], repo_id=REPO_ID, use_direct_put=True)\n",
    "    print(remote_artifacts)\n",
    "    return str(remote_artifacts[0]) if remote_artifacts else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "pipe_param_grid  {'xgbclassifier__objective': ['binary:logistic'], 'xgbclassifier__learning_rate': [0.2], 'xgbclassifier__max_depth': [10], 'xgbclassifier__min_child_weight': [11], 'xgbclassifier__silent': [0], 'xgbclassifier__subsample': [0.5], 'xgbclassifier__colsample_bytree': [0.7], 'xgbclassifier__n_estimators': [500, 1000], 'xgbclassifier__missing': [-999], 'xgbclassifier__seed': [1337]}\n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'issuer_country', 'first_response_code', 'day_of_month', 'first_decline_type', 'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'issuer_country', 'first_response_code', 'day_of_month', 'first_decline_type', 'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Best parameters set found on development set:\n",
      "{'xgbclassifier__colsample_bytree': 0.7, 'xgbclassifier__learning_rate': 0.2, 'xgbclassifier__max_depth': 10, 'xgbclassifier__min_child_weight': 11, 'xgbclassifier__missing': -999, 'xgbclassifier__n_estimators': 1000, 'xgbclassifier__objective': 'binary:logistic', 'xgbclassifier__seed': 1337, 'xgbclassifier__silent': 0, 'xgbclassifier__subsample': 0.5}\n",
      "# Grid scores on development set:\n",
      "0.901 (+/-0.003) for {'xgbclassifier__colsample_bytree': 0.7, 'xgbclassifier__learning_rate': 0.2, 'xgbclassifier__max_depth': 10, 'xgbclassifier__min_child_weight': 11, 'xgbclassifier__missing': -999, 'xgbclassifier__n_estimators': 500, 'xgbclassifier__objective': 'binary:logistic', 'xgbclassifier__seed': 1337, 'xgbclassifier__silent': 0, 'xgbclassifier__subsample': 0.5}\n",
      "0.907 (+/-0.001) for {'xgbclassifier__colsample_bytree': 0.7, 'xgbclassifier__learning_rate': 0.2, 'xgbclassifier__max_depth': 10, 'xgbclassifier__min_child_weight': 11, 'xgbclassifier__missing': -999, 'xgbclassifier__n_estimators': 1000, 'xgbclassifier__objective': 'binary:logistic', 'xgbclassifier__seed': 1337, 'xgbclassifier__silent': 0, 'xgbclassifier__subsample': 0.5}\n",
      "x_train (69282, 17)\n",
      "x_test (23094, 15)\n",
      "best_parameters  {'colsample_bytree': 0.7, 'learning_rate': 0.2, 'max_depth': 10, 'min_child_weight': 11, 'missing': -999, 'n_estimators': 1000, 'objective': 'binary:logistic', 'seed': 1337, 'silent': 0, 'subsample': 0.5}\n",
      "training xgb ....... \n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'issuer_country', 'first_response_code', 'day_of_month', 'first_decline_type', 'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'issuer_country', 'first_response_code', 'day_of_month', 'first_decline_type', 'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training time: 573.465653181076\n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'issuer_country', 'first_response_code', 'day_of_month', 'first_decline_type', 'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'issuer_country', 'first_response_code', 'day_of_month', 'first_decline_type', 'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_dummy: 0.49636269160820995\n",
      "training accuracy: 0.9865477324557606\n",
      "test accuracy: 0.9263878063566294\n",
      "# confusion_matrix -  test:\n",
      " [[10356  1107]\n",
      " [  593 11038]]\n",
      "model_file is generated:  ['/opt/docker/workspace/ml/models/ML-BR-1.22.pkl']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "\n",
    "#XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "classifier = XGBClassifier\n",
    "\n",
    "tuned_parameters = {\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.2], #so called `eta` value\n",
    "              'max_depth': [10],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [0],\n",
    "              'subsample': [0.5],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500, 1000], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "model_file = ''\n",
    "# for idx, df_2018_f in enumerate(df_2018_filtered):\n",
    "#     print(\"Iteration: \", idx)\n",
    "model_id = 'ML-BR-1'\n",
    "version = get_latest_version(model_id) + 1\n",
    "model_name = model_id + '.' + str(version)\n",
    "xgb_clf = build_and_train(retry_success_balanced_all, classifier, tuned_parameters, 'xgbclassifier', model_file)\n",
    "model_file, model_file_name = write_model(xgb_clf, model_name)\n",
    "print(\"model_file is generated: \", model_file)\n",
    "\n",
    "# insert_model_info(model_id, version, model_file_name, desc=\"For testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://nexus.digitalriverws.net/nexus/content/repositories/foundationreleases/com/digitalriver/srs-ml/ML-BR/1.22/ML-BR-1.22.pkl\n",
      "[com.digitalriver.srs-ml:ML-BR:1.22::pkl]\n",
      "Model ML-BR-1 version 22 is inserted into model repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following files were uploaded to repository foundationreleases\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Upload model to Nexus repo and insert the model info into Cassandra table\"\"\"\n",
    "\n",
    "repo_path = upload_artifact(model_file_name)\n",
    "size_desc = str(\", original size: %s, balanced_size: %s\" % (original_size, balanced_size))\n",
    "desc = desc + size_desc\n",
    "insert_model_info(model_id, version, repo_path, desc=desc, eval_metrics=\"training accuracy: 0.9793473731754407, test accuracy: 0.9655285933757646\", \n",
    "                  hyper_parameter=\"{'colsample_bytree': 0.7, 'learning_rate': 0.2, 'max_depth': 10, 'min_child_weight': 11, 'missing': -999, 'n_estimators': 1000, 'objective': 'binary:logistic', 'seed': 1337, 'silent': 0, 'subsample': 0.5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
