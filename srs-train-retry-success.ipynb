{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dependencies\n",
    "* install anaconda is recommended\n",
    "\n",
    "```\n",
    "cassandra-driver          3.11.0                   py35_1    conda-forge\n",
    "pandas                    0.19.1              np111py35_0\n",
    "scikit-learn              0.18.1              np111py35_0\n",
    "scipy                     0.18.1              np111py35_0\n",
    "matplotlib                2.0.0               np111py35_0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# import training as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model File Directory\n",
    "WORK_DIR = '/drhdev/workspace/ml/'\n",
    "\n",
    "#Label Keys\n",
    "LABEL = \"success\"\n",
    "\n",
    "MODEL_TYPE = \"ML-BR\"\n",
    "\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2018-03-30'\n",
    "site_ids = ['avast']\n",
    "# site_ids = ['avast', 'kasperus', 'adbehap', 'adbehbr', 'adbehkr', 'mcafeeus', 'mfeap', 'mfeeu']\n",
    "desc = str('%s_%s_%s' % (start_date, end_date, \"_\".join(site_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cassandra_endpoint = '10.224.12.32'\n",
    "cluster = Cluster([cassandra_endpoint])\n",
    "# session = cluster.connect('subs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "import calendar, datetime\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Convert from str to datetime\n",
    "def to_date(datestr):\n",
    "    struct = time.strptime(datestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date = datetime.date(struct.tm_year,struct.tm_mon,struct.tm_mday)\n",
    "    return date\n",
    "\n",
    "#Get difference between d2 and d1 in days.\n",
    "def days_between(d1, d2):\n",
    "    d1 = to_date(d1)\n",
    "    d2 = to_date(d2)\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "#Determines the week (number) of the month\n",
    "def week_of_month(datestr):\n",
    "    date = to_date(datestr)\n",
    "    #Calendar object. 6 = Start on Sunday, 0 = Start on Monday\n",
    "    cal_object = calendar.Calendar(6)\n",
    "    month_calendar_dates = cal_object.itermonthdates(date.year,date.month)\n",
    "\n",
    "    day_of_week = 1\n",
    "    week_number = 1\n",
    "\n",
    "    for day in month_calendar_dates:\n",
    "        #add a week and reset day of week\n",
    "        if day_of_week > 7:\n",
    "            week_number += 1\n",
    "            day_of_week = 1\n",
    "\n",
    "        if date == day:\n",
    "            break\n",
    "        else:\n",
    "            day_of_week += 1\n",
    "\n",
    "    return str(week_number)\n",
    "\n",
    "def to_weekday(datestr):\n",
    "    struct = time.strptime(datestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date = datetime.date(struct.tm_year,struct.tm_mon,struct.tm_mday)\n",
    "    return date.isoweekday()\n",
    "\n",
    "def daterange(date1_str, date2_str):\n",
    "    start_date = to_date(date1_str + ' 00:00:00')\n",
    "    end_date = to_date(date2_str + ' 00:00:00')\n",
    "    if start_date > end_date:\n",
    "        raise ValueError('start date cannot be after the end date.')\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield str(start_date + timedelta(n))\n",
    "\n",
    "# class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "#     \"\"\"Custom Pre-Processing estimator for SRS\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def transform(self, df):\n",
    "#         \"\"\"Regular transform() that is a help for training, validation & testing datasets\n",
    "#            (NOTE: The operations performed here are the ones that we did prior to this cell)\n",
    "#         \"\"\"\n",
    "#         # Consolidated feature processing\n",
    "        \n",
    "#         df['week_of_month'] = df['transaction_date_in_string'].apply(week_of_month)\n",
    "#         df['day_of_week'] =  df['transaction_date_in_string'].apply(to_weekday)\n",
    "#         # one hot for categorical feature ###\n",
    "# #         df_encoded = pd.get_dummies(df[FEATURES].astype(str), prefix=FEATURES)\n",
    "# #         df_encoded = pd.DataFrame(df_encoded, columns=self.features_list).fillna(0)\n",
    "\n",
    "#         df_encoded  = pd.DataFrame(columns=self.features_list)\n",
    "#         for k, v in self.encoders.items():\n",
    "#             if df[k].dtype == 'float64':\n",
    "#                 df[k] = df[k].fillna(-1).astype(int)\n",
    "            \n",
    "#             df_encoded[k] = v.transform(df[k].astype(str).str.lower().str.replace(' ',''))\n",
    "        \n",
    "#         #Num processing\n",
    "#         df_num = df[self.FEATURES_NUM].astype(float)\n",
    "#         df_num = self.scaler.transform(df_num)\n",
    "\n",
    "#         df_encoded[self.FEATURES_NUM] = df_num\n",
    "\n",
    "#         return df_encoded.as_matrix()\n",
    "\n",
    "#     def fit(self, df, y=None, features_dict={}, **fit_params):\n",
    "        \n",
    "#         print('features_dict: ', features_dict)\n",
    "#         self.FEATURES_CAT = features_dict['FEATURES_CAT']\n",
    "#         self.FEATURES_NUM = features_dict['FEATURES_NUM']\n",
    "#         self.FEATURES_ENCODED = features_dict['FEATURES_ENCODED']\n",
    "#         self.FEATURES = self.FEATURES_CAT + self.FEATURES_ENCODED\n",
    "        \n",
    "#         df['week_of_month'] = df['transaction_date_in_string'].apply(week_of_month)\n",
    "#         df['day_of_week'] =  df['transaction_date_in_string'].apply(to_weekday)\n",
    "\n",
    "        \n",
    "#         # one hot for categorical feature ###\n",
    "# #         self.features_list = list(pd.get_dummies(df[FEATURES].astype(str), prefix=FEATURES).columns.values) + FEATURES_NUM\n",
    "\n",
    "#         self.features_list = self.FEATURES + self.FEATURES_NUM\n",
    "#         print(\"self.features_list: \", self.features_list)\n",
    "#         feature_encoders = {}\n",
    "#         for f in self.FEATURES:\n",
    "#             if df[f].dtype == 'float64':\n",
    "#                 df[f] = df[f].fillna(-1).astype(int)\n",
    "                \n",
    "#             encoder = SafeLabelEncoder().fit(df[f].astype(str).str.lower().str.replace(' ',''))\n",
    "#             feature_encoders[f] = encoder\n",
    "            \n",
    "#         self.encoders = feature_encoders  \n",
    "\n",
    "#         #Fit a scaler\n",
    "#         df_num = df[self.FEATURES_NUM].astype(float)\n",
    "#         self.scaler = preprocessing.StandardScaler().fit(df_num)\n",
    "#         return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM cpg_transaction where received_date = '20180101' and site_id = 'avast'\n"
     ]
    },
    {
     "ename": "InvalidRequest",
     "evalue": "Error from server: code=2200 [Invalid query] message=\"unconfigured table cpg_transaction\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequest\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8f1fcc72b6b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\"\"\" Load the raw data into df_cs_original \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdf_cs_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdf_cs_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-8f1fcc72b6b9>\u001b[0m in \u001b[0;36mquery_raw_data\u001b[0;34m(start, end, site_ids)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_by_site\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\"\"\" Load the raw data into df_cs_original \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-8f1fcc72b6b9>\u001b[0m in \u001b[0;36mquery_by_site\u001b[0;34m(dates, site_ids)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mquery_by_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_by_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_by_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/drhdev/anaconda3/lib/python3.6/site-packages/cassandra/cluster.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mcassandra.cluster.Session.execute\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/drhdev/anaconda3/lib/python3.6/site-packages/cassandra/cluster.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mcassandra.cluster.ResponseFuture.result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mInvalidRequest\u001b[0m: Error from server: code=2200 [Invalid query] message=\"unconfigured table cpg_transaction\""
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "subs_session = cluster.connect('subs')\n",
    "df_cs_original = None\n",
    "def get_site_date_cartesian_products(site_ids, dates):\n",
    "    return itertools.product(*[site_ids, dates])\n",
    "\n",
    "def query_all_sites(dates):\n",
    "    \"\"\" Queries cpg raw data based on the given start, end date and all site_ids \"\"\"\n",
    "    query = \"SELECT * FROM cpg_transaction where received_date = '%s'\"\n",
    "    _result = pd.DataFrame()\n",
    "    for date in dates:\n",
    "        query_by_date = str(query % date.replace('-',''))\n",
    "        print(query_by_date)\n",
    "        _result = pd.concat([_result, pd.DataFrame(list(subs_session.execute(query_by_date)))] )\n",
    "        \n",
    "    return _result\n",
    "\n",
    "def query_by_site(dates, site_ids):\n",
    "    \"\"\" Queries cpg raw data based on the given start, end date and list of site_ids \"\"\"\n",
    "    query = \"SELECT * FROM cpg_transaction where received_date = '%s' and site_id = '%s'\"    \n",
    "    _result = pd.DataFrame()\n",
    "    for site_id, date in get_site_date_cartesian_products(site_ids, dates):\n",
    "        query_by_date = str(query % (date.replace('-',''), site_id))\n",
    "        print(query_by_date)\n",
    "        _result = pd.concat([_result, pd.DataFrame(list(subs_session.execute(query_by_date)))] )\n",
    "    \n",
    "    return _result\n",
    "\n",
    "def query_raw_data(start, end, site_ids):\n",
    "    \"\"\" Queries cpg raw data based on the given start, end date and list of site_ids \"\"\"\n",
    "    \n",
    "#     _result = pd.DataFrame()\n",
    "    dates = daterange(start, end)\n",
    "    \n",
    "    if not site_ids:\n",
    "        return query_all_sites(dates)\n",
    "        \n",
    "    else:\n",
    "        return query_by_site(dates, site_ids)\n",
    "\n",
    "\"\"\" Load the raw data into df_cs_original \"\"\"\n",
    "df_cs_original = query_raw_data(start_date, end_date, site_ids)\n",
    "df_cs_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3398837, 47)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = str('df_cs_original_%s_%s_%s.csv' % (start_date, end_date, \"_\".join(site_ids)))\n",
    "csv_file_name = WORK_DIR + csv_name\n",
    "df_cs_original.to_csv(csv_file_name)\n",
    "\n",
    "csv_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (14,15,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (14,15,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# #Combine two df from csv\n",
    "# file_1 = 'df_cs_original_2018-01-01_2018-03-31_avast_kasperus_adbehap_adbehbr_adbehkr_mcafeeus_mfeap_mfeeu.csv'\n",
    "# df_cs_original_1 = pd.read_csv(WORK_DIR + file_1)\n",
    "\n",
    "# file_2 = 'df_cs_original_2018-04-01_2018-06-30_avast_kasperus_adbehap_adbehbr_adbehkr_mcafeeus_mfeap_mfeeu.csv'\n",
    "\n",
    "\n",
    "# df_cs_original = pd.concat([df_cs_original_1, pd.read_csv(WORK_DIR + file_2)], axis=0)\n",
    "# df_cs_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avast       3368772\n",
       "adbehap     1543648\n",
       "kasperus    1118450\n",
       "adbehkr      649196\n",
       "adbehbr      605376\n",
       "mfeeu        112575\n",
       "mfeap         17979\n",
       "mcafeeus        116\n",
       "Name: site_id, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cs_original['site_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>received_date</th>\n",
       "      <th>site_id</th>\n",
       "      <th>subscription_id</th>\n",
       "      <th>event_uuid</th>\n",
       "      <th>added_expiry_years</th>\n",
       "      <th>bank_code</th>\n",
       "      <th>bank_name</th>\n",
       "      <th>billing_country</th>\n",
       "      <th>bin</th>\n",
       "      <th>card_brand</th>\n",
       "      <th>card_category</th>\n",
       "      <th>card_class</th>\n",
       "      <th>card_usage</th>\n",
       "      <th>cc_expiration_date</th>\n",
       "      <th>cid</th>\n",
       "      <th>date_increment</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>exp_setting_id</th>\n",
       "      <th>funding_source</th>\n",
       "      <th>issuer_country</th>\n",
       "      <th>merchant_descriptor</th>\n",
       "      <th>merchant_number</th>\n",
       "      <th>mid_entity_code</th>\n",
       "      <th>new_status</th>\n",
       "      <th>order_entity_code</th>\n",
       "      <th>payment_amount</th>\n",
       "      <th>payment_amount_usd</th>\n",
       "      <th>payment_currency</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_service_id</th>\n",
       "      <th>renew_att_num</th>\n",
       "      <th>request_amount</th>\n",
       "      <th>request_amount_usd</th>\n",
       "      <th>request_currency</th>\n",
       "      <th>requisition_id</th>\n",
       "      <th>response_code</th>\n",
       "      <th>response_message</th>\n",
       "      <th>retry_rule_id</th>\n",
       "      <th>status</th>\n",
       "      <th>subsegment_id</th>\n",
       "      <th>time_of_day_rule_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_date_in_string</th>\n",
       "      <th>transaction_hour</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180101</td>\n",
       "      <td>avast</td>\n",
       "      <td>10025882901</td>\n",
       "      <td>4a2d05e6-36b8-491c-8d10-aa79ca6dc399</td>\n",
       "      <td>None</td>\n",
       "      <td>99EBEB6D41E6A44A</td>\n",
       "      <td>CAPITAL ONE BANK (USA), NATIONAL ASSOCIATION</td>\n",
       "      <td>US</td>\n",
       "      <td>517805</td>\n",
       "      <td>MasterCard</td>\n",
       "      <td>Platinum MasterCard Card</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>True credit (No PIN/Signature capability)</td>\n",
       "      <td>0920</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>Credit</td>\n",
       "      <td>US</td>\n",
       "      <td>DRI*AVAST Software</td>\n",
       "      <td>311009012882</td>\n",
       "      <td>DR_INC-ENTITY</td>\n",
       "      <td>Completed</td>\n",
       "      <td></td>\n",
       "      <td>85.59</td>\n",
       "      <td>85.59</td>\n",
       "      <td>USD</td>\n",
       "      <td>MasterCard</td>\n",
       "      <td>firstdata</td>\n",
       "      <td>2</td>\n",
       "      <td>85.59</td>\n",
       "      <td>85.59</td>\n",
       "      <td>USD</td>\n",
       "      <td>43984979000</td>\n",
       "      <td>100</td>\n",
       "      <td>00: Approved</td>\n",
       "      <td>None</td>\n",
       "      <td>New</td>\n",
       "      <td>13308769700</td>\n",
       "      <td></td>\n",
       "      <td>2018-01-01 07:45:45</td>\n",
       "      <td>2018-01-01 01:45:45</td>\n",
       "      <td>01</td>\n",
       "      <td>6333349913</td>\n",
       "      <td>Authorize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180101</td>\n",
       "      <td>avast</td>\n",
       "      <td>10036263201</td>\n",
       "      <td>e7510e07-0e46-44b0-a6e3-ff49047df3d5</td>\n",
       "      <td>None</td>\n",
       "      <td>3286758E8A3568CB</td>\n",
       "      <td>Navy Federal Credit Union</td>\n",
       "      <td>US</td>\n",
       "      <td>400022</td>\n",
       "      <td>Visa</td>\n",
       "      <td>Visa Classic</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Debit Hybrid (PIN and Signature)</td>\n",
       "      <td>0120</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>Debit</td>\n",
       "      <td>US</td>\n",
       "      <td>DRI*AVAST Software</td>\n",
       "      <td>311009012882</td>\n",
       "      <td>DR_INC-ENTITY</td>\n",
       "      <td>Completed</td>\n",
       "      <td></td>\n",
       "      <td>99.99</td>\n",
       "      <td>99.99</td>\n",
       "      <td>USD</td>\n",
       "      <td>Visa</td>\n",
       "      <td>firstdata</td>\n",
       "      <td>2</td>\n",
       "      <td>99.99</td>\n",
       "      <td>99.99</td>\n",
       "      <td>USD</td>\n",
       "      <td>43981753500</td>\n",
       "      <td>100</td>\n",
       "      <td>00: Approved</td>\n",
       "      <td>None</td>\n",
       "      <td>New</td>\n",
       "      <td>13325277000</td>\n",
       "      <td></td>\n",
       "      <td>2018-01-01 07:30:06</td>\n",
       "      <td>2018-01-01 01:30:06</td>\n",
       "      <td>01</td>\n",
       "      <td>6333297633</td>\n",
       "      <td>Authorize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180101</td>\n",
       "      <td>avast</td>\n",
       "      <td>10039189701</td>\n",
       "      <td>ba44bf00-d656-4515-9488-88024f0fa463</td>\n",
       "      <td>None</td>\n",
       "      <td>NON3DS</td>\n",
       "      <td>Banco de Credito del Peru</td>\n",
       "      <td>PE</td>\n",
       "      <td>455788</td>\n",
       "      <td>Visa</td>\n",
       "      <td>Visa Gold</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>NON USA Consumer Debit - No Pin Access</td>\n",
       "      <td>1220</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>Debit</td>\n",
       "      <td>PE</td>\n",
       "      <td>DRI*AVAST Software</td>\n",
       "      <td>1156962062</td>\n",
       "      <td>DR_IRELAND-ENTITY</td>\n",
       "      <td>Declined</td>\n",
       "      <td></td>\n",
       "      <td>179</td>\n",
       "      <td>55.223255</td>\n",
       "      <td>PEN</td>\n",
       "      <td>Visa</td>\n",
       "      <td>netgiro-seb</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>55.223255</td>\n",
       "      <td>PEN</td>\n",
       "      <td>43976743900</td>\n",
       "      <td>27001</td>\n",
       "      <td>Transaction refused[51] [000000] [] []</td>\n",
       "      <td>None</td>\n",
       "      <td>New</td>\n",
       "      <td>13331289800</td>\n",
       "      <td></td>\n",
       "      <td>2018-01-01 07:06:55</td>\n",
       "      <td>2018-01-01 01:06:55</td>\n",
       "      <td>01</td>\n",
       "      <td>6333065673</td>\n",
       "      <td>Authorize</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180101</td>\n",
       "      <td>avast</td>\n",
       "      <td>10039189701</td>\n",
       "      <td>f2d16301-9c3b-4637-9af0-7a4f0fd09a99</td>\n",
       "      <td>None</td>\n",
       "      <td>NON3DS</td>\n",
       "      <td>Banco de Credito del Peru</td>\n",
       "      <td>PE</td>\n",
       "      <td>455788</td>\n",
       "      <td>Visa</td>\n",
       "      <td>Visa Gold</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>NON USA Consumer Debit - No Pin Access</td>\n",
       "      <td>1220</td>\n",
       "      <td>RETRY_DECLINED.2</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>Debit</td>\n",
       "      <td>PE</td>\n",
       "      <td>DRI*AVAST Software</td>\n",
       "      <td>1411163460</td>\n",
       "      <td>DR_IRELAND-ENTITY</td>\n",
       "      <td>Declined</td>\n",
       "      <td></td>\n",
       "      <td>179</td>\n",
       "      <td>55.223255</td>\n",
       "      <td>PEN</td>\n",
       "      <td>Visa</td>\n",
       "      <td>netgiro-bms</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>55.223255</td>\n",
       "      <td>PEN</td>\n",
       "      <td>43976743900</td>\n",
       "      <td>27054</td>\n",
       "      <td>Attempt lower amount.[51] [] [000800] [000800]</td>\n",
       "      <td>None</td>\n",
       "      <td>New</td>\n",
       "      <td>13331289800</td>\n",
       "      <td></td>\n",
       "      <td>2018-01-01 07:06:57</td>\n",
       "      <td>2018-01-01 01:06:57</td>\n",
       "      <td>01</td>\n",
       "      <td>6333066053</td>\n",
       "      <td>Authorize</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180101</td>\n",
       "      <td>avast</td>\n",
       "      <td>10040113001</td>\n",
       "      <td>3dc16619-c991-4bce-8b54-389ef3eba8b8</td>\n",
       "      <td>None</td>\n",
       "      <td>7B8BE8370D67058F</td>\n",
       "      <td>J.S.C. Federal Credit Union</td>\n",
       "      <td>US</td>\n",
       "      <td>460753</td>\n",
       "      <td>Visa</td>\n",
       "      <td>Visa Classic</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Debit Hybrid (PIN and Signature)</td>\n",
       "      <td>0818</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>Debit</td>\n",
       "      <td>US</td>\n",
       "      <td>DRI*AVAST Software</td>\n",
       "      <td>311009012882</td>\n",
       "      <td>DR_INC-ENTITY</td>\n",
       "      <td>Completed</td>\n",
       "      <td></td>\n",
       "      <td>86.59</td>\n",
       "      <td>86.59</td>\n",
       "      <td>USD</td>\n",
       "      <td>Visa</td>\n",
       "      <td>firstdata</td>\n",
       "      <td>1</td>\n",
       "      <td>86.59</td>\n",
       "      <td>86.59</td>\n",
       "      <td>USD</td>\n",
       "      <td>43978651700</td>\n",
       "      <td>100</td>\n",
       "      <td>00: Approved</td>\n",
       "      <td>None</td>\n",
       "      <td>New</td>\n",
       "      <td>13332230000</td>\n",
       "      <td></td>\n",
       "      <td>2018-01-01 07:18:58</td>\n",
       "      <td>2018-01-01 01:18:58</td>\n",
       "      <td>01</td>\n",
       "      <td>6333186483</td>\n",
       "      <td>Authorize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  received_date site_id subscription_id                            event_uuid  \\\n",
       "0      20180101   avast     10025882901  4a2d05e6-36b8-491c-8d10-aa79ca6dc399   \n",
       "1      20180101   avast     10036263201  e7510e07-0e46-44b0-a6e3-ff49047df3d5   \n",
       "2      20180101   avast     10039189701  ba44bf00-d656-4515-9488-88024f0fa463   \n",
       "3      20180101   avast     10039189701  f2d16301-9c3b-4637-9af0-7a4f0fd09a99   \n",
       "4      20180101   avast     10040113001  3dc16619-c991-4bce-8b54-389ef3eba8b8   \n",
       "\n",
       "  added_expiry_years         bank_code  \\\n",
       "0               None  99EBEB6D41E6A44A   \n",
       "1               None  3286758E8A3568CB   \n",
       "2               None            NON3DS   \n",
       "3               None            NON3DS   \n",
       "4               None  7B8BE8370D67058F   \n",
       "\n",
       "                                      bank_name billing_country     bin  \\\n",
       "0  CAPITAL ONE BANK (USA), NATIONAL ASSOCIATION              US  517805   \n",
       "1                     Navy Federal Credit Union              US  400022   \n",
       "2                     Banco de Credito del Peru              PE  455788   \n",
       "3                     Banco de Credito del Peru              PE  455788   \n",
       "4                   J.S.C. Federal Credit Union              US  460753   \n",
       "\n",
       "   card_brand             card_category card_class  \\\n",
       "0  MasterCard  Platinum MasterCard Card   Consumer   \n",
       "1        Visa              Visa Classic   Consumer   \n",
       "2        Visa                 Visa Gold   Consumer   \n",
       "3        Visa                 Visa Gold   Consumer   \n",
       "4        Visa              Visa Classic   Consumer   \n",
       "\n",
       "                                  card_usage cc_expiration_date  \\\n",
       "0  True credit (No PIN/Signature capability)               0920   \n",
       "1           Debit Hybrid (PIN and Signature)               0120   \n",
       "2     NON USA Consumer Debit - No Pin Access               1220   \n",
       "3     NON USA Consumer Debit - No Pin Access               1220   \n",
       "4           Debit Hybrid (PIN and Signature)               0818   \n",
       "\n",
       "                cid date_increment day_of_month day_of_week exp_setting_id  \\\n",
       "0              None           None           01      Monday           None   \n",
       "1              None           None           01      Monday           None   \n",
       "2              None           None           01      Monday           None   \n",
       "3  RETRY_DECLINED.2           None           01      Monday           None   \n",
       "4              None           None           01      Monday           None   \n",
       "\n",
       "  funding_source issuer_country merchant_descriptor merchant_number  \\\n",
       "0         Credit             US  DRI*AVAST Software    311009012882   \n",
       "1          Debit             US  DRI*AVAST Software    311009012882   \n",
       "2          Debit             PE  DRI*AVAST Software      1156962062   \n",
       "3          Debit             PE  DRI*AVAST Software      1411163460   \n",
       "4          Debit             US  DRI*AVAST Software    311009012882   \n",
       "\n",
       "     mid_entity_code new_status order_entity_code payment_amount  \\\n",
       "0      DR_INC-ENTITY  Completed                            85.59   \n",
       "1      DR_INC-ENTITY  Completed                            99.99   \n",
       "2  DR_IRELAND-ENTITY   Declined                              179   \n",
       "3  DR_IRELAND-ENTITY   Declined                              179   \n",
       "4      DR_INC-ENTITY  Completed                            86.59   \n",
       "\n",
       "  payment_amount_usd payment_currency payment_method_id payment_service_id  \\\n",
       "0              85.59              USD        MasterCard          firstdata   \n",
       "1              99.99              USD              Visa          firstdata   \n",
       "2          55.223255              PEN              Visa        netgiro-seb   \n",
       "3          55.223255              PEN              Visa        netgiro-bms   \n",
       "4              86.59              USD              Visa          firstdata   \n",
       "\n",
       "  renew_att_num request_amount request_amount_usd request_currency  \\\n",
       "0             2          85.59              85.59              USD   \n",
       "1             2          99.99              99.99              USD   \n",
       "2             1            179          55.223255              PEN   \n",
       "3             1            179          55.223255              PEN   \n",
       "4             1          86.59              86.59              USD   \n",
       "\n",
       "  requisition_id response_code  \\\n",
       "0    43984979000           100   \n",
       "1    43981753500           100   \n",
       "2    43976743900         27001   \n",
       "3    43976743900         27054   \n",
       "4    43978651700           100   \n",
       "\n",
       "                                 response_message retry_rule_id status  \\\n",
       "0                                    00: Approved          None    New   \n",
       "1                                    00: Approved          None    New   \n",
       "2          Transaction refused[51] [000000] [] []          None    New   \n",
       "3  Attempt lower amount.[51] [] [000800] [000800]          None    New   \n",
       "4                                    00: Approved          None    New   \n",
       "\n",
       "  subsegment_id time_of_day_rule_id    transaction_date  \\\n",
       "0   13308769700                     2018-01-01 07:45:45   \n",
       "1   13325277000                     2018-01-01 07:30:06   \n",
       "2   13331289800                     2018-01-01 07:06:55   \n",
       "3   13331289800                     2018-01-01 07:06:57   \n",
       "4   13332230000                     2018-01-01 07:18:58   \n",
       "\n",
       "  transaction_date_in_string transaction_hour transaction_id transaction_type  \\\n",
       "0        2018-01-01 01:45:45               01     6333349913        Authorize   \n",
       "1        2018-01-01 01:30:06               01     6333297633        Authorize   \n",
       "2        2018-01-01 01:06:55               01     6333065673        Authorize   \n",
       "3        2018-01-01 01:06:57               01     6333066053        Authorize   \n",
       "4        2018-01-01 01:18:58               01     6333186483        Authorize   \n",
       "\n",
       "   success  \n",
       "0        1  \n",
       "1        1  \n",
       "2        0  \n",
       "3        0  \n",
       "4        1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cs_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4126194, 49)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Get only data that failed at first attempt\n",
    "\n",
    "# df_nl = df_cs_original\n",
    "# # Add 'success' as column\n",
    "# df_nl['success'] = df_nl['new_status'].map({'Completed':1,'Declined':0, 'Failed':0, 'Reversed':1})\n",
    "# # Remove renewal data that success at first attempt\n",
    "# df_nl = df_nl[~((df_nl['renew_att_num'] == 1.0) & (df_nl['success'] == 1))]\n",
    "# df_nl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cs_original' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4f4ed74c8d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get only data that failed at first attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_nl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cs_original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add 'success' as column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_nl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'success'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_nl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Completed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Declined'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Failed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Reversed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cs_original' is not defined"
     ]
    }
   ],
   "source": [
    "# Get only data that failed at first attempt\n",
    "\n",
    "df_nl = df_cs_original\n",
    "# Add 'success' as column\n",
    "df_nl['success'] = df_nl['new_status'].map({'Completed':1,'Declined':0, 'Failed':0, 'Reversed':1})\n",
    "# Remove renewal data that success at first attempt\n",
    "df_nl = df_nl[~((df_nl['renew_att_num'] == '1') & (df_nl['success'] == 1))]\n",
    "df_nl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "msg_group = { 'declined' : 'decline', \n",
    "             'do_not_honor' : 'do not honor', \n",
    "             'txn_refused' : 'refuse', \n",
    "             'attempt_lower_amount' : 'lower amount',\n",
    "            'Insufficient Funds' : 'insufficient',\n",
    "            'not_allowed' : 'not allowed',\n",
    "            'correct_cc_retry' : 'correct card',\n",
    "            'invalid_cc' : 'invalid card',\n",
    "            'lost_stolen' : 'lost or stolen',\n",
    "            'invalid_account' : 'invalid account',\n",
    "            'do_not_try' : 'do not try',\n",
    "            'expired_card' : 'expired',\n",
    "            'pickup_card' : 'pick',\n",
    "            'blocked_first_used' : 'blocked',\n",
    "            'invalid_txn' : 'invalid trans',\n",
    "            'restricted_card' : 'restricted',\n",
    "            'not_permitted' : 'not permitted',\n",
    "            'expired card' : 'expired card',\n",
    "            'unable to determine format' : 'determine format',\n",
    "            'system error' : 'error'\n",
    "            }\n",
    "\n",
    "def group_response_msg(msg):\n",
    "    other = 'Base'\n",
    "    if isinstance(msg, str) == False:\n",
    "        return other\n",
    "    \n",
    "    msg_lower = msg.lower()\n",
    "    for key, val in msg_group.items():\n",
    "        if val in msg_lower:\n",
    "            return key\n",
    "        \n",
    "    return other   \n",
    "\n",
    "def merge_by_sub(group):\n",
    "\n",
    "    first = group[ group['renew_att_num'] == '1' ]\n",
    "    \n",
    "    if first.empty:\n",
    "        return pd.DataFrame({})\n",
    "    \n",
    "    last = group[ group['renew_att_num'] == group['renew_att_num'].max() ]\n",
    "        \n",
    "    first_attempt_date = first['transaction_date_in_string'].iloc[0]\n",
    "    last_attempt_date = last['transaction_date_in_string'].iloc[0]\n",
    "    \n",
    "    last['first_attempt_date'] = first_attempt_date\n",
    "    last['first_day_of_month'] = first['day_of_month'].iloc[0]\n",
    "    last['day_of_month'] = last['day_of_month'].astype(int)\n",
    "    last['first_transaction_hour'] = first['transaction_hour'].iloc[0]\n",
    "    last['first_response_code'] = first['response_code'].iloc[0]\n",
    "    \n",
    "    first_response_msg = first['response_message'].iloc[0]\n",
    "    last['first_response_message'] = first_response_msg  \n",
    "    last['first_response_group'] =   first['response_message'].apply(group_response_msg).iloc[0]\n",
    "    \n",
    "    last['first_decline_type'] = decline_type(first_response_msg)\n",
    "    last['days_between'] =  days_between(first_attempt_date, last_attempt_date)\n",
    "        \n",
    "    return last\n",
    "\n",
    "def decline_type(response_msg):\n",
    "    '''Converts to decline_type based on the given response_msg'''\n",
    "    dec_type = df_decline_type[df_decline_type['DECLINE_TEXT'] == response_msg]['DECLINE_TYPE']\n",
    "    if dec_type.empty or dec_type.iloc[0] == 'Base' :\n",
    "        return group_response_msg(response_msg)\n",
    "    else:\n",
    "        return dec_type.iloc[0]\n",
    "\n",
    "#Convert from str to datetime\n",
    "def to_date(datestr):\n",
    "    struct = time.strptime(datestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date = datetime.date(struct.tm_year,struct.tm_mon,struct.tm_mday)\n",
    "    return date\n",
    "\n",
    "#Get difference between d2 and d1 in days.\n",
    "def days_between(d1, d2):\n",
    "    d1 = to_date(d1)\n",
    "    d2 = to_date(d2)\n",
    "    return abs((d2 - d1).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decline_type = pd.read_csv(WORK_DIR + 'Decline_Type.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by subscription_id for failed data at first attempt\n",
    "time_start = time.time()\n",
    "df_nl = df_nl[~df_nl['renew_att_num'].isna()]\n",
    "df_subs_merged = pd.DataFrame()\n",
    "for site_id in site_ids:\n",
    "    df_subs = df_nl[df_nl['site_id'] == site_id].groupby(['subscription_id', 'subsegment_id'], sort=False)\n",
    "    df_subs_merged = pd.concat([df_subs_merged, df_subs.apply(merge_by_sub)], axis=0)\n",
    "    \n",
    "print(\"# merge time:\", time.time() - time_start)\n",
    "df_subs_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved into /drhdev/workspace/ml/subs_retry_2018-04-01_2018-06-30_avast_kasperus_adbehap_adbehbr_adbehkr_mcafeeus_mfeap_mfeeu.csv\n"
     ]
    }
   ],
   "source": [
    "#Only take subscription that has at least two transactions that occured on different day\n",
    "retry_success = df_subs_merged[df_subs_merged['days_between'] > 0]\n",
    "\n",
    "# Drop duplicate columns\n",
    "retry_success = retry_success.drop(columns=['subscription_id', 'subsegment_id'])\n",
    "\n",
    "#Write merged subs to csv\n",
    "csv_name = str('subs_retry_%s_%s_%s.csv' % (start_date, end_date, \"_\".join(site_ids)))\n",
    "retry_success.to_csv(WORK_DIR + csv_name)\n",
    "print(\"Saved into \" + WORK_DIR + csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/drhdev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (12,23,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(997957, 57)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine two df from csv\n",
    "df_cs_original = None\n",
    "file_1 = 'subs_retry_2018-01-01_2018-03-31_avast_kasperus_adbehap_adbehbr_adbehkr_mcafeeus_mfeap_mfeeu.csv'\n",
    "retry_success_1 = pd.read_csv(WORK_DIR + file_1)\n",
    "\n",
    "file_2 = 'subs_retry_2018-04-01_2018-06-30_avast_kasperus_adbehap_adbehbr_adbehkr_mcafeeus_mfeap_mfeeu.csv'\n",
    "\n",
    "\n",
    "retry_success = pd.concat([retry_success_1, pd.read_csv(WORK_DIR + file_2)], axis=0)\n",
    "retry_success.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (37,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1836209, 57)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retry_success =  pd.read_csv('/drhdev/workspace/ml/pdf_all_site_2018-01-01_2018-06-30.csv')\n",
    "retry_success.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "monday       276639\n",
       "wednesday    267770\n",
       "tuesday      265219\n",
       "saturday     261475\n",
       "friday       260917\n",
       "sunday       260161\n",
       "thursday     244028\n",
       "Name: day_of_week, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retry_success['day_of_week'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1534637\n",
       "1     301572\n",
       "Name: success, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retry_success[retry_success['subscription_id'] == 10001280501]\n",
    "retry_success['success'].value_counts().sort_index()\n",
    "# retry_success[retry_success['subscription_id'] == 10001280501]\n",
    "# retry_success.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resampling, balancing classes ###\n",
    "def balancing_class(df_ml):\n",
    "    print(df_ml[LABEL].value_counts(normalize=True))\n",
    "    df_cls_0 = df_ml[df_ml[LABEL] == 0]\n",
    "    df_cls_1 = df_ml[df_ml[LABEL] == 1]\n",
    "\n",
    "    #over sampling\n",
    "#     df_cls_0_over = df_cls_0.sample(len(df_cls_1), replace=True)\n",
    "#     df_ml_bl = pd.concat([df_cls_0_over, df_cls_1], axis=0)\n",
    "    df_cls_1_over = df_cls_1.sample(len(df_cls_0), replace=True)\n",
    "    df_ml_bl = pd.concat([df_cls_1_over, df_cls_0], axis=0)\n",
    "\n",
    "#     #under sampling\n",
    "#     df_cls_1_under = df_cls_1.sample(len(df_cls_0), replace=True)\n",
    "#     df_ml_bl = pd.concat([df_cls_1_under, df_cls_0], axis=0)\n",
    "    \n",
    "    print(df_ml_bl[LABEL].value_counts(normalize=True))\n",
    "    print(df_ml_bl.shape)\n",
    "    return df_ml_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.835764\n",
      "1    0.164236\n",
      "Name: success, dtype: float64\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: success, dtype: float64\n",
      "(3069276, 57)\n"
     ]
    }
   ],
   "source": [
    "retry_success_balanced_all = balancing_class(retry_success)\n",
    "# csv_name = str('subs_retry_balanced__%s_%s_%s.csv' % (start_date, end_date, \"_\".join(site_ids)))\n",
    "# retry_success_balanced_all.to_csv(WORK_DIR + csv_name)\n",
    "# print(\"Saved into \" + WORK_DIR + csv_name)\n",
    "# retry_success_balanced_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = len(retry_success)\n",
    "balanced_size = len(retry_success_balanced_all)\n",
    "fail_size = retry_success[LABEL].value_counts(normalize=True)[0.0]\n",
    "success_size =  retry_success[LABEL].value_counts(normalize=True)[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drhdev/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import for training\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from utils import PreProcessing\n",
    "from encoder import SafeLabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.utils.validation import check_is_fitted\n",
    "# from sklearn.utils import column_or_1d\n",
    "# # from sklearn.preprocessing.label import _check_numpy_unicode_bug\n",
    "\n",
    "# def _get_unseen():\n",
    "#     \"\"\"Basically just a static method\n",
    "#     instead of a class attribute to avoid\n",
    "#     someone accidentally changing it.\"\"\"\n",
    "#     return 99999\n",
    "\n",
    "# class SafeLabelEncoder(LabelEncoder):\n",
    "#     \"\"\"An extension of LabelEncoder that will\n",
    "#     not throw an exception for unseen data, but will\n",
    "#     instead return a default value of 99999\n",
    "#     Attributes\n",
    "#     ----------\n",
    "#     classes_ : the classes that are encoded\n",
    "#     \"\"\"\n",
    "\n",
    "#     def transform(self, y):\n",
    "#         \"\"\"Perform encoding if already fit.\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         y : array_like, shape=(n_samples,)\n",
    "#             The array to encode\n",
    "#         Returns\n",
    "#         -------\n",
    "#         e : array_like, shape=(n_samples,)\n",
    "#             The encoded array\n",
    "#         \"\"\"\n",
    "#         check_is_fitted(self, 'classes_')\n",
    "#         y = column_or_1d(y, warn=True)\n",
    "\n",
    "#         classes = np.unique(y)\n",
    "# #         _check_numpy_unicode_bug(classes)\n",
    "\n",
    "#         # Check not too many:\n",
    "#         unseen = _get_unseen()\n",
    "#         if len(classes) >= unseen:\n",
    "#             raise ValueError('Too many factor levels in feature. Max is %i' % unseen)\n",
    "\n",
    "#         e = np.array([\n",
    "#                          np.searchsorted(self.classes_, x) if x in self.classes_ else unseen\n",
    "#                          for x in y\n",
    "#                          ])\n",
    "\n",
    "#         return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "def print_accuracy_report(classifier, X, y, num_validations=5):\n",
    "    accuracy = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='accuracy', cv=num_validations)\n",
    "    print(\"CV Accuracy: \" + str(round(100*accuracy.mean(), 2)) + \"%\")\n",
    "\n",
    "    f1 = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='f1_weighted', cv=num_validations)\n",
    "    print(\"CV F1: \" + str(round(100*f1.mean(), 2)) + \"%\")\n",
    "\n",
    "    precision = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='precision_weighted', cv=num_validations)\n",
    "    print(\"CV Precision: \" + str(round(100*precision.mean(), 2)) + \"%\")\n",
    "\n",
    "    recall = cross_validation.cross_val_score(classifier, \n",
    "            X, y, scoring='recall_weighted', cv=num_validations)\n",
    "    print(\"CV Recall: \" + str(round(100*recall.mean(), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES_CAT = ['card_brand', 'funding_source', 'card_category', 'card_class', \n",
    "#                  'issuer_country', 'first_response_code', 'day_of_month',\n",
    "#                'first_decline_type',  'mid_entity_code', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month']\n",
    "\n",
    "# FEATURES_NUM = ['payment_amount_usd']\n",
    "# FEATURES_ENCODED = [ 'week_of_month', 'day_of_week']\n",
    "# FEATURES = FEATURES_CAT + FEATURES_ENCODED\n",
    "\n",
    "\n",
    "# # from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "\n",
    "# # from training import PreProcessing\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# FIELDS = FEATURES_CAT + FEATURES_NUM + ['transaction_date_in_string']\n",
    "\n",
    "# features_dict = {'FEATURES_CAT': FEATURES_CAT, 'FEATURES_NUM':FEATURES_NUM, 'FEATURES_ENCODED':FEATURES_ENCODED}\n",
    "# features_dict_key = 'preprocessing__features_dict'\n",
    "# def display_feature_importance(pipe, model_name, df_features):\n",
    "#     classifier = pipe.named_steps[model_name]\n",
    "#     feature_importance = classifier.feature_importances_\n",
    "#     feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "#     sorted_idx = np.argsort(feature_importance)\n",
    "#     print(\"feature_importance column \",df_features.columns[sorted_idx])\n",
    "#     print(\"feature_importance val \",feature_importance[sorted_idx])\n",
    "#     pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "#     pvals = feature_importance[sorted_idx]\n",
    "#     pcols = df_features.columns[sorted_idx]\n",
    "#     plt.figure(figsize=(8,12))\n",
    "#     plt.barh(pos, pvals, align='center')\n",
    "#     plt.yticks(pos, pcols)\n",
    "#     plt.xlabel('Relative Importance')\n",
    "#     plt.title('Variable Importance')\n",
    "\n",
    "# def build_and_train(df, clf, param_grid, model_name, model_file = ''):\n",
    "#     model_prefix = model_name + '__'\n",
    "#     time_start = time.time()\n",
    "#     df_X = df[FIELDS]\n",
    " \n",
    "#     x_train, x_test, y_train, y_test = cross_validation.train_test_split(df_X, df[LABEL], \\\n",
    "#                                                         test_size=0.25, random_state=42)\n",
    "\n",
    "#     pipe = make_pipeline(PreProcessing(), clf())\n",
    "\n",
    "#     score = 'accuracy' #  ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "#     print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        \n",
    "#     pipe_param_grid = {model_prefix + k: v for k, v in param_grid.items()}\n",
    "#     print(\"pipe_param_grid \", pipe_param_grid)\n",
    "#     clf_gs = GridSearchCV(sc, pipe, pipe_param_grid, cv=3, scoring=score, n_jobs=-1, fit_params={features_dict_key: features_dict})\n",
    "# #     clf_gs = GridSearchCV(pipe, pipe_param_grid, cv=5, scoring=score, n_jobs=-1, fit_params={features_dict_key: features_dict})\n",
    "#     clf_gs.fit(x_train, y_train)\n",
    "\n",
    "#     print(\"# Best parameters set found on development set:\")\n",
    "#     print(clf_gs.best_params_)\n",
    "#     print(\"# Grid scores on development set:\")\n",
    "#     means = clf_gs.cv_results_['mean_test_score']\n",
    "#     stds = clf_gs.cv_results_['std_test_score']\n",
    "#     for mean, std, params in zip(means, stds, clf_gs.cv_results_['params']):\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "\n",
    "#     print(\"x_train\", x_train.shape)\n",
    "#     print(\"x_test\", x_test.shape)\n",
    "\n",
    "#     best_parameters = clf_gs.best_params_\n",
    "#     best_parameters = {k.replace(model_prefix,''): v for k, v in best_parameters.items()}\n",
    "#     print(\"best_parameters \", best_parameters)\n",
    "#     pipe = make_pipeline(PreProcessing(), clf(**best_parameters))\n",
    "    \n",
    "#     if model_name == 'xgbclassifier':\n",
    "#         print(\"training xgb ....... \")\n",
    "#         if model_file == '':\n",
    "#             pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict, xgbclassifier__eval_metric='error')\n",
    "#         else:\n",
    "#             print(\"Using model_file to train: \", model_file)\n",
    "#             pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict, xgbclassifier__xgb_model=model_file, xgbclassifier__eval_metric='error')\n",
    "#     else:        \n",
    "#         pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict)\n",
    "        \n",
    "#     preprocess = pipe.named_steps['preprocessing']\n",
    "\n",
    "        \n",
    "#     y_pred_train = pipe.predict(x_train).round()\n",
    "#     y_pred_test = pipe.predict(x_test).round()\n",
    "    \n",
    "#     print(\"# training time:\", time.time() - time_start)\n",
    "\n",
    "#     clf_d = DummyClassifier(strategy='most_frequent')\n",
    "#     dummy_pipe = make_pipeline(PreProcessing(), clf_d)\n",
    "#     dummy_pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict)\n",
    "#     y_pred_test_dummy = dummy_pipe.predict(x_test).round()\n",
    "#     print(\"accuracy_dummy:\", metrics.accuracy_score(y_test, y_pred_test_dummy))\n",
    "#     print(\"training accuracy:\", metrics.accuracy_score(y_train, y_pred_train))\n",
    "#     print(\"test accuracy:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "     \n",
    "# #     print_accuracy_report(pipe, x_train, y_train, num_validations=3)\n",
    "#     conf_mx = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "#     print(\"# confusion_matrix -  test:\\n\", conf_mx)\n",
    "    \n",
    "\n",
    "#     return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES_CAT = ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage',\n",
    "#                  'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country',\n",
    "#                'first_decline_type',  'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'first_day_of_month']\n",
    "\n",
    "FEATURES_CAT = ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage',\n",
    "                 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country',\n",
    "               'first_decline_type',  'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'first_day_of_month']\n",
    "\n",
    "FEATURES_NUM = ['payment_amount_usd']\n",
    "FEATURES_ENCODED = [ 'week_of_month', 'day_of_week']\n",
    "FEATURES = FEATURES_CAT + FEATURES_ENCODED\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "\n",
    "# from training import PreProcessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "FIELDS = FEATURES_CAT + FEATURES_NUM + ['transaction_date_in_string']\n",
    "\n",
    "features_dict = {'FEATURES_CAT': FEATURES_CAT, 'FEATURES_NUM':FEATURES_NUM, 'FEATURES_ENCODED':FEATURES_ENCODED}\n",
    "features_dict_key = 'preprocessing__features_dict'\n",
    "def display_feature_importance(pipe, model_name, df_features):\n",
    "    classifier = pipe.named_steps[model_name]\n",
    "    feature_importance = classifier.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    print(\"feature_importance column \",df_features.columns[sorted_idx])\n",
    "    print(\"feature_importance val \",feature_importance[sorted_idx])\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    pvals = feature_importance[sorted_idx]\n",
    "    pcols = df_features.columns[sorted_idx]\n",
    "    plt.figure(figsize=(8,12))\n",
    "    plt.barh(pos, pvals, align='center')\n",
    "    plt.yticks(pos, pcols)\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "\n",
    "def build_and_train(df, clf, param_grid, model_name, model_file = '', best_param=None):\n",
    "    model_prefix = model_name + '__'\n",
    "    time_start = time.time()\n",
    "    df_X = df[FIELDS]\n",
    "    result_dict = {}\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(df_X, df[LABEL], \\\n",
    "                                                        test_size=0.25, random_state=42)\n",
    "\n",
    "    \n",
    "    if best_param is None:\n",
    "    \n",
    "        pipe = make_pipeline(PreProcessing(), clf())\n",
    "        score = 'accuracy' #  ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        \n",
    "        pipe_param_grid = {model_prefix + k: v for k, v in param_grid.items()}\n",
    "        print(\"pipe_param_grid \", pipe_param_grid)\n",
    "#         clf_gs = GridSearchCV(sc, pipe, pipe_param_grid, cv=3, scoring=score, n_jobs=-1, fit_params={features_dict_key: features_dict})\n",
    "        clf_gs = GridSearchCV(pipe, pipe_param_grid, cv=3, scoring=score, n_jobs=-1, fit_params={features_dict_key: features_dict})\n",
    "        clf_gs.fit(x_train, y_train)\n",
    "\n",
    "        print(\"# Best parameters set found on development set:\")\n",
    "        print(clf_gs.best_params_)\n",
    "        result_dict['hyper_params'] = clf_gs.best_params_\n",
    "        print(\"# Grid scores on development set:\")\n",
    "        means = clf_gs.cv_results_['mean_test_score']\n",
    "        stds = clf_gs.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf_gs.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "\n",
    "        best_parameters = clf_gs.best_params_\n",
    "        best_parameters = {k.replace(model_prefix,''): v for k, v in best_parameters.items()}\n",
    "        \n",
    "    else:\n",
    "        best_parameters= best_param\n",
    "   \n",
    "    print(\"best_parameters \", best_parameters)        \n",
    "    print(\"x_train\", x_train.shape)\n",
    "    print(\"x_test\", x_test.shape)\n",
    "\n",
    "\n",
    "    pipe = make_pipeline(PreProcessing(), clf(**best_parameters))\n",
    "    \n",
    "    if model_name == 'xgbclassifier':\n",
    "        print(\"training xgb ....... \")\n",
    "        if model_file == '':\n",
    "            pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict, xgbclassifier__eval_metric='error')\n",
    "        else:\n",
    "            print(\"Using model_file to train: \", model_file)\n",
    "            pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict, xgbclassifier__xgb_model=model_file, xgbclassifier__eval_metric='error')\n",
    "    else:        \n",
    "        pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict)\n",
    "        \n",
    "    preprocess = pipe.named_steps['preprocessing']\n",
    "\n",
    "        \n",
    "    y_pred_train = pipe.predict(x_train).round()\n",
    "    y_pred_test = pipe.predict(x_test).round()\n",
    "\n",
    "    training_time = time.time() - time_start\n",
    "    print(\"# training time:\", training_time)\n",
    "    result_dict['training_time'] = training_time\n",
    "    clf_d = DummyClassifier(strategy='most_frequent')\n",
    "    dummy_pipe = make_pipeline(PreProcessing(), clf_d)\n",
    "    dummy_pipe.fit(x_train, y_train, preprocessing__features_dict=features_dict)\n",
    "    y_pred_test_dummy = dummy_pipe.predict(x_test).round()\n",
    "    \n",
    "    accuracy_dummy = metrics.accuracy_score(y_test, y_pred_test_dummy)\n",
    "    train_accuracy = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    result_dict['accuracy_dummy'] = accuracy_dummy\n",
    "    result_dict['train_accuracy'] = train_accuracy\n",
    "    result_dict['test_accuracy'] = test_accuracy\n",
    "#     print_accuracy_report(pipe, x_train, y_train, num_validations=3)\n",
    "#     print(metrics.classification_report(y_test, y_pred_test))\n",
    "    train_class_report = metrics.classification_report(y_train, y_pred_train)\n",
    "    test_class_report = metrics.classification_report(y_test, y_pred_test)\n",
    "    result_dict['train_class_report'] = train_class_report\n",
    "    result_dict['test_class_report'] = test_class_report\n",
    "    conf_mx = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    print(\"accuracy_dummy:\", accuracy_dummy)\n",
    "    print(\"training accuracy:\", train_accuracy)\n",
    "    print(train_class_report)\n",
    "    print(\"test accuracy:\", test_accuracy)\n",
    "    print(test_class_report)\n",
    "    print(\"# confusion_matrix -  test:\\n\", conf_mx)\n",
    "    result_dict['conf_mx'] = conf_mx\n",
    "\n",
    "    return pipe, result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "MODEL_DIR = WORK_DIR + \"models\"\n",
    "\n",
    "\n",
    "def write_model(model, model_name, idx=None): \n",
    "    build_id = \"\" if idx is None else \"_\" + str(idx)\n",
    "    file_name = path.join(MODEL_DIR, '%s%s.pkl' % (model_name, build_id))\n",
    "    if not os.path.exists(os.path.dirname(file_name)):\n",
    "        os.makedirs(os.path.dirname(file_name))\n",
    "    file = joblib.dump(model, file_name)\n",
    "    return (file, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cassandra_endpoint = '10.224.12.32'\n",
    "cluster = Cluster([cassandra_endpoint])\n",
    "mlp_session = cluster.connect('mlp')\n",
    "\n",
    "\n",
    "def insert_model_info(model_id, version, file_name, desc, model_type=MODEL_TYPE, algorithm='XGBClassifier', hyper_parameter=None, eval_metrics=None):\n",
    "#     file_name = model_id + '.' + str(version) + \".pkl\"\n",
    "    \n",
    "    mlp_session.execute(\n",
    "    \"\"\"\n",
    "    INSERT INTO ml_model_storage (model_type, model_id, version, features_cat, features_encoded, features_num, repo_path, description, creation_date, modification_date, algorithm, hyper_parameter, eval_metrics)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\",\n",
    "    (model_type, model_id, version, str(FEATURES_CAT), str(FEATURES_ENCODED), str(FEATURES_NUM), file_name, desc, datetime.datetime.utcnow(), datetime.datetime.utcnow(), algorithm, hyper_parameter, eval_metrics)\n",
    "        \n",
    "    )\n",
    "    print(\"Model %s version %d is inserted into model repo\" % (model_id, version))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_version(model_id, model_type=MODEL_TYPE):\n",
    "    \"\"\"Get latest version of the given model_id\"\"\"\n",
    "#     session = cluster.connect('mlp')\n",
    "    latest_version_query = \"select version from ml_model_storage  where model_type = '%s' and model_id = '%s' limit 1\" % (model_type, model_id)\n",
    "    query_result = mlp_session.execute(latest_version_query).one()\n",
    "    if query_result is None:\n",
    "        latest_version = 1\n",
    "    else:\n",
    "        latest_version = query_result.version\n",
    "    \n",
    "    return latest_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_URL = 'http://nexus.digitalriverws.net/nexus'\n",
    "REPO_USER = 'deployment'\n",
    "REPO_PWD = 'deployment123'\n",
    "REPO_ID = 'foundationreleases'\n",
    "REPO_GROUP = 'com.digitalriver.prediction-service'\n",
    "\n",
    "import repositorytools\n",
    "\n",
    "def upload_artifact(file_path):\n",
    "    \"\"\"Upload artifact to Nexus Repo\"\"\"\n",
    "    artifact = repositorytools.LocalArtifact(local_path=file_path, group=REPO_GROUP)\n",
    "\n",
    "    client = repositorytools.repository_client_factory(repository_url=REPOSITORY_URL, user=REPO_USER, password=REPO_PWD)\n",
    "    remote_artifacts = client.upload_artifacts(local_artifacts=[artifact], repo_id=REPO_ID, use_direct_put=True)\n",
    "    print(remote_artifacts)\n",
    "    return str(remote_artifacts[0]) if remote_artifacts else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameters  {'objective': 'binary:logistic', 'learning_rate': 0.2, 'max_depth': 10, 'min_child_weight': 11, 'silent': 0, 'subsample': 0.5, 'colsample_bytree': 0.7, 'n_estimators': 1000, 'missing': -999, 'seed': 1337}\n",
      "x_train (2301957, 18)\n",
      "x_test (767319, 18)\n",
      "training xgb ....... \n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n",
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n",
      "# training time: 14599.82113647461\n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n",
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n",
      "accuracy_dummy: 0.499836443513063\n",
      "training accuracy: 0.9401930618165326\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.93      0.94   1150853\n",
      "          1       0.93      0.95      0.94   1151104\n",
      "\n",
      "avg / total       0.94      0.94      0.94   2301957\n",
      "\n",
      "test accuracy: 0.9253661123991456\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.92      0.92    383785\n",
      "          1       0.92      0.94      0.93    383534\n",
      "\n",
      "avg / total       0.93      0.93      0.93    767319\n",
      "\n",
      "# confusion_matrix -  test:\n",
      " [[351429  32356]\n",
      " [ 24912 358622]]\n",
      "model_file is generated:  ['/drhdev/workspace/ml/models/ML-BR-1.33.pkl']\n",
      "result_dict:  {'training_time': 14599.82113647461, 'accuracy_dummy': 0.499836443513063, 'train_accuracy': 0.9401930618165326, 'test_accuracy': 0.9253661123991456, 'train_class_report': '             precision    recall  f1-score   support\\n\\n          0       0.95      0.93      0.94   1150853\\n          1       0.93      0.95      0.94   1151104\\n\\navg / total       0.94      0.94      0.94   2301957\\n', 'test_class_report': '             precision    recall  f1-score   support\\n\\n          0       0.93      0.92      0.92    383785\\n          1       0.92      0.94      0.93    383534\\n\\navg / total       0.93      0.93      0.93    767319\\n', 'conf_mx': array([[351429,  32356],\n",
      "       [ 24912, 358622]])}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "\n",
    "#XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "classifier = XGBClassifier\n",
    "\n",
    "tuned_parameters = {\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.2], #so called `eta` value\n",
    "              'max_depth': [10],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [0],\n",
    "              'subsample': [0.5],\n",
    "              'colsample_bytree': [0.7],\n",
    "#               'n_estimators': [500, 1000], #number of trees, change it to 1000 for better results\n",
    "              'n_estimators': [1000], #number of trees, change it to 1000 for better results  \n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "best_parameters = {\n",
    "              'objective': 'binary:logistic',\n",
    "              'learning_rate': 0.2, #so called `eta` value\n",
    "              'max_depth': 10,\n",
    "              'min_child_weight': 11,\n",
    "              'silent': 0,\n",
    "              'subsample': 0.5,\n",
    "              'colsample_bytree': 0.7,\n",
    "#               'n_estimators': [500, 1000], #number of trees, change it to 1000 for better results\n",
    "              'n_estimators': 1000, #number of trees, change it to 1000 for better results  \n",
    "              'missing':-999,\n",
    "              'seed': 1337}\n",
    "\n",
    "model_file = ''\n",
    "# for idx, df_2018_f in enumerate(df_2018_filtered):\n",
    "#     print(\"Iteration: \", idx)\n",
    "model_id = 'ML-BR-1'\n",
    "version = get_latest_version(model_id) + 1\n",
    "model_name = model_id + '.' + str(version)\n",
    "xgb_clf, result_d = build_and_train(retry_success_balanced_all, classifier, tuned_parameters, 'xgbclassifier', model_file, best_param=best_parameters)\n",
    "model_file, model_file_name = write_model(xgb_clf, model_name)\n",
    "print(\"model_file is generated: \", model_file)\n",
    "print(\"result_dict: \", result_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameters  {'objective': 'binary:logistic', 'learning_rate': 0.2, 'max_depth': 10, 'min_child_weight': 11, 'silent': 0, 'subsample': 0.5, 'colsample_bytree': 0.7, 'n_estimators': 1000, 'missing': -999, 'seed': 1337}\n",
      "x_train (2301957, 19)\n",
      "x_test (767319, 19)\n",
      "training xgb ....... \n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'first_day_of_month'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n",
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'first_day_of_month', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n",
      "# training time: 15835.044255256653\n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'first_day_of_month'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n",
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'first_day_of_month', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n",
      "accuracy_dummy: 0.499836443513063\n",
      "training accuracy: 0.9516385405982822\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.95      0.95   1150853\n",
      "          1       0.95      0.96      0.95   1151104\n",
      "\n",
      "avg / total       0.95      0.95      0.95   2301957\n",
      "\n",
      "test accuracy: 0.9364736178825235\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.93      0.94    383785\n",
      "          1       0.93      0.94      0.94    383534\n",
      "\n",
      "avg / total       0.94      0.94      0.94    767319\n",
      "\n",
      "# confusion_matrix -  test:\n",
      " [[357240  26545]\n",
      " [ 22200 361334]]\n",
      "model_file is generated:  ['/drhdev/workspace/ml/models/ML-BR-1.34.pkl']\n",
      "result_dict:  {'training_time': 15835.044255256653, 'accuracy_dummy': 0.499836443513063, 'train_accuracy': 0.9516385405982822, 'test_accuracy': 0.9364736178825235, 'train_class_report': '             precision    recall  f1-score   support\\n\\n          0       0.96      0.95      0.95   1150853\\n          1       0.95      0.96      0.95   1151104\\n\\navg / total       0.95      0.95      0.95   2301957\\n', 'test_class_report': '             precision    recall  f1-score   support\\n\\n          0       0.94      0.93      0.94    383785\\n          1       0.93      0.94      0.94    383534\\n\\navg / total       0.94      0.94      0.94    767319\\n', 'conf_mx': array([[357240,  26545],\n",
      "       [ 22200, 361334]])}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "\n",
    "#XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "classifier = XGBClassifier\n",
    "\n",
    "tuned_parameters = {\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.2], #so called `eta` value\n",
    "              'max_depth': [10],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [0],\n",
    "              'subsample': [0.5],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "best_parameters = {\n",
    "              'objective': 'binary:logistic',\n",
    "              'learning_rate': 0.2, #so called `eta` value\n",
    "              'max_depth': 10,\n",
    "              'min_child_weight': 11,\n",
    "              'silent': 0,\n",
    "              'subsample': 0.5,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'n_estimators': 1000, #number of trees, change it to 1000 for better results  \n",
    "              'missing':-999,\n",
    "              'seed': 1337}\n",
    "\n",
    "model_file = ''\n",
    "\n",
    "model_id = 'ML-BR-1'\n",
    "version = get_latest_version(model_id) + 1\n",
    "model_name = model_id + '.' + str(version)\n",
    "xgb_clf, result_d = build_and_train(retry_success_balanced_all, classifier, tuned_parameters, 'xgbclassifier', model_file, best_param=best_parameters)\n",
    "model_file, model_file_name = write_model(xgb_clf, model_name)\n",
    "print(\"model_file is generated: \", model_file)\n",
    "print(\"result_dict: \", result_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameters  {'objective': 'binary:logistic', 'learning_rate': 0.2, 'max_depth': 10, 'min_child_weight': 11, 'silent': 0, 'subsample': 0.5, 'colsample_bytree': 0.7, 'n_estimators': 1000, 'missing': -999, 'seed': 1337}\n",
      "x_train (2301955, 18)\n",
      "x_test (767319, 18)\n",
      "training xgb ....... \n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n",
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n",
      "# training time: 15266.259535551071\n",
      "features_dict:  {'FEATURES_CAT': ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num'], 'FEATURES_NUM': ['payment_amount_usd'], 'FEATURES_ENCODED': ['week_of_month', 'day_of_week']}\n",
      "self.features_list:  ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', 'first_response_code', 'day_of_month', 'site_id', 'billing_country', 'first_decline_type', 'merchant_number', 'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'week_of_month', 'day_of_week', 'payment_amount_usd']\n",
      "accuracy_dummy: 0.4998351402741233\n",
      "training accuracy: 0.9408220403960981\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.93      0.94   1150851\n",
      "          1       0.93      0.95      0.94   1151104\n",
      "\n",
      "avg / total       0.94      0.94      0.94   2301955\n",
      "\n",
      "test accuracy: 0.9267475456752667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.92      0.93    383786\n",
      "          1       0.92      0.94      0.93    383533\n",
      "\n",
      "avg / total       0.93      0.93      0.93    767319\n",
      "\n",
      "# confusion_matrix -  test:\n",
      " [[351784  32002]\n",
      " [ 24206 359327]]\n",
      "model_file is generated:  ['/drhdev/workspace/ml/models/ML-BR-1.32.pkl']\n",
      "result_dict:  {'training_time': 15266.259535551071, 'accuracy_dummy': 0.4998351402741233, 'train_accuracy': 0.9408220403960981, 'test_accuracy': 0.9267475456752667, 'train_class_report': '             precision    recall  f1-score   support\\n\\n          0       0.95      0.93      0.94   1150851\\n          1       0.93      0.95      0.94   1151104\\n\\navg / total       0.94      0.94      0.94   2301955\\n', 'test_class_report': '             precision    recall  f1-score   support\\n\\n          0       0.94      0.92      0.93    383786\\n          1       0.92      0.94      0.93    383533\\n\\navg / total       0.93      0.93      0.93    767319\\n', 'conf_mx': array([[351784,  32002],\n",
      "       [ 24206, 359327]])}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "\n",
    "#XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "classifier = XGBClassifier\n",
    "\n",
    "tuned_parameters = {\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.2], #so called `eta` value\n",
    "              'max_depth': [10],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [0],\n",
    "              'subsample': [0.5],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "best_parameters = {\n",
    "              'objective': 'binary:logistic',\n",
    "              'learning_rate': 0.2, #so called `eta` value\n",
    "              'max_depth': 10,\n",
    "              'min_child_weight': 11,\n",
    "              'silent': 0,\n",
    "              'subsample': 0.5,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'n_estimators': 1000, #number of trees, change it to 1000 for better results  \n",
    "              'missing':-999,\n",
    "              'seed': 1337}\n",
    "\n",
    "model_file = ''\n",
    "\n",
    "model_id = 'ML-BR-1'\n",
    "version = get_latest_version(model_id) + 1\n",
    "model_name = model_id + '.' + str(version)\n",
    "xgb_clf, result_d = build_and_train(retry_success_balanced_all, classifier, tuned_parameters, 'xgbclassifier', model_file, best_param=best_parameters)\n",
    "model_file, model_file_name = write_model(xgb_clf, model_name)\n",
    "print(\"model_file is generated: \", model_file)\n",
    "print(\"result_dict: \", result_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following files were uploaded to repository foundationreleases\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://nexus.digitalriverws.net/nexus/content/repositories/foundationreleases/com/digitalriver/prediction-service/ML-BR/1.34/ML-BR-1.34.pkl\n",
      "[com.digitalriver.prediction-service:ML-BR:1.34::pkl]\n",
      "Model ML-BR-1 version 34 is inserted into model repo\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Upload model to Nexus repo and insert the model info into Cassandra table\"\"\"\n",
    "\n",
    "repo_path = upload_artifact(model_file_name)\n",
    "size_desc = str(\", original size: %s (fail: %s, success: %s), balanced_size: %s\" % (original_size, fail_size, success_size, balanced_size))\n",
    "desc = desc + size_desc\n",
    "hyper_params = result_d.pop('hyper_params', None)\n",
    "insert_model_info(model_id, version, repo_path, desc=desc, eval_metrics=str(result_d), \n",
    "                  hyper_parameter=str(hyper_params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
