{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dependencies\n",
    "* install anaconda is recommended\n",
    "\n",
    "```\n",
    "cassandra-driver          3.11.0                   py35_1    conda-forge\n",
    "pandas                    0.19.1              np111py35_0\n",
    "scikit-learn              0.18.1              np111py35_0\n",
    "scipy                     0.18.1              np111py35_0\n",
    "matplotlib                2.0.0               np111py35_0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from src.web.train_util import read_from\n",
    "from SortedSet.sorted_set import SortedSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters. \n",
    "Parameters that are defined in this cell can be injected and overwritten by the machine learning platform.\n",
    "'''\n",
    "\n",
    "# MLP defined parameters \n",
    "training_runner = None\n",
    "project_id = None\n",
    "training_id = None\n",
    "metrics_feedback_url = None\n",
    "model_destination = None\n",
    "\n",
    "# user defined parameters\n",
    "\n",
    "# label keys\n",
    "label = 'success'\n",
    "# model file directory\n",
    "work_dir = '/var/spark/ml_files/'\n",
    "model_type = 'ML-BR'\n",
    "model_id = 'ML-BR-1'\n",
    "# desc = '%s_%s_for_calendar_retry_attempt'.format(start_date, end_date)\n",
    "\n",
    "# data\n",
    "training_data = ''\n",
    "# bin_profile_data =  work_dir + 'bin_profile_2019_01_to_2019_05.csv'\n",
    "# payment_mid_bin_data = work_dir + 'payment_mid_bin_2019_01_to_05.csv'\n",
    "# decline_type_data = work_dir + 'Decline_Type.csv'\n",
    "\n",
    "# features\n",
    "input_features = {\n",
    "            \"billing_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"bin\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_brand\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_category\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_class\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"card_usage\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"cc_expiration_date\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"day_of_month\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"failed_attempt_date\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"failed_response_code\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"failed_response_message\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"funding_source\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"issuer_country\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"merchant_number\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_amount_usd\": {\n",
    "                \"type\": \"number\"\n",
    "            },\n",
    "            \"payment_currency\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_method_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"payment_service_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"renew_att_num\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"site_id\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"transaction_date_in_string\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"renewal_window\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"duration\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"segment_num\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"sub_age\": {\n",
    "                \"type\": \"integer\"\n",
    "            }, \n",
    "            \"bank_name\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"first_calendar_attempt_date\": {\n",
    "                \"type\": \"string\"\n",
    "            }, \n",
    "            \"previous_cal_response_message_1\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"previous_cal_response_message_2\": {\n",
    "                \"type\": \"string\"\n",
    "            }, \n",
    "            \"previous_cal_response_message_3\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"previous_cal_response_code_1\": {\n",
    "                \"type\": \"string\"\n",
    "            }, \n",
    "            \"previous_cal_response_code_2\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"previous_cal_response_code_3\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"first_cal_response_message_1\": {\n",
    "                \"type\": \"string\"\n",
    "            }, \n",
    "            \"first_cal_response_message_2\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"first_cal_response_message_3\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "features_cat = [\n",
    "    'failed_decline_type',\n",
    "    'failed_decline_type_from_first_cal',\n",
    "    'day_of_month', \n",
    "    'funding_source', \n",
    "    'payment_currency', \n",
    "    'days_between',\n",
    "    'days_between_from_first_cal',\n",
    "    'renewal_window',  \n",
    "    'renew_att_num',\n",
    "    'is_first_renewal',\n",
    "    'failed_response_codes_from_previous_cal',\n",
    "    'payment_amount_group',\n",
    "    'billing_country',\n",
    "    'card_brand']\n",
    "\n",
    "features_float = [ \n",
    "    'bin',  \n",
    "    'failed_response_code', \n",
    "    'date_increment', \n",
    "    'renewal_window'\n",
    "]\n",
    "\n",
    "features_num = [ \n",
    "    'duration', \n",
    "    'sub_age',\n",
    "    'payment_amount_usd'\n",
    "]\n",
    "features_num_calculated = []\n",
    "features_num_bin_profile = []\n",
    "\n",
    "features_num_encoded = ['success_bank_card_count_per_day_of_month', 'txn_amount_bank_card_max_per_date_diff']\n",
    "\n",
    "features_cat_encoded = [\n",
    "    'month', \n",
    "    'days_between', \n",
    "    'days_between_from_first_cal',\n",
    "    'failed_decline_type_from_first_cal',\n",
    "    'renew_att_num', \n",
    "    'day_of_week', \n",
    "    'num_of_days', \n",
    "    'payment_service_id', \n",
    "    'merchant_number', \n",
    "    'month', \n",
    "    'is_expired', \n",
    "    'segment_num_group',\n",
    "    'sub_duration_group',\n",
    "    'payment_amount_group',\n",
    "    'sub_age_group',\n",
    "    'is_first_renewal',\n",
    "    'card_brand',\n",
    "    'failed_response_codes_from_previous_cal',\n",
    "    'failed_response_codes_from_first_cal'\n",
    "]\n",
    "\n",
    "\n",
    "features_grouped = [\n",
    "    ['payment_service_id', 'merchant_number'],\n",
    "    ['failed_decline_type', 'payment_service_id'],\n",
    "    ['failed_decline_type', 'funding_source'],\n",
    "    ['failed_decline_type', 'card_brand'],\n",
    "    ['failed_decline_type', 'is_expired'],\n",
    "    ['failed_decline_type', 'failed_response_codes_from_previous_cal'],\n",
    "    ['payment_service_id', 'merchant_number', 'payment_currency'],\n",
    "    ['bin', 'is_expired'],\n",
    "    ['bank_name', 'is_expired'],\n",
    "    ['bank_name', 'card_category'],\n",
    "    ['card_brand', 'funding_source'],\n",
    "    ['payment_service_id', 'is_expired'],\n",
    "    ['days_between', 'failed_decline_type'],\n",
    "    ['days_between', 'failed_response_codes_from_previous_cal'],\n",
    "    ['failed_response_codes_from_previous_cal', 'funding_source'],\n",
    "    ['days_between_from_first_cal', 'failed_response_codes_from_first_cal'],\n",
    "    ['days_between_from_first_cal', 'renewal_window'],\n",
    "    ['days_between_from_first_cal', 'funding_source'],\n",
    "    ['days_between', 'funding_source'],\n",
    "    ['card_brand', 'is_expired'],\n",
    "    ['sub_age_group', 'sub_duration_group'],\n",
    "    ['sub_duration_group', 'is_expired'],\n",
    "    ['segment_num_group', 'is_expired'],\n",
    "    ['day_of_week'],\n",
    "    ['is_expired'],\n",
    "    ['issuer_country', 'is_expired'] \n",
    "]\n",
    "\n",
    "#     ['failed_response_messages_from_first_cal_sorted', 'funding_source'],\n",
    "#     ['failed_response_messages_from_previous_cal', 'funding_source'],\n",
    "#     ['days_between_from_first_cal', 'failed_decline_type_from_first_cal'],\n",
    "#     ['renew_att_num', 'days_between', 'renewal_window'],\n",
    "# ['days_between_from_first_cal', 'renewal_window'],\n",
    "\n",
    "features_encoded = features_cat_encoded + features_num_encoded\n",
    "\n",
    "additional_fields = [\n",
    "    'card_brand',  \n",
    "    'segment_num', \n",
    "    'segment_num_group',\n",
    "    'payment_amount_group',\n",
    "    'bank_name', \n",
    "    'duration', \n",
    "    'is_expired', \n",
    "    'renewal_window', \n",
    "    'payment_currency', \n",
    "    'funding_source', \n",
    "    'card_category', \n",
    "    'card_class', \n",
    "    'card_usage', \n",
    "    'renew_att_num', \n",
    "    'site_id', \n",
    "    'bin', \n",
    "    'merchant_number', \n",
    "    'billing_country', \n",
    "    'funding_source', \n",
    "    \"payment_service_id\", \n",
    "    'day_of_month', \n",
    "    'failed_decline_type',  \n",
    "    'failed_day_of_month', \n",
    "    'failed_response_code', \n",
    "    'payment_amount_usd', \n",
    "    'issuer_country',  \n",
    "    'failed_response_message',\n",
    "    'days_between',  \n",
    "    'transaction_date_in_string', \n",
    "    'cc_expiration_date', \n",
    "    'failed_attempt_date',\n",
    "    'days_between_from_first_cal',\n",
    "    'failed_decline_type_from_first_cal',\n",
    "    'failed_response_codes_from_previous_cal',\n",
    "    'failed_response_codes_from_first_cal'\n",
    "]\n",
    "\n",
    "feature_candidates = ['card_brand', 'funding_source', 'card_category', 'card_class', 'card_usage', 'issuer_country', \n",
    "                 'day_of_month', 'site_id', 'failed_decline_type', 'merchant_number', \n",
    "                'payment_service_id', 'payment_method_id', 'bin', 'renew_att_num', 'failed_day_of_month', \n",
    "                'payment_currency', 'days_between', 'failed_response_code', 'payment_amount_usd', 'date_increment', \n",
    "                'transaction_hour', 'failed_response_messages_from_previous_cal', 'failed_response_codes_from_previous_cal', \n",
    "                'failed_decline_type_from_previous_cal', 'failed_response_messages_from_first_cal', 'failed_decline_type_from_first_cal', 'days_between_from_first_cal', \n",
    "                'failed_decline_type_from_first_cal','failed_response_codes_from_first_cal']\n",
    "\n",
    "usecols = feature_candidates +  ['new_status','subscription_id', 'subsegment_id', 'success', 'cid' ,'bank_name','added_expiry_years', 'failed_response_message','date_increment', 'received_date', 'billing_country', 'transaction_date_in_string', 'cc_expiration_date', 'failed_attempt_date']\n",
    "\n",
    "'''\n",
    "data parameters\n",
    "'''\n",
    "excluded_processors = ['mes', 'paypalExpress']\n",
    "\n",
    "subs_creation_date_files_2019 = [\n",
    "     'subs_subscription_creation_date_2017_01_2017_12.csv', \n",
    "     'subs_subscription_creation_date_2018_01_2018_05.csv',\n",
    "     'subs_subscription_creation_date_2018_06_2018_12.csv',\n",
    "     'subs_subscription_creation_date_2019_01_2019_12.csv']\n",
    "\n",
    "subs_creation_date_files = [\n",
    "    'subs_subscription_creation_date_2020-01.csv',\n",
    "    'subs_subscription_creation_date_2020-02.csv',\n",
    "    'subs_subscription_creation_date_2020-03.csv',\n",
    "    'subs_subscription_creation_date_2020-04.csv',\n",
    "    'subs_subscription_creation_date_2020-05.csv',\n",
    "    'subs_subscription_creation_date_2020-06.csv',\n",
    "    'subs_subscription_creation_date_2020-07.csv',\n",
    "    'subs_subscription_creation_date_2020-08.csv',\n",
    "    'subs_subscription_creation_date_2020-09.csv'\n",
    "]\n",
    "\n",
    "subs_files = [\n",
    "    'subs_subscription_2018_12_to_2020_01.csv', \n",
    "    'subs_li_item_2020-02.csv',  \n",
    "    'subs_li_item_2020-03.csv',\n",
    "    'subs_li_item_2020-04.csv',\n",
    "    'subs_li_item_2020-05.csv',  \n",
    "    'subs_li_item_2020-06.csv',\n",
    "    'subs_li_item_2020-07.csv',\n",
    "    'subs_li_item_2020-08.csv',\n",
    "    'subs_li_item_2020-09.csv'\n",
    "]\n",
    "\n",
    "sub_seg_expire_files = [\n",
    "    'sub_seg_expire_2019_all.csv', \n",
    "    'sub_seg_expire_2020_01.csv',\n",
    "    'sub_seg_expire_2020-02.csv',\n",
    "    'sub_seg_expire_2020-03.csv',\n",
    "    'sub_seg_expire_2020-04.csv',\n",
    "    'sub_seg_expire_2020-05.csv',\n",
    "    'sub_seg_expire_2020-06.csv',\n",
    "    'sub_seg_expire_2020-07.csv',\n",
    "    'sub_seg_expire_2020-08.csv',\n",
    "    'sub_seg_expire_2020-09.csv'\n",
    "]\n",
    "\n",
    "\n",
    "#     'sub_seg_expire_2020_01_2020_02.csv', \n",
    "#     'sub_seg_expire_2020_03_to_2020_04.csv', \n",
    "#     'sub_seg_expire_2020_04_2020_07.csv'\n",
    "\n",
    "#'dca_2019_06.csv', 'dca_2019_07.csv', 'dca_2019_08.csv', 'dca_2019_09.csv','dca_2019_10.csv'\n",
    "# TO DO : dca_2019_11\n",
    "start_date = '2019-10-01'\n",
    "end_date = '2020-08-31'\n",
    "\n",
    "# 'dca_2019_01.csv', 'dca_2019_02.csv', 'dca_2019_03.csv', 'dca_2019_04.csv',\n",
    "#                   'dca_2019_05.csv', 'dca_2019_06.csv', 'dca_2019_07.csv',\n",
    "#                   'dca_2019_08.csv',\n",
    "\n",
    "addition_success_samples = ['dca_2019_05.csv', 'dca_2019_06.csv', 'dca_2019_07.csv', 'dca_2019_08.csv']\n",
    "# training_files = [\n",
    "#                     'dca_2019_06.csv', 'dca_2019_07.csv', 'dca_2019_08.csv',\n",
    "#                    'dca_2019_09.csv', 'dca_2019_10.csv', 'dca_2019_11.csv', 'dca_2019_12.csv', \n",
    "#                   'dca_2020_01.csv', 'dca_2020_02.csv', 'dca_2020_03.csv', 'dca_2020_04.csv', 'dca_2020_06.csv']\n",
    "\n",
    "training_files = [\n",
    "                'dca_2019_10.csv', 'dca_2019_11.csv', 'dca_2019_12.csv',    \n",
    "                'dca_2019_10.csv', 'dca_2019_11.csv', 'dca_2019_12.csv',\n",
    "                'dca_2020_01.csv', 'dca_2020_02.csv', 'dca_2020_03.csv', 'dca_2020_06.csv', 'dca_2020_05.csv',  \n",
    "                'dca_2020_04.csv', 'dca_2020_08.csv']\n",
    "\n",
    "# training_files = [\n",
    "#                    'dca_2019_07.csv', 'dca_2019_08.csv',\n",
    "#                    'dca_2019_09.csv', 'dca_2019_10.csv', 'dca_2019_11.csv', 'dca_2019_12.csv', \n",
    "#                   'dca_2020_01.csv', 'dca_2020_02.csv', 'dca_2020_03.csv', 'dca_2020_05.csv']\n",
    "\n",
    "eval_files = [ 'dca_2020_07.csv' ]\n",
    "test_files = [ 'dca_2020_09.csv']\n",
    "bin_profile_per_date_month_path = 'bin_profile_per_date_month_2018_2020_08.csv'\n",
    "bank_profile_per_date_month_path = 'bank_profile_per_date_month_2018_2020_08.csv'\n",
    "bin_profile_per_day_of_month_path = 'bin_profile_per_day_of_month_2018_2020_08.csv'\n",
    "bank_profile_per_day_of_month_path = 'bank_profile_per_day_of_month_2018_2020_08.csv'\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "training hyperparameters\n",
    "'''\n",
    "scale_pos_weight = None\n",
    "\n",
    "tuned_parameters = {}\n",
    "\n",
    "best_parameters = {\n",
    "              'depth': 6,\n",
    "              'iterations': 1201,\n",
    "              'random_seed': 7,\n",
    "              'scale_pos_weight': scale_pos_weight,\n",
    "              'subsample': 0.5,\n",
    "              'bagging_temperature': 3.5,\n",
    "              'rsm': 0.35,\n",
    "              'eval_metric': 'BrierScore',\n",
    "              'early_stopping_rounds': 500,\n",
    "              'model_size_reg': 2.5,\n",
    "              'l2_leaf_reg': 20.9,\n",
    "              'random_strength': 5.0\n",
    "              }\n",
    "\n",
    "#  'BrierScore',\n",
    "\n",
    "# included_billing_countries = ['US']\n",
    "\n",
    "card_fields=['card_brand', 'card_category', 'card_class', 'card_usage']\n",
    "generic_decline_codes = SortedSet([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.web.utils import to_date\n",
    "from src.web.utils import days_between\n",
    "from src.web.utils import is_expired\n",
    "from SortedSet.sorted_set import SortedSet\n",
    "\n",
    "\n",
    "\n",
    "def convert_str_to_sorted_set(s):\n",
    "    x = str(s).replace(' ', '').replace('.0', '')\n",
    "    response_codes = SortedSet(x.split(',')) - SortedSet([''])\n",
    "    unique_codes =  response_codes - generic_decline_codes\n",
    "    if not unique_codes:\n",
    "        unique_codes =  response_codes\n",
    "    return \",\".join(unique_codes)\n",
    "\n",
    "def days_between_period(df):\n",
    "    d1 = to_date(df['next_renewal_date'])\n",
    "    d2 = to_date(df['grace_period_date'])\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "def process_dca_data(dca_df, df_sub, df_sub_seg, df_creation_date):\n",
    "    df = dca_df.copy()\n",
    "    \n",
    "    \"\"\"Exclude subs that use mes as processor but didn't use either amex or discover card brand\"\"\"\n",
    "    df_amex = df[(df.payment_service_id == 'mes') & (df.card_brand.isin(['American Express', 'Discover']))] \n",
    "    df = df[~(df.payment_service_id.isin(excluded_processors))]\n",
    "    df = pd.concat([df, df_amex])\n",
    "      \n",
    "#     df = df[df.billing_country.isin(included_billing_countries)]\n",
    "    df = pd.merge(df, df_sub[['subsegment_id', 'renewal_window', 'grace_period_date', 'next_renewal_date']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "    df = pd.merge(df, df_sub_seg[['subsegment_id', 'duration', 'segment_num']], left_on='subsegment_id', right_on='subsegment_id', how='left')\n",
    "    \n",
    "    df['is_expired'] = df.apply(is_expired, axis=1)\n",
    "    df.loc[~df['date_increment'].isna(), 'is_expired'] = True\n",
    "\n",
    "    df = df[~(df.duration.isna())]\n",
    "    df['bin'] = df['bin'].astype(str)\n",
    "    df = df[~(df['bin'] == 'nan')]\n",
    "    df = df[~(df['cc_expiration_date'] == 'nan')]\n",
    "\n",
    "    df = df[~(df['new_status'] == 'Reversed')]\n",
    "    df = df[~df['payment_amount_usd'].isna()]\n",
    "    \n",
    "    df = pd.merge(df, df_creation_date, left_on='subscription_id', right_on='subscription_id', how='left')\n",
    "    df.subs_activation_date.fillna('2017-01-01 00:00:00', inplace=True)\n",
    "    df['sub_age'] = df.apply(lambda x: days_between(x.transaction_date_in_string, x.subs_activation_date), axis=1)\n",
    "\n",
    "    df = df.rename(columns={\"next_renewal_date\": \"first_calendar_attempt_date\"})\n",
    "    \n",
    "#     #new addition\n",
    "#     df = df[~df.first_calendar_attempt_date.isna()]\n",
    "    \n",
    "    df.failed_decline_type = df.failed_decline_type_from_previous_cal\n",
    "#     df.loc[df['failed_decline_type_from_first_cal'] == 'Base', 'failed_decline_type_from_first_cal'] = df.failed_decline_type\n",
    "    \n",
    "    df.failed_response_codes_from_previous_cal = df.failed_response_codes_from_previous_cal.apply(convert_str_to_sorted_set)\n",
    "    df.failed_response_codes_from_first_cal = df.failed_response_codes_from_first_cal.apply(convert_str_to_sorted_set)\n",
    "\n",
    "#     df.failed_response_messages_from_previous_cal = df.failed_response_messages_from_previous_cal_sorted\n",
    "#     df.failed_response_messages_from_first_cal = df.failed_response_messages_from_first_cal_sorted\n",
    "    \n",
    "    \n",
    "    df[card_fields] = df[card_fields].astype(str).apply(\n",
    "                lambda x: x.str.lower().replace(' ', '', regex=True) \\\n",
    "                .replace(\"nodatafound',value:'n/a\", \"\", regex=False) \\\n",
    "                .replace(\"nodatafound\", \"\",regex=False) \\\n",
    "                .replace(\"nodatafound'value:'n/a\", \"\",regex=False))\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subscription_id</th>\n",
       "      <th>subs_activation_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13824581610</td>\n",
       "      <td>2020-01-24 13:49:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13835225210</td>\n",
       "      <td>2020-01-27 11:57:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13749247110</td>\n",
       "      <td>2020-01-06 07:01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13847169310</td>\n",
       "      <td>2020-01-30 11:12:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13830408310</td>\n",
       "      <td>2020-01-26 08:20:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subscription_id subs_activation_date\n",
       "0      13824581610  2020-01-24 13:49:26\n",
       "1      13835225210  2020-01-27 11:57:02\n",
       "2      13749247110  2020-01-06 07:01:08\n",
       "3      13847169310  2020-01-30 11:12:54\n",
       "4      13830408310  2020-01-26 08:20:54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "subs_creation_date_2019 =  pd.concat((read_from(file, s3_dir='ml_files') for file in subs_creation_date_files_2019) , ignore_index=True)\n",
    "subs_creation_date_2019 = subs_creation_date_2019.rename(columns={\"SUBSCRIPTION_ID\": \"subscription_id\", \"CREATION_DATE\": \"subs_activation_date\"})\n",
    "\n",
    "subs_creation_date =  pd.concat((read_from(file, s3_dir='ml_files') for file in subs_creation_date_files) , ignore_index=True)\n",
    "# subs_creation_date = subs_creation_date.rename(columns={\"SUBSCRIPTION_ID\": \"subscription_id\", \"CREATION_DATE\": \"subs_activation_date\"})\n",
    "\n",
    "subs_creation_date =  pd.concat([subs_creation_date, subs_creation_date_2019])\n",
    "subs_creation_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subsegment_id</th>\n",
       "      <th>next_renewal_date</th>\n",
       "      <th>grace_period_date</th>\n",
       "      <th>segment_number</th>\n",
       "      <th>line_item_type</th>\n",
       "      <th>renewal_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17904709400</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>8.0</td>\n",
       "      <td>RENEWED</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17958051000</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>2019-01-19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>RENEWED</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17795605900</td>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>RENEWED</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14777236400</td>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>2018-12-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENEWED</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15218507800</td>\n",
       "      <td>2019-01-27</td>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENEWED</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subsegment_id next_renewal_date grace_period_date  segment_number  \\\n",
       "0    17904709400        2018-12-15        2018-12-26             8.0   \n",
       "1    17958051000        2018-12-20        2019-01-19             2.0   \n",
       "2    17795605900        2018-12-04        2019-01-03             7.0   \n",
       "3    14777236400        2018-12-04        2018-12-19             NaN   \n",
       "4    15218507800        2019-01-27        2019-02-20             NaN   \n",
       "\n",
       "  line_item_type  renewal_window  \n",
       "0        RENEWED              11  \n",
       "1        RENEWED              30  \n",
       "2        RENEWED              30  \n",
       "3        RENEWED              15  \n",
       "4        RENEWED              24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_subs =  pd.concat((read_from(file, s3_dir='training_files') for file in subs_files) , ignore_index=True)\n",
    "\n",
    "df_subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           17904709400\n",
       "1           17958051000\n",
       "2           17795605900\n",
       "3           14777236400\n",
       "4           15218507800\n",
       "               ...     \n",
       "53468944    24364356400\n",
       "53468945    20989957000\n",
       "53468946    24650442000\n",
       "53468947    24417006400\n",
       "53468948    24628199800\n",
       "Name: subsegment_id, Length: 53468949, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs.subsegment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_seg_expire =  pd.concat((read_from(file, s3_dir='training_files') for file in sub_seg_expire_files) , ignore_index=True).drop_duplicates(subset=['subsegment_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py:284: DtypeWarning: Columns (15,48) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  sort=sort,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py:284: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = pd.concat((read_from(file, usecols=usecols, s3_dir='ml_files') for file in training_files) , ignore_index=True)\n",
    "\n",
    "df_train = process_dca_data(df_train, df_subs, df_sub_seg_expire, subs_creation_date)\n",
    "\n",
    "\n",
    "'''Additional success sample for df_train'''\n",
    "# df_train_2 = pd.concat((read_from(file, usecols=usecols, s3_dir='ml_files') for file in addition_success_samples) , ignore_index=True)\n",
    "# df_train_2 = df_train_2[(df_train_2.success == 1)]\n",
    "# df_train_2 = process_dca_data(df_train_2, df_subs, df_sub_seg_expire, subs_creation_date)\n",
    "# df_train = pd.concat([df_train, df_train_2])\n",
    "\n",
    "df_train.shape #(6543250, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.concat((read_from(file, usecols=usecols, s3_dir='ml_files') for file in eval_files) , ignore_index=True)\n",
    "df_val['subsegment_id'] = df_val['subsegment_id'].astype(int)\n",
    "\n",
    "df_val = process_dca_data(df_val, df_subs, df_sub_seg_expire, subs_creation_date)\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = pd.concat((read_from(file, usecols=usecols, s3_dir='ml_files') for file in test_files) , ignore_index=True)\n",
    "\n",
    "df_test.subsegment_id = df_test.subsegment_id.astype(int)\n",
    "df_test = process_dca_data(df_test, df_subs, df_sub_seg_expire, subs_creation_date)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = df_train[df_train.billing_country=='US'].groupby(['days_between', 'funding_source'])\n",
    "df_gp = gp.agg({'success':['sum', 'count'], 'subsegment_id':['nunique']})\n",
    "df_gp[('','success_rate')] = df_gp[('success', 'sum')] / df_gp[('subsegment_id', 'nunique')] * 100\n",
    "df_gp.columns = df_gp.columns.droplevel(0)\n",
    "df_gp = df_gp.rename(columns={'sum': 'success',  'nunique': 'num_of_subs'})\n",
    "df_gp[df_gp['num_of_subs'] > 10].sort_values(by=['success_rate', 'num_of_subs'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import for training\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "# from src.web.utils import PreProcessing\n",
    "from src.web.preprocessing import PreProcessing\n",
    "from src.web.encoder import EnhancedLeaveOneOutEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_profile_per_date_month = read_from(bin_profile_per_date_month_path)\n",
    "\n",
    "bin_profile_per_date_month['bin'] = bin_profile_per_date_month['bin'].apply(str).str.replace('.0', '', regex=False)\n",
    "max_per_date_month_dict = bin_profile_per_date_month.set_index(['bin', 'month', 'day_of_month'])['Max_99'].T.to_dict()\n",
    "\n",
    "bin_profile_per_day_of_month = read_from(bin_profile_per_day_of_month_path)\n",
    "\n",
    "bin_profile_per_day_of_month['bin'] = bin_profile_per_day_of_month['bin'].apply(str).str.replace('.0', '', regex=False)\n",
    "max_per_day_of_month_dict = bin_profile_per_day_of_month.set_index(['bin', 'day_of_month'])['Max_99'].T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_profile_per_date_month = read_from(bank_profile_per_date_month_path)\n",
    "bank_profile_per_date_month['bank_name'] = bank_profile_per_date_month['bank_name'].astype(str).apply(lambda x: x.lower().replace(' ', '').replace(\"nationalassociation\", \"n.a\").replace(\",\", \"\"))\n",
    "bank_profile_per_date_month['card_category'] = bank_profile_per_date_month['card_category'].astype(str).apply(lambda x: x.lower().replace(' ', '').replace(\",\", \"\"))\n",
    "\n",
    "max_per_bank_card_date_month_dict = bank_profile_per_date_month.set_index(['bank_name', 'card_category', 'month', 'day_of_month'])['Max_99'].T.to_dict()\n",
    "\n",
    "bank_profile_per_day_of_month = read_from(bank_profile_per_day_of_month_path)\n",
    "bank_profile_per_day_of_month['bank_name'] = bank_profile_per_day_of_month['bank_name'].astype(str).apply(lambda x: x.lower().replace(' ', '').replace(\"nationalassociation\", \"n.a\").replace(\",\", \"\"))\n",
    "bank_profile_per_day_of_month['card_category'] = bank_profile_per_day_of_month['card_category'].astype(str).apply(lambda x: x.lower().replace(' ', '').replace(\",\", \"\"))\n",
    "\n",
    "max_per_bank_card_day_of_month_dict = bank_profile_per_day_of_month.set_index(['bank_name', 'card_category', 'day_of_month'])['Max_99'].T.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_profile_per_day_of_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success_per_date_month_dict = bin_profile_per_date_month.set_index(['bin', 'month', 'day_of_month'])['count'].T.to_dict()\n",
    "# success_per_day_of_month_dict = bin_profile_per_day_of_month.set_index(['bin', 'day_of_month'])['count'].T.to_dict()\n",
    "\n",
    "success_per_bank_card_date_month_dict = bank_profile_per_date_month.set_index(['bank_name', 'card_category', 'month', 'day_of_month'])['count'].T.to_dict()\n",
    "success_per_bank_card_day_of_month_dict = bank_profile_per_day_of_month.set_index(['bank_name', 'card_category', 'day_of_month'])['count'].T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = len(df_train)\n",
    "balanced_size = len(df_train)\n",
    "fail_size = df_train[label].value_counts(normalize=True)[0.0]\n",
    "success_size =  df_train[label].value_counts(normalize=True)[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[card_fields] = df_train[card_fields].astype(str).apply(\n",
    "                lambda x: x.str.lower().replace(' ', '', regex=True) \\\n",
    "                .replace(\"nodatafound',value:'n/a\", \"\", regex=False) \\\n",
    "                .replace(\"nodatafound\", \"\",regex=False) \\\n",
    "                .replace(\"nodatafound'value:'n/a\", \"\",regex=False))\n",
    "\n",
    "df_val[card_fields] = df_val[card_fields].astype(str).apply(\n",
    "                lambda x: x.str.lower().replace(' ', '', regex=True) \\\n",
    "                .replace(\"nodatafound',value:'n/a\", \"\", regex=False) \\\n",
    "                .replace(\"nodatafound\", \"\",regex=False) \\\n",
    "                .replace(\"nodatafound'value:'n/a\", \"\",regex=False))\n",
    "\n",
    "df_test[card_fields] = df_test[card_fields].astype(str).apply(\n",
    "                lambda x: x.str.lower().replace(' ', '', regex=True) \\\n",
    "                .replace(\"nodatafound',value:'n/a\", \"\", regex=False) \\\n",
    "                .replace(\"nodatafound\", \"\",regex=False) \\\n",
    "                .replace(\"nodatafound'value:'n/a\", \"\",regex=False))\n",
    "\n",
    "df_train['card_info_is_empty'] =  (df_train['card_brand'] == '') & (df_train['card_category'] == '')\n",
    "df_val['card_info_is_empty'] =  (df_val['card_brand'] == '') & (df_val['card_category'] == '')\n",
    "df_test['card_info_is_empty'] =  (df_test['card_brand'] == '') & (df_test['card_category'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segment_num_group = [-1, 1, 2, 3, 4, 5, 6, 7, 8, 15, 20, 25, 30, 40, 50, 70, 100, 150]\n",
    "\n",
    "df_train['segment_num_group'] = pd.cut(df_train['segment_num'], segment_num_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_val['segment_num_group'] = pd.cut(df_val['segment_num'], segment_num_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_test['segment_num_group'] = pd.cut(df_test['segment_num'], segment_num_group).astype(str).str.replace('.0', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_group = [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 170, 190, 210, 250, 300, 400, 500, 1000, 1500, 2000, 5000, 10000, 20000]\n",
    "\n",
    "df_train['payment_amount_group'] = pd.cut(df_train['payment_amount_usd'], amount_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_val['payment_amount_group'] = pd.cut(df_val['payment_amount_usd'], amount_group).astype(str).str.replace('.0', '', regex=False)\n",
    "df_test['payment_amount_group'] = pd.cut(df_test['payment_amount_usd'], amount_group).astype(str).str.replace('.0', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_first_renewal'] = (df_train['segment_num'] < 2) & (df_train['segment_num'] >= 0)\n",
    "df_val['is_first_renewal'] = (df_val['segment_num'] < 2) & (df_val['segment_num'] >= 0)\n",
    "df_test['is_first_renewal'] = (df_test['segment_num'] < 2) & (df_test['segment_num'] >= 0)\n",
    "\n",
    "df_train.loc[(df_train.card_brand.str.lower().str.startswith('american', na=False)), 'card_category'] = 'american_express'\n",
    "df_val.loc[(df_val.card_brand.str.lower().str.startswith('american', na=False)), 'card_category'] = 'american_express'\n",
    "df_test.loc[(df_test.card_brand.str.lower().str.startswith('american', na=False)), 'card_category'] = 'american_express'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[(df_train['duration'] == 28) | (df_train['duration'] == 29) | (df_train['duration'] == 31) , 'duration'] = 30\n",
    "df_train.loc[(df_train['duration'] == 366) , 'duration'] = 365\n",
    "df_train.loc[(df_train['duration'] == 731) , 'duration'] = 730\n",
    "\n",
    "df_val.loc[(df_val['duration'] == 28) | (df_val['duration'] == 29) | (df_val['duration'] == 31) , 'duration'] = 30\n",
    "df_val.loc[(df_val['duration'] == 366) , 'duration'] = 365\n",
    "df_val.loc[(df_val['duration'] == 731) , 'duration'] = 730\n",
    "\n",
    "df_test.loc[(df_test['duration'] == 28) | (df_test['duration'] == 29) | (df_test['duration'] == 31) , 'duration'] = 30\n",
    "df_test.loc[df_test['duration'] == 366 , 'duration'] = 365\n",
    "df_test.loc[(df_test['duration'] == 731) , 'duration'] = 730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_group = [0, 3, 6, 9, 13, 17, 20, 25, 27, 33, 39, 43, 62, 70, 80, 88, 94, 100, 118, 125, 130, 146, 155, 176, 184, 200, 213, 230, 263, 300, 363, 368, 373,729, 733, 1000, 2000]\n",
    "\n",
    "df_train['sub_duration_group'] = pd.cut(df_train['duration'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_val['sub_duration_group'] = pd.cut(df_val['duration'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_test['sub_duration_group'] = pd.cut(df_test['duration'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "\n",
    "df_train['sub_age_group'] = pd.cut(df_train['sub_age'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_val['sub_age_group'] = pd.cut(df_val['sub_age'], duration_group).astype(str).str.replace('.0', '', regex=False) \n",
    "df_test['sub_age_group'] = pd.cut(df_test['sub_age'], duration_group).astype(str).str.replace('.0', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# from spark_sklearn import GridSearchCV\n",
    "\n",
    "from src.web.preprocessing import PreProcessing\n",
    "from src.web.preprocessing import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "# additional_fields = ['card_brand',  'segment_num', 'segment_num_group' ,'bank_name', 'duration', 'is_expired', 'renewal_window', 'payment_currency', 'funding_source', 'card_category', 'card_class', 'card_usage', 'renew_att_num', 'site_id', 'bin', 'merchant_number', 'billing_country', 'funding_source', \"payment_service_id\", 'day_of_month', 'failed_decline_type',  'failed_day_of_month', 'failed_response_code', 'payment_amount_usd', 'issuer_country',  'failed_response_message','days_between',  'transaction_date_in_string', 'cc_expiration_date', 'failed_attempt_date']\n",
    "additional_fields = [x for x in additional_fields if x not in (features_cat+features_num)]\n",
    "fields = features_cat + features_num + additional_fields\n",
    "features_num_encoded = features_num_encoded + features_num_bin_profile\n",
    "\n",
    "\n",
    "features_dict = {'LABEL': label, 'FIELDS': fields ,'FEATURES_CAT': features_cat, 'FEATURES_NUM':features_num, 'FEATURES_ENCODED':features_encoded, 'FEATURES_NUM_ENCODED':features_num_encoded, 'FEATURES_NUM_CALCULATED':features_num_calculated, 'FEATURES_FLOAT': features_float}\n",
    "features_dict_key = 'preprocessing__features_dict'\n",
    "features_dict['df_bin_profile'] = None \n",
    "features_dict['df_decline_type'] = None\n",
    "\n",
    "features_dict['FEATURES_NUM_BIN_PROFILE'] = features_num_bin_profile\n",
    "features_dict['FEATURES_GROUPED'] = features_grouped\n",
    "features_dict['ADDITIONAL_FIELDS'] = additional_fields\n",
    "\n",
    "\n",
    "max_per_bank_card_date_month_dict = {}\n",
    "success_per_bank_card_date_month_dict = {}\n",
    "features_dict['group_dict'] = {\"max_per_date_month_dict\": max_per_date_month_dict, \"max_per_day_of_month_dict\": max_per_day_of_month_dict, \\\n",
    "                               \"max_per_bank_card_date_month_dict\": max_per_bank_card_date_month_dict, \"max_per_bank_card_day_of_month_dict\": max_per_bank_card_day_of_month_dict, \\\n",
    "                              \"success_per_bank_card_date_month_dict\": success_per_bank_card_date_month_dict, \"success_per_bank_card_day_of_month_dict\": success_per_bank_card_day_of_month_dict}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prepares training parameters'''\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import src.web.preprocessing\n",
    "from src.web.preprocessing import PreProcessing\n",
    "from src.web.train_util import *\n",
    "from importlib import import_module\n",
    "import sys\n",
    "\n",
    "classifier = CatBoostClassifier\n",
    "\n",
    "cat_features_len = len(features_cat) +  len (features_grouped)\n",
    "input_data = df_train\n",
    "\n",
    "features_dict['use_cat_encoder'] = False\n",
    "_preProcessor = PreProcessing().fit(input_data, input_data['success'], features_dict=features_dict)\n",
    "\n",
    "_df_val = df_val \n",
    "_df_test = df_test\n",
    "\n",
    "if not scale_pos_weight:\n",
    "    scale_pos_weight = (_df_val[label].value_counts(normalize=True)[0.0] / _df_val[label].value_counts(normalize=True)[1.0] )\n",
    "best_parameters['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "_x_eval = _preProcessor.transform(_df_val)\n",
    "_y_eval = _df_val[\"success\"]\n",
    "\n",
    "\n",
    "alg_name = 'catboostclassifier'\n",
    "\n",
    "\n",
    "model_file = ''\n",
    "\n",
    "\n",
    "cat_features = list(range(0,cat_features_len))\n",
    "\n",
    "fit_params = {\n",
    "    f\"{alg_name}__verbose\": True,\n",
    "    f\"{alg_name}__cat_features\": cat_features,\n",
    "    f\"{alg_name}__plot\": True,\n",
    "    f\"{alg_name}__eval_set\": Pool(_x_eval, _y_eval, cat_features)\n",
    "}\n",
    "\n",
    "\n",
    "features_dict['fit_params'] = fit_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print out all parameters.\n",
    "'''\n",
    "\n",
    "print('training_runner = ', training_runner)\n",
    "print('project_id =', project_id)\n",
    "print('training_id =', training_id)\n",
    "print('metrics_feedback_url =', metrics_feedback_url)\n",
    "print('model_destination =', model_destination)\n",
    "print('label =', label)\n",
    "print('training_data =', training_data)\n",
    "\n",
    "print('training_files =', training_files)\n",
    "print('eval_files =', eval_files)\n",
    "print('test_files =', test_files)\n",
    "print('sub_seg_expire_files =', sub_seg_expire_files)\n",
    "print('subs_files =', subs_files)\n",
    "print('subs_creation_date_files =', subs_creation_date_files)\n",
    "print('excluded_processors =', excluded_processors)\n",
    "\n",
    "'''\n",
    "print out manipulated and aggregated features.\n",
    "'''\n",
    "print('\\n============== training parameters & features ================ ')\n",
    "print('input_features =', input_features)\n",
    "print('additional_fields =', additional_fields)\n",
    "print('tuned_parameters =', tuned_parameters)\n",
    "print('best_parameters =', best_parameters)\n",
    "print('features_cat =', features_cat)\n",
    "print('features_float =', features_float)\n",
    "print('features_num =', features_num)\n",
    "print('features_grouped =', features_grouped)\n",
    "\n",
    "print('feature_num_encoded =', features_encoded)\n",
    "print('features_encoded =', features_encoded)\n",
    "print('features_num_calculated =', features_num_calculated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Train the model\"\"\"\n",
    "if training_runner is None:\n",
    "    version = get_latest_version(model_id, model_type) + 1\n",
    "    model_name = model_id + '.' + str(version)\n",
    "    features_dict['model_name'] = model_name\n",
    "    output_dir=None\n",
    "\n",
    "clf, result_d = build_and_train(\n",
    "    input_data, \n",
    "    classifier, \n",
    "    tuned_parameters, \n",
    "    alg_name, \n",
    "    model_file, \n",
    "    best_param=best_parameters, \n",
    "    features_dict=features_dict, \n",
    "    test_data=_df_test,\n",
    "    output_dir=output_dir)\n",
    "                                   \n",
    "print(\"result_dict: \", result_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "output the model\n",
    "'''\n",
    "\n",
    "if training_runner is None:\n",
    "    model_file, model_file_name = write_model(clf, model_name)\n",
    "    \n",
    "    preprocess_repo_path = handle_preprocessing_file(model_id, version)\n",
    "    size_desc = str(\", original size: %s (fail: %s, success: %s), balanced_size: %s\" % (original_size, fail_size, success_size, original_size))\n",
    "    desc = 'Global model. Fixing preprocessing bug. Include 530 as response_code. Only using day of month bank_profile. Start_date from 2019-10. Update bank_profile, test data: 2020-09, eval data: 2020-07.  Add day_of_week-funding_source. Remove day_of_week-bank_name.  Remove days_between-bank_name-card_category. Exclude site_id. Start Date: 2019-09. Test data: 2020-08. Eval data: 2020-04.  Not using max_per_bank_card_date_month_dict and success_per_bank_card_date_month_dict. Fix empty duration, segment_num and renewal_window. Remove site_id. Add failed_decline_type-is_expired. Test file: 2020-06. Replace all na/ no data found value with emptry string. Add failed_decline_type-card_brand. 2020-04 as eval data. Revert skip max_per_date_month_dict. Use failed_response_codes_from_previous_cal-funding_source. Skip success_per_date_month. Revert bank_profile to use data until 2020-03. 2020-01 as eval data. Fix renewal_window on df_test. Add failed_response_codes_from_first_cal-funding-source. Remove failed_response_codes_from_first_cal-failed_response_codes_from_previous. Remove days_between_from_first_cal_failed_decline_type_from_first_cal. Training from 2020-01. Add failed_response_codes_from_previous_cal-days_between. Fix bin_profile_per_day_of_month_path file_path. Add fixed failed_response_messages_from_previous_cal-funding_source. Update decline type dict. Rearrange stop recurring decline type. Add card_brand-funding_source. Update decline_type dict 328:.  Use payment_amount_group only.  Add failed_decline_type-funding-source. Add payment_amount_group. Use txn_amount_bank_card_max_per_date_diff. Add failed_decline_type_from_first_cal. Add days_between-failed_response_codes_from_previous_cal Fix failed_response_codes_from_previous_cal. Add failed_response_message. Add failed_decline_type-failed_response_codes_from_previous_cal. Remove card_class. Add failed_decline_type-payment_service_id. Add payment_amount_usd.  Revert filter condition. Assign days_between_from_first_cal as days_between when na.  Add days_between_from_first_cal and group with renewal_window and funding_source. Using more specific failed_decline_type. Add days_between_from_first_cal-funding-source. Remove renew_att_num-days_between-renewal_window. Add days_between_from_first_cal-failed_decline_type_from_first_cal. Include first_calendar_attempt_date = na. Training data from 2020-06. Add is_expired. Revert bank_name-is_expired, bank_name-card_category-is_expired. Use success_bank_card_count_per_day_of_month instead of success_bin. Use days_between_from_first_cal-renewal_window only instead of days_between-renewal_window. Training data until 2020-05. Use 2020-04 as eval data. Eval data: 2020-04. Test data: 2020-06. Group segment_num 0 and 1 as the same group. Use scale_pos_weight. Add days_between_first_cal and renewal_window. Assign scale_pos_weight. Remove site_id.  Remove days_between-card_brand and funding_source. Add days_between-failed_decline_code. Include data with null first_cal_attempt. Add bank_name-is_expired, handle card_brand to be lower when grouping. Remove days_between_first_cal. With days_between-card_brand, reduce model_size_reg. Handle sub_duration_group in preprocessing. Minus segment_num. Using updated bank_profile, bin_profile to end of March. Use duration, sub_age, segment_num as numeric. Update sub_duration_group, days_between-card_brand,  duration(handle 28, 29,31 366 and 731), card_brand, renew_att_num,  sub_age.  Add days_between-failed_decline_type.  bank_card_max_per_date.  Add issuer_country-is_expired and bin-is_expired .is_expired to be True for all non na date increment. More individual features. Add more specific failed_decline_type.  {}_{}_for_calendar retry model,  eval_metric= BrierScore, with no date_increment, no payment amount and bin profile). {}'.format(start_date, end_date, size_desc)\n",
    "\n",
    "    hyper_params = result_d.pop('hyper_params', None)\n",
    "    extended_att = {\"preprocess_repo_path\": preprocess_repo_path, \"input_features\": input_features}\n",
    "    repo_path = upload_artifact(model_file_name)\n",
    "    insert_model_info(model_id, version, repo_path, desc=desc, model_type=model_type,eval_metrics=json.dumps(result_d), \n",
    "                      hyper_parameter=json.dumps(hyper_params), extended_att=json.dumps(extended_att), features_dict=features_dict, algorithm='CatBoostClassifier')\n",
    "    \n",
    "else:\n",
    "    model_file = joblib.dump(clf, model_destination)\n",
    "\n",
    "print('model_file generated: ', model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
